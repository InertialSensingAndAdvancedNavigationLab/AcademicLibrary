IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 1

## **DynaNet: Neural Kalman Dynamical Model** **for Motion Estimation and Prediction**


Changhao Chen, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni and Andrew Markham



_**Abstract**_ **—Dynamical models estimate and predict the temporal**
**evolution of physical systems. State Space Models (SSMs) in**
**particular represent the system dynamics with many desirable**
**properties, such as being able to model uncertainty in both**
**the model and measurements, and optimal (in the Bayesian**
**sense) recursive formulations e.g. the Kalman Filter. However,**
**they require significant domain knowledge to derive the para-**
**metric form and considerable hand-tuning to correctly set all**
**the parameters. Data driven techniques e.g. Recurrent Neural**
**Networks have emerged as compelling alternatives to SSMs with**
**wide success across a number of challenging tasks, in part due**
**to their impressive capability to extract relevant features from**
**rich inputs. They however lack interpretability and robustness**
**to unseen conditions. Thus, data-driven models are hard to be**
**applied in safety-critical applications, such as self-driving vehicles.**
**In this work, we present DynaNet, a hybrid deep learning and**
**time-varying state-space-model which can be trained end-to-**
**end. Our neural Kalman dynamical model allows us to exploit**
**the relative merits of both state-space model and deep neural**
**networks. We demonstrate its effectiveness of the estimation and**
**prediction on a number of physically challenging tasks, including**
**visual odometry, sensor fusion for visual-inertial navigation and**
**motion prediction. In addition we show how DynaNet can indicate**
**failures through investigation of properties such as the rate of**
**innovation (Kalman Gain).**


_**Index Terms**_ **—Dynamical Model, Motion Estimation, Deep**
**Neural Network, State Space Model**


I. I NTRODUCTION


From catching a ball to tracking the motion of planets across
the celestial sphere, the ability to estimate and predict the
future trajectory of moving objects is key for interaction with
our physical world. With ever increasing automation e.g. selfdriving vehicles and mobile robotics, the ability to not only
estimate system states based on sensor data, but also to reason
about latent dynamics and therefore predict states with partial
or even without any observation is of huge importance to the
safety and reliability of intelligent systems [1].
Newtonian/classical mechanics has been developed as an
explicit mathematical model which can be used to predict
future motion and infer how an object has moved in the past.
This is commonly captured in a state-space-model (SSM) that
describes the temporal relationship and evolution of states
through first-order differential equations. For example, in the
task of estimating egomotion from visual sensors, also known


Changhao Chen is with the College of Intelligence Science and Technology,
National University of Defense Technology, Changsha, 410073, China
Bing Wang, Niki Trigoni and Andrew Markham are with the Department of
Computer Science, University of Oxford, Oxford, OX1 3QD, United Kingdom.
Chris Xiaoxuan Lu is with the School of Informatics, University of
Edinburgh, Edinburgh, EH8 9AB, United Kingdom
Corresponding author: Changhao Chen (chenchanghao@nudt.edu.cn)
This work was supported by NFSC (Grant number: 62103427, 62073331)
and EPSRC Program “ACE-OPS: From Autonomy to Cognitive assistance in
Emergency OPerationS” (Grant number: EP/S030832/1)



as Visual Odometry (VO) [ 2 ], [ 3 ], [ 4 ], velocity, position,
and orientation are usually chosen as physically attributable
states for mobile robots. These models are typically handcrafted based on domain knowledge and require significant
expertise to develop and tune their hyper-parameters. Simplifying assumptions are often made, for example, to treat
the system as being linear, time-invariant with uncertainty
being additive and Gaussian. A canonical example of an
optimal Bayesian filter for linear systems is the Kalman Filter

[ 5 ], which is an optimal linear quadratic estimator. Although
capable of controlling sophisticated mechanical systems (e.g.
the Apollo 11 lander used a 21 state Kalman Filter [ 6 ]),
it becomes more challenging to use in complex, nonlinear
systems, giving rise to alternative variants such as the Sequential
Monte Carlo [ 7 ] or nonlinear graph optimisation [ 8 ]. However,
even when using these sophisticated approaches, imperfections
in model parameters and measurement errors from sensory
data contribute to issues such as accumulative drift in visual

navigation systems. Furthermore, there is a disconnect between
the complexities of rich sensor data e.g. images and derived

states.

Identifying the underlying mechanism governing the motion
of an object is a hard problem to solve for dynamical systems
operating in real world. As a consequence, in recent years,
there has been an explosive growth in applying deep neural
networks (DNNs) for motion estimation [ 9 ], [ 10 ], [ 11 ], [ 12 ],

[ 13 ], [ 14 ], [ 15 ], [ 16 ], [ 17 ]. These learning based approaches
can extract useful representations from high-dimensional raw
data to estimate the key motion states in an end-to-end fashion.
Although these learned models demonstrate good performance in both accuracy and robustness, they are ‘black-boxes’
regressing measurements with implicit latent states and difficult
to interpret. In contrast to neural networks, state-space-models
(SSMs) are able to construct a parametric model description and
offer an explicit transition relation that describes the evolution
of system states and uncertainty into the future. They can also
optimally fuse measurements from multiple sensors based on
their innovation gain, rather than simply stacking them as in a
neural network.

In this work, we propose _DynaNet_  - neural Kalman dynamical models, combining the respective advantages of deep
neural networks (powerful feature representation) and state
space models (explicit modelling of physical processes). As
illustrated in Figure 1, our proposed end-to-end model enables
automatic feature extraction from raw data and constructs a

time-varying transition model. The recursive Kalman Filter
is incorporated to provide optimal filtering on the feature
state space and estimate process covariance to reason about
system behaviour and stability. This allows for accurate system
identification in an end-to-end manner without human effort.


Our DynaNet can learn a linear-like state-space-model
directly from raw data. This linear-like structure is a drop-in
replacement for the typical recurrent neural network estimator.
Specifically, our contributions are three-fold:
(1) We propose a novel hybrid model with differentiable
Kalman filter that is adopted on the feature level instead of
system states level for latent state inference.
(2) We design a strategy to ensure the stability of learned
dynamical model by resampling the transition matrix from
Dirichlet distribution, in which the system proves to be stable
with a probability of one.
(3) With the design of neural emission model to connect
observations with full states, our DynaNet can cope with a number of challenging situations e.g. when only partial/corrupted
observations are available or even without any observations.
To demonstrate the effectiveness of the proposed technique,
we conducted extensive experiments and a systematic study on
challenging real world motion estimation and prediction tasks,
including visual odometry, visual-inertial odometry and motion
prediction. We show how the proposed method outperforms
the state-of-the-art deep-learning techniques, whilst yielding
valuable interpretable information about model performance.
The interpretability analysis discovers the interesting relation
between sensor data quality and the explicit model terms.
The rest of this paper is organized as follows: Section II
reviews the relevant work; Section III presents our proposed
neural Kalman dynamical model; Section IV evaluates our
proposed model applied to three different tasks, i.e. visual
egomotion estimation and prediction, visual-inertial navigation,
and motion prediction through extensive experiments; Section
V finally discusses conclusions.


II. R ELATED W ORK


_A. State-Space-Models (SSMs)_

State-space-model is a convenient and compact way to
represent and predict system dynamics. In classical control



engineering, system identification techniques are widely employed to build parametric SSMs [ 18 ], [ 19 ]. In order to alleviate
the effort of analytic description, data-driven methods, such as
Gaussian Processes [ 20 ], or Expectation–Maximization (EM)

[ 21 ], emerged as alternatives to identify nonlinear models.
Linear dynamic model, e.g. Kalman filtering has been explored
to combine with recurrent neural network that ensures the

convergence of neural network training [ 22 ]. With advances in
deep neural networks (DNNs), deep SSMs have been recently
studied to handle very complex nonlinearity. Specifically,
Backprop KF [ 23 ] and DPF [ 24 ] used DNNs to extract
effective features and feed them to a predefined physical
model (i.e. conditioned on algorithmic priors) to improve
filtering algorithms. [ 25 ] incorporates particle filter as an
algorithmic prior into the neural network for visual localization.
Besides feature extraction, DNNs have also been used in reparameterizing the transition matrix in SSMs from raw data

[ 26 ], [ 27 ], [ 28 ], [ 29 ]. Unlike prior art, our work exploits recent
findings on stable dynamical models [ 30 ] and uses resampling
to generate a transition matrix from the Dirichlet distribution,
whose concentration is learnt via a neural network. The specific
Dirichlet distribution ensures the stability of dynamic systems,
which is an important yet absent property of previous DNNs
based SSMs.


_B. Motion Estimation_


Motion estimation has been studied for decades and plays
a central role in robotics and autonomous driving. Conventional visual odometry/SLAM methods rely on multiple-view
geometry to estimate motion displacement between images [ 2 ],

[ 31 ], [ 32 ], [ 3 ], [ 33 ], [ 4 ], [ 34 ]. Owning to the huge availability
and complementary property of inertial and visual sensors,
integrating these two sensor modalities has raised increasing
attentions to give more robust and accurate motion estimates

[ 35 ]. A large portion of work in this direction is visual-inertial
odometry, where filtering [ 36 ], [ 37 ] and nonlinear optimisation


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 3















































Fig. 2: The DynaNet framework consists of the neural observation model to extract latent states **z**, the neural transition model
to generate the evolving relation **A**, and a recursive Kalman Filter to infer and predict system states. **P** is the covariance matrix
of latent states, **Q** and **R** are process noise matrix and observation noise matrix respectively.




[ 38 ], [ 35 ], [ 39 ] are two mainstream model-based methods for
sensor fusion. Meanwhile, recent studies also found that the
methods using data-driven DNNs are able to provide competitive robustness and accuracy over some model-based methods.
These deep learning methods often use convolutional neural
networks (ConvNets) to discover useful geometry features from
images for effective odometry estimation [ 11 ], [ 16 ], [ 17 ], [ 40 ],

[ 41 ], and/or employ RNNs to model the temporary dependency
in motion dynamics [ 12 ], [ 42 ], [ 10 ], [ 43 ]. Besides self-motion
estimation in robotics and autonomous driving, RNNs have
also been introduced to model human dynamics, and address
the problem of human-skeleton motion prediction [ 44 ], [ 45 ]. To
improve the capacity of long-term prediction, [ 46 ] leverages
temporal attention mechanism to predict next step motion
based on all historical information. Furthermore, [ 47 ] considers
both the spatial and temporal relations of human skeleton
motions, and proposes a novel skeleton-joint attention with
RNNs to achieve better performance in the task of human
motion prediction. Nevertheless, DNN based methods are hard
to interpret or expect/modulate their behaviours [ 1 ]. Motivated
by this, our DynaNet aims to bridge the gap of performance
and interpretability through a deeply coupled framework of
model-based and DNN-based methods.


III. N EURAL K ALMAN D YNAMIC M ODELS


We consider a time-dependent dynamical system, governed
by a complex evolving function _f_ :


**z** _t_ = _f_ ( **z** _t−_ 1 _,_ **w** _t_ ) (1)


where **z** _∈_ R _[d]_ is _d_ -dimensional latent state, _t_ is the current
timestep, and **w** is a random variable capturing system and
measurement noise. The evolving function _f_ is assumed to



be Markovian, describing the state-dependent relation between
latent states **z** _t_ and **z** _t−_ 1 . The model in Equation (1) can be
reformulated as a linear-like structure, i.e. the state-dependent
coefficient (SDC) form, with a time-varying transition matrix
**A** :

**z** _t_ = **A** _t_ **z** _t−_ 1 (2)


Notably, the system nonlinearity is not restricted by this linearlike structure, as there always exists a SDC form _f_ ( _z_ ) =
_A_ ( _z_ ) _z_ to express any continuous differentiable function _f_ with
_f_ (0) = 0 [48].
In this regard, our problem of the dynamic model is _how_
_to recover the latent states_ **z** _and their time-varying transition_
_relation_ **A** _from high-dimensional measurements_ **x** _(e.g. a_
_sequence of images), without resorting to a hand-crafted_
_physical model_ .
This work aims to construct and reparameterize this dynamic
model using the expressive power of deep neural networks
and explicit state-space models. Figure 2 shows the main
framework, which will be discussed in details in the following
subsections. To avoid confusion, in the rest of this paper, _latent_
states **z** are exclusively used for dynamical models and _hidden_
states **h** exclusively represent the neurons in a deep neural
network. In the meantime, we will use sensor measurements
and observations interchangeably.


_A. Neural Emission Model_


Intuitively, the system states containing useful information
often lie in a latent space that is different from the original
measurements. For example, given a sequence of images (the
sensor measurements), the key system states of visual odometry


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 4























_**a. Deterministic Transition**_ _**b. Resampled Transition**_


Fig. 3: Two transition generation strategies - (a) deterministic transition or (b) resampled transition are proposed to achieve
desired system behaviours. The transition matrix is generated by a recurrent neural network, i.e. LSTM in this work conditioned
on previous latent system states.



are velocity, orientation and position. Nevertheless, it is nontrivial for conventional models to formulate a temporal linear
model that can precisely describe the relation between these
physical representations.
Rather than explicitly specifying physical states as in a
classical SSM, we use a deep neural network to automatically
extract the latent state features whilst forcing them to follow
the linear-like relation in Equation (2). This can be achieved
automatically by optimizing the model via stochastic gradient
descent and backpropagation algorithms. This linearization is
particularly useful as it allows us to directly use a Kalman
Filter for state feature inference. Note that the differentiable

Kalman filtering in our DynaNet model performs on the high
dimensional latent feature space rather than the physical state
space (e.g. velocity, orientation and position) as in Backprop
KF [23] and DPF [24].
In our neural emission model, an encoder _f_ encoder is used to
extract both features **a** _t_ and an estimation of uncertainty _**σ**_ _t_ _[a]_
from the observations **x** _t_ at timestep _t_ :


**a** _t_ _,_ _**σ**_ _t_ = _f_ encoder ( **x** _t_ ) _._ (3)


The features **a** act as observations of the latent feature state

space. The coupled uncertainties _**σ**_ represent the measurement
belief that is transformed into the observation noise **R** in a

Kalman Filter. **a** and _**σ**_ are further used in the update stage of
a differentiable Kalman filter in Equation (14), which forces
them to follow the distribution of KF parameters during end-toend optimization. Thus, unlike VAE[ 49 ], [ 29 ], the two terms
are not directly and explicitly constrained in Equation (3) to
follow a prior distribution, but leave the learning model to
search for suitable latent states and uncertainty estimation that
can construct a linear-like structure and exploit the full capacity
of differentiable Kalman filter. However, the observations **a**
are sometimes unable to provide sufficient information for
all latent states **z** in a dynamical system, for example, the
occasional absences of sensory data. Hence, a deterministic
emission matrix **H** is defined to connect with the full latent

states **z** :

**a** _t_ = **Hz** _t_ (4)



In a practical setting, when the extracted features contain all
the information for dynamical systems, the emission matrix
**H** is set to d-dimensional identity matrix **I** _d_ as features and
states are identical at this moment. On the other side, the
identity matrix needs to adapt to **H** = [ **I** _m_ _,_ **0** _m×_ ( _d−m_ ) ], when
observations only give rise to _m_ features. In this case, the rest
( _d_ _−_ _m_ ) latent states will be attained from historical states. Our
experiment in Section IV-C demonstrates the superiority this
neural emission model in addressing the issue of observation
absences for sensor fusion in visual-inertial odometry.


_B. Neural Transition Model_


In a SSM, the temporal evolution of latent states is determined by the transition matrix **A** as in Equation 2. Obviously,
the transition matrix is of considerable importance as it directly
describes the governing mechanism of a system. Nevertheless,
such a matrix is difficult to specify manually, especially when
it is time-varying. Figure 3 demonstrates two methods we
propose to estimate **A** on the fly, based on prior system
states: (a) a deterministic way to learn it end-to-end from raw
data; (b) a stochastic way to resample it from a distribution,
e.g. the Dirichlet distribution in this work. The deterministic
transition and resampled transition are two individual strategies
to generate transition matrix. The parameters of LSTM in these
two modules are separately learned from data. We will explain
them accordingly in what follows.
_1) Deterministic Transition:_ Intuitively, a movement change
depends on historical system states, which are encoded in the
latent states **z** 0: _t−_ 1 . Prior works mostly apply a recurrent neural
network (RNN) to specify dynamic weights for choosing and
interpolating between a fixed number of different transition
modes [ 29 ], [ 28 ]. Inspired by [ 50 ], our model generates the
transition matrix **A** directly from the history of latent states **z** .
In this deterministic transition model, the dependence of
the transition matrix on historical latent states is specified
by a RNN. This RNN recursively processes previous hidden
states ( **z** _t−_ 1 _,_ **h** _t−_ 1 ) of the dynamic model and Long Short-Term
Memory (LSTM) [51], [52] cell respectively, and outputs the


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 5



time-dependent transition matrix **A** _t_ via:


**A** _t_ = LSTM( **z** _t−_ 1 _,_ **h** _t−_ 1 ) _,_ (5)


where **z** _t−_ 1 is the latent states of dynamical system at the
timestep ( _t −_ 1), **h** _t−_ 1 is the hidden states of LSTM module,
**A** _t_ is the learned transition matrix at current timestep.
_2) Resampled Transition:_ Powerful deep neural networks
are able to approximate dynamical models effectively from
data. However, these learned ‘black-box’ models are difficult
to be interpreted or modulated. Especially, in safety-critical
applications (e.g. self-driving vehicles), the lack of model
behavioural indicator and the absence of system reliability
largely limit the adoption of these learning models. The
stability of a dynamical system is essential and fundamental
to autonomous systems, as it guarantees that the predictions
of dynamic models will not change abruptly, given slightly
perturbed inputs. Unfortunately, this desirable property is not
ensured by most pure DNN models. Therefore, we here aim to
ensure the stability of learned dynamical model by resampling
the transition matrix from a specific distribution.
The linear-like SSM structure in Equation (2) allows for
a quadratic Lyapunov stability analysis, whilst advances in
stochastic optimisation allow to construct neural probabilistic
models [ 53 ]. We thus propose to resample the transition matrix
from a predefined probability distribution to enforce the desired
stability. Based on the findings in [ 30 ], if the state transition
follows a Dirichlet distribution in a positive system, it will
lead to a model being asymptotically stable i.e. it will be
Bounded Input Bounded Output (BIBO) stable. Constructing
the stochastic variable and determining the parameters of
the distribution are easy to achieve with the widely used
reparameterisation trick [54].
To further reduce hand-crafted engineering, the concentration
_**α**_ of the Dirichlet distribution in our framework is generated
from historical system states via an LSTM based RNN:


_**α**_ = LSTM( **z** _t−_ 1 _,_ **h** _t−_ 1 ) _,_ (6)


where **z** _t−_ 1 is the latent states of dynamical system at the
timestep ( _t −_ 1), **h** _t−_ 1 is the hidden states of LSTM module.
A small Gaussian random noise is also added in this process to
improve model robustness. At each timestep, a realisation of
the transition matrix **A** is drawn from the constructed Dirichlet

distribution:
**A** _t_ _∼_ Dirichlet( _**α**_ ) (7)


Note that in DynaNet the transition states are in the latent
feature space rather than the final target states (e.g. the states
of orientation and position in VO). The latent features are
extracted by the encoder, which ensures the transition states
strictly positive through a ReLU activation and a tiny random
positive number on the last layer of the DNN based encoder.
Our DynaNet extends the work [ 30 ] of nearest neighbor
method based stable system to a deep neural network based
approach, for modelling more complex nonlinear dynamics.
Analytically, the following proof supports our proposed system:
_given a DNN based dynamical system_ **z** _t_ +1 = **A** ( _**α**_ ) **z** _t_ _, where_
**z** _t_ _∈_ R _[d]_ _is_ _d_ _-dimensional system state at the timestep_ _t_ _,_
_extracted by a neural network from raw data,_ **A** ( _**α**_ ) _∈_ R _[d][×][d]_



_is the transition matrix generated by a neural network with_
_a concentration_ _**α**_ _, if the transition matrix_ **A** _is constructed_
_from a Dirichlet distribution_ **A** _∼_ _Dir_ ( _θ_ ( _**α**_ )) _, this dynamical_
_system is asymptotically stable_ .
**Proof:** By resampling the transition matrix **A** _∈_ R _[d][×][d]_ from
a Dirichlet Distribution, the elements inside **A** satisfy:


**A** ( _i,j_ ) _>_ 0 _,_ **A** ( _i,j_ ) _<_ 1 _∀i, j_ = 1 _...d,_



_d_

(8)

� **A** ( _i,j_ ) = 1

_j_ =1



and


We can have



_d_
�


_i_ =1



_d_
� **A** ( _i,j_ ) _<_ 1 _∀i_ = _⇒_ _i_ max =1: _d_

_j_ =1



_d_
� **A** ( _i,j_ ) _<_ 1 (9)

_j_ =1



If all elements inside **A** are strictly positive, then the Maximum
absolute Row Sum Norm _∥_ **A** _∥_ _∞_ follows:



_∥_ **A** _∥_ _∞_ = max
_i_ =1: _d_



_d_
� _|_ **A** ( _i,j_ ) _| <_ 1 (10)

_j_ =1



The hidden feature states **z** _k_ + _M_ at the time step _t_ + _M_ is
derived from the system states **z** _k_ at the time step _t_ and _M_
consecutive transition matrix via:



_M_
� _∥_ **A** _[m]_ _∥_ _∞_ _∥_ **z** _∥_ _∞_


_m_ =1



_∥_ **z** _k_ + _M_ _∥_ _∞_ = _∥_



_M_
� **A** _[m]_ **z** _k_ _∥_ _∞_ _≤_


_m_ =1



_≤_ (max _m_ _[∥]_ **[A]** _[m]_ _[∥]_ _[∞]_ [)] _[M]_ _[∥]_ **[z]** _[k]_ _[∥]_ _[∞]_
(11)


As the _M_ is infinite, the system states **z** _k_ + _M_ will become:


_M_ _→∞_
_∥_ **z** _k_ + _M_ _∥_ _∞_ _−−−−→_ 0 (12)


Therefore, the system is stable with a probability of one.


_C. Prediction and Inference with a Kalman Filter_

The neural emission model estimates system states from
noisy sensor measurements, while the generated transition
model describes the system evolution and predicts the system
states with previous ones. However, uncertainties exist in both
of them and motivate us to integrate a Kalman Filter into
our framework. The Kalman Filter recursively deals with the
uncertainties, and produces a weighted average of the state
predictions and fresh observations. With the aforementioned
neural emission and transition models, the prediction and
inference are performed on the feature state space and follow
a standard Kalman filtering pipeline. We also note that the
Kalman Filter’s gain controls how much to update the residual
error (i.e., the difference between prediction and observation),
which is a useful metric to represent the relative quality of
measurements (as shown in Section IV-E).
More specifically, the Kalman Filter consists of two blocks:
prediction and update. In the prediction stage, prior estimates
of the mean value and covariance ( **z** _t|t−_ 1 _,_ **P** _t|t−_ 1 ) at the
current timestep are derived from the posterior state estimates
( **z** _t−_ 1 _|t−_ 1 _,_ **P** _t−_ 1 _|t−_ 1 ) in the previous timestep:


**z** _t|t−_ 1 = **A** _t_ **z** _t−_ 1 _|t−_ 1 _,_

(13)
**P** _t|t−_ 1 = **A** _t_ **P** _t−_ 1 _|t−_ 1 **A** _[T]_ _t_ [+] **[ Q]** _[t]_ _[.]_


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 6



When current observations **a** _t_ are available, the update process
allows us to produce a posterior mean and covariance of hidden
states ( **z** _t|t_ _,_ **P** _t|t_ ) as follows:


**r** _t_ = **a** _t_ _−_ **H** _t_ **z** _t|t−_ 1 _,_

**S** _t_ = **R** _t_ + **H** _t_ **P** _t|t−_ 1 **H** _[T]_ _t_ _[,]_



**K** _t_ = **P** _t|t−_ 1 **H** _[T]_ _t_ **[S]** _[−]_ _t_ [1] _[,]_

**z** _t|t_ = **z** _t|t−_ 1 + **K** _t_ **r** _t_ _,_

**P** _t|t_ = ( **I** _−_ **K** _t_ **H** _t_ ) **P** _t|t−_ 1 _,_



(14)



only available in the unsynced data packages, we selected the
raw files with the corresponding to KITTI Odometry Dataset.
Inertial data and images are manually synchronized according
to their timestamps. We adopted the same data split mentioned
above, discarding _Sequence 03_ as its raw data is unavailable.
Thus, in this experiment, we used _Sequences 00, 01, 02, 04,_
_05, 06, 07, 08_ for training and _Sequences 09, 10_ for testing.
We chose the sequence length as 5, and thus generated 20373
sub-sequences from training set to train neural models. We
chose the sequence length as 5, and thus generated a total of
20361 sub-sequences from training set to train deep neural
networks.


_2) Baselines:_ In the visual odometry experiment, we compare our DynaNet models with three representative three deep
learning based VO models, i.e. SfmLearner [ 11 ], Bian et al.

[ 56 ] and DeepVO [ 10 ]. DeepVO shares the same architecture
as in our models including the encoder and predictor, but
uses the 2-layers LSTM [ 52 ] to estimate system latent states.
This can be viewed as an ablation study, and we keep their
dimension of hidden states (128) the same as our models for
a fair comparison. Except learning based baselines, our model
is also compared with two classical monocular VO system,
i.e. ORB-SLAM [ 34 ] and VISO2 [ 57 ]. ORB-SLAM [ 34 ] is
monocular visual SLAM algorithm based on hand-crafted
features and multi-view geometry. Its loop closing module
is disabled for a fair comparison with odometry estimation.
VISO2 [ 57 ] is a monocular VO algorithm, and implemented
as an official baseline on KITTI dataset.


In the visual-inertial odometry (VIO) experiment, we chose
a state-of-the-art learning based VIO approach, i.e. VINet [ 12 ]
as our baseline. VINet shares similar structure as in our models,
but it uses a two-layers LSTM rather than our DynaNet module.
A popular classical model-based VIO system, i.e. mono-VINS

[ 39 ] is also adopted here as baseline. Mono-VINS [ 39 ] is a
tightly-coupled sliding window-based optimization approach
for visual inertial odometry, which achieves state-of-the-art
performance on several VIO datasets.


In the motion prediction task, we compare our models with
LSTM based approaches. All the other modules for LSTMs
including encoder and predictor, and the dimension of hidden
states (128) are kept the same as in our proposed models for a
fair comparison. Besides, we implemented an attention based
LSTM approach to show how attention mechanism improves
the motion prediction in future steps. As the approach is an
end-to-end model that directly deals with high-dimensional raw
images, at each time step the attention mechanism aggregates
the features extracted from visual data or visual/inertial data,
and then feeds the updated features to LSTM module. The rest
modules are kept the same to be compared fairly.


_3) Experiment Setups:_ We implemented the proposed framework with Pytorch, and trained on a NVIDIA Titan X GPU.
The implementation details of DynaNet frameworks can be
found at APPENDIX A. All of our models are trained with

the Adam optimizer with a batch-size of 32 and a learning
rate of 1 _e_ _[−]_ [4] . The trained models are tested on the test set, in
which data have never been seen in the training set.



where **r** is the residual error (aka. innovation), **S** is the residual
covariance and **K** is Kalman gain. In contrast to hand-tuning
process noise **Q** and measurement noise **R** in a conventional
KF, these two terms are jointly learned by our proposed neural
dynamical model. Finally, the predictor (e.g. a FC network)
outputs the target values **y** _t_ from the estimated optimal hidden
states **z** _t|t_ :
**˜y** _t_ = _f_ predictor ( **z** _t|t_ ) _._ (15)


In the case that current measurements i.e. **a** _t_ are unavailable,
the reconstructed values **ˆy** _t_ are inferred from the prior estimate

**z** _t|t−_ 1 :
**ˆy** _t_ = _f_ predictor ( **z** _t|t−_ 1 ) _._ (16)


All parameters _θ_ in our model are end-to-end learned with a
mean square loss function. This loss function jointly compares
the ground truth **y** _t_ with posterior predictions **˜y** _t_ and prior
prediction **ˆy** _t_ :



_L_ ( _θ_ ) = [1]

_T_



_T_
�( _||_ **y** _t_ _−_ **˜y** _t_ _||_ [2] + _||_ **y** _t_ _−_ **ˆy** _t_ _||_ [2] ) (17)


_t_ =1


IV. E XPERIMENTS



We systematically evaluate our system through extensive
experiments including (1) Section IV-B - pose estimation for
visual odometry, (2) Section IV-C - pose estimation for visualinertial odometry and (3) Section IV-D - motion prediction
without observations or with partial observations. Moreover,
an interpretability study is also conducted in Section IV-E.


_A. Datasets, Baselines and Experiment Setups_


_1) Datasets:_ To evaluate our proposed DynaNet models, we
used public datasets to conduct experiments: KITTI Odometry
dataset [ 55 ] for visual pose estimation and prediction, and
KITTI Raw Dataset [ 55 ] for visual-inertial pose estimation and
prediction.
**KITTI Odometry Dataset [55]** is a commonly used
benchmark dataset that contains 11 sequences (00-10) with
images collected by car-mounted cameras and ground-truth
trajectories provided by GPS. We used it for visual odometry
experiment, with _Sequences 00-08_ for training and _Sequences_
_09, 10_ for testing. The images and ground truth are collected at
10 Hz. We chose the sequence length as 5, and thus generated
a total of 20373 sub-sequences from training set to train neural
models.

**KITTI Raw Dataset [55]** contains both raw images (10 Hz)
and high-frequency inertial data (100Hz). Since inertial data are


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 7


TABLE I: The performance of visual odometry on the KITTI odometry dataset for motion estimation, reported in the RMSE of
translation (%) and orientation ( _[◦]_ )

|Col1|ORB-SLAM|VISO2|SfmLearner|Bian et al.|DeepVO (LSTM)|Ours (Deter.)|Ours (Dirichlet)|
|---|---|---|---|---|---|---|---|
|09<br>10|45.52%, 3.10~~_◦_~~<br>**6.39%**, 3.20_◦_<br>|18.06%, 1.25~~_◦_~~<br>26.10%, 3.26_◦_<br>|17.84%, 6.78~~_◦_~~<br>37.91%, 15.78_◦_<br>|11.2%, 3.35~~_◦_~~<br>10.1%, 4.96_◦_<br>|8.01%, 3.10~~_◦_~~<br>8.53%, 2.41_◦_<br>|6.43%, 2.19~~_◦_~~<br>8.35%, 2.39_◦_<br>|**4.97%**,** 2.10**~~_◦_~~<br>9.08%,** 2.15**_◦_<br>|
|ave|25.95%, 3.15~~_◦_~~|22.08%, 2.25~~_◦_~~|27.87%, 11.28~~_◦_~~|10.65%, 4.15~~_◦_~~|8.27%, 2.75~~_◦_~~|7.39%, 2.29~~_◦_~~|**7.03%**,** 2.12**~~_◦_~~|



_•_ _t_ _rel_ (%) is the average translational RMSE drift (%) on lengths of 100m-800m.

_•_ _r_ _rel_ ( _[◦]_ ) is the average rotational RMSE drift ( _[◦]_ /100m) on lengths of 100m-800m.

_•_ The DeepVO (LSTM), our proposed deterministic (Ours (Deter.)) and Dirichlet (Ours (Dirichlet)) model are trained on Sequence 00 - 08 of the KITTI
dataset [55] with same hyperparameters for a fair comparison, and tested on Sequence 09 and 10.





600


500


400


300


200


100


0





300


200


|Groun<br>Deep|dtruth<br>VO (LST|M)|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|Dyna<br>Dyna|Net (Dete<br>Net (Dir.)|r.)|||||
||||||||
||||||||
||||||||
||||||||
||||||||


|Groundtut<br>DeepVO (<br>DynaNet (|h<br>LSTM)<br>Deter.)|Col3|Col4|Col5|
|---|---|---|---|---|
|DynaNet (|Dir.)|Dir.)|||
||||||
||||||
||||||



-200 -100 0 100 200 300 400
East (m)


(a) Poses estimation in Seq 09



100


0


-100

0 200 400 600 800
East (m)


(b) Poses estimation in Seq 10



Fig. 4: The testing trajectories on (a) Sequence 09 and (b) Sequence 10 of the KITTI dataset indicate that our models produce
robust and accurate pose estimates in visual odometry.



TABLE II: The performance of visual-inertial odometry on
the KITTI raw dataset for motion estimation, reported in the
RMSE of translation (%) and orientation ( _[◦]_ ).


VINS-Mono VINet Ours (Deter.) Ours (Dir.)
09 41.5%, 2.41 ~~_[◦]_~~ **3.89%**, 2.02 ~~_[◦]_~~ 5.45%, **1.24** ~~_[◦]_~~ 4.13%, 1.39 ~~_[◦]_~~

10 20.3%, 2.73 _[◦]_ 8.99%, **1.39** _[◦]_ **5.49%**, 2.02 _[◦]_ 7.03%, 2.64 _[◦]_

ave 30.9%, 2.57 ~~_[◦]_~~ 6.44%, 1.70 ~~_[◦]_~~ **5.47%**, **1.63** ~~_[◦]_~~ 5.58%, 2.01 ~~_[◦]_~~

_•_ _t_ _rel_ (%) is the average translational RMSE drift (%) on lengths of
100m-800m.

_•_ _r_ _rel_ ( _[◦]_ ) is the average rotational RMSE drift ( _[◦]_ /100m) on lengths of
100m-800m.

_•_ The VINet (LSTM), our proposed deterministic (Ours (Deter.)) and
Dirichlet (Ours (Dirichlet)) model are trained on Sequence 00, 01, 02,
04, 05, 07, 08 of the KITTI raw dataset [ 55 ] with same hyperparameters
for a fair comparison, and tested on Sequence 09 and 10.


_B. Visual Odometry_


Our evaluation starts with a set of visual ego-motion (Visual
Odometry) experiments for 6-DoF pose estimation. Here, pose
estimation means that our model produces 6-DoF pose given
sensor data (i.e. images). In this experiment, a sequence of raw
images are given to models to produce pose transformations,
i.e. translation and rotation.

Table I reports the performance of our proposed DynaNet
models, comparing with other learning based approaches and
classical VO algorithm. All neural networks were trained
above KITTI Odometry dataset with Sequences 00 - 08, while
tested with two new sequences (Sequence 09 and 10). The
motion transformations from models are integrated into global
trajectories, and them we evaluated them according to the
official KITTI metrics, commonly adopted to evaluate VO



algorithms, which calculates the average Root Mean Square
Errors (RMSEs) of the translation and rotation for all the
subsequences in the lengths 100, 200, ..., 800 meters. This
evaluation metrics can capture both the global and local drifts
of VO systems.
As shown in Table I, our proposed models clearly outperform
the baselines of ORB-SLAM [ 34 ], VISO2 [ 57 ], SfmLearner

[ 11 ], Bian et al. [ 56 ] and DeepVO [ 11 ], and the largest gain is
achieved by our Dirichlet model. Note that the only different
between our models and DeepVO is the state estimation part,
and we keep the model hyberparameters, e.g. the dimension
of hidden states, the same for a fair comparison. By replacing
the LSTM module in DeepVO with our proposed DynaNet,
our Deterministic model improves the performance of DeepVO
around 10.64% in translation and 16.73% in orientation, and
our Dirichlet model further improves DeepVO around 14.99%
in translation and 22.91% in orientation. This indicates that

incorporating the physical prior into neural network benefits
learning state estimation from data. And it also implies that the
nonlinearities of VO systems are not lost despite the linear-like
structures inside our models.

Figure 4 illustrates the trajectories of Sequence 09 and 10
predicted by our models. Sequence 09 and 10 are difficult
scenarios, as the driving car experienced large movement in
height. Our DynaNet models, especially the Dirichlet model,
are still capable of providing robust results, which are closer to
the groundtruth trajectories, and consistently show competitive
performance over LSTM based DeepVO.
It must be pointing out that the performance of learning
model depends on its training process. Both under-fitting


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 8


TABLE III: Visual-inertial navigation on the KITTI raw dataset for motion prediction (Translation RMSE [0.01 m]).


Prediction w/o Inputs Visual Only Prediction Inertial Only Prediction
LSTM (1-layer) 32.5 7.86 23.7
LSTM (2-layers) 21.2 6.90 24.3
LSTM + Attention 17.4 7.54 23.1
Ours (Deterministic) 13.0 **6.27** **11.3**
Ours (Dirichlet) **12.3** 6.40 12.3



TABLE IV: Visual odometry on the KITTI odometry dataset
for motion prediction (Translation RMSE [0.01 m]).


5 Steps Prediction 10 Steps Prediction
LSTM (1-layer) 16.8 23.4
LSTM (2-layers) 11.0 17.7
LSTM + Attention 12.1 17.4
Ours (Deterministic) 10.8 16.3
Ours (Dirichlet) **8.69** **13.5**


Fig. 5: The testing trajectories of our proposed DynaNet model
(Dirichlet) on Sequence 9 of the KITTI dataset in the underfitting, well-fitting and over-fitting conditions.


and over-fitting should be avoided for machine learning
methods, or their testing performance will be degraded in such
circumstances. Figure 5 shows the trajectories of our proposed
DynaNet model with Dirichlet distribution on Sequence 9
of the KITTI dataset in three different training stages. Our
model is trained for a total of 100 epochs. When DynaNet
model is trained under the well-fitting condition (Epoch 89),
the testing trajectory is comparable closer to the ground-truth.
However, when our DynaNet is in the under-fitting (Epoch 50)
or over-fitting (Epoch 90) condition, it sees larger drifts in the
corresponding trajectory.


_C. Visual-Inertial Odometry_


How to effectively integrate and fuse two modalities to
provide accurate and robust pose remains a challenging problem.
In this experiment, we demonstrate that our proposed models
can learn a compact state-space-model for sensor fusion from
two modalities, i.e. visual and inertial data. We also show
that our DynaNet models enable robust prediction under the
circumstances with partial observations in Section IV-D . When
the training model is underfitting or overfitting,



_1) Hyper-parameters Setup:_ Initially, a visual encoder and
an inertial encoder extract _m_ -dimensional visual features

**a** visual _∈_ R _[m]_ and _n_ -dimensional inertial features **a** inertial _∈_ R _[n]_

separately. These two features are then concatenated together
as **a** = [ **a** visual _,_ **a** inertial ] _∈_ R _[m]_ [+] _[n]_ . Notably, our emission matrix
is defined as identity matrix **H** = **I** _m_ + _n_, when both modalities
are available. If visual or inertial cues are absent, the emission
matrix is changed to **H** = [ **I** _m_ _,_ **0** _m×n_ ] or **H** = [ **0** _n×m_ _,_ **I** _n_ ] .
The training and testing of the visual-inertial dynamic model
follows the same procedures as in visual odometry.
_2) VIO Pose Estimation:_ In this experiment, we adopt
the official KITTI evaluation metric to evaluate our models
and baseline, which is the same as in VO experiments.
Table II reports the RMSE of the translation and orientation
of the proposed DynaNet models, a classical model based
VIO algorithm, i.e. VINS-Mono [ 39 ] and a learning based
approach, i.e. VINet [ 12 ]. Due to the problem of loosely timesynchronization between visual and inertial sensors in KITTI
dataset, the performance of VINS-mono is not as good as
learning-based methods. This supports the claim that learningbased approaches perform more robustly than hand-designed
systems. From Table II, our proposed models outperform
VINet with 2-layers LSTMs, when given both visual and
inertial observations. VINet shares the same framework and

hyperparameters as our models, except that it uses LSTM
rather than a differentiable Kalman filtering. Our deterministic
DynaNet (Ours (Deter.)) further reduces the RMSE of VINet
from 6.44% to 5.47% in translation, and from 1.70 _[◦]_ to 1.63 _[◦]_ .
This demonstrates that our proposed models excel at fusing
multiple sensor modalities for more accurate state estimates
than the LSTM based VIO model.


_D. Motion Prediction_


In this experiment, we show the evaluation of DynaNet
models on pose prediction that offers pose without sensor data
(i.e. future states prediction).
_1) VO Pose Prediction:_ We first fed VO neural models a
sequence of 5 images for initialisation, and then let them predict
the next 5 and 10 states without any further observations (i.e.
trajectory prediction). All models including baselines were
trained on the training set of the KITTI dataset, and tested
on the sub-sequences of 10 or 15 frames, generated from the
testing set. In order to compare with LSTM baselines fairly,
the structures and hyperparameters of baseline models are kept
the same as our models except the DynaNet module.
Table IV illustrates the quantitative results of our approaches,
comparing with LSTM based and “LSTM+Attention” based
models. We report the RMSE of relative positions of the next 5
and 10 steps predicted by neural networks. As shown in Table


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 9



492


490


488


486


484















180


178


176


174


172


170


168


166


164


162










|Col1|Col2|
|---|---|
|10 Steps Prediction||
|Start Predicting||
|Start Point<br>Driving Dire|Ground Truth<br>Ours (Dirichlet)<br>ction|


|Ground|Truth|Col3|Col4|
|---|---|---|---|
|Ours (D<br>Ours (D<br>LSTM 1<br>LSTM 2<br>|richlet)<br>eterministic)<br>-Layer<br>-Layers<br>|||
|Drivin|g Direction|10 Steps P|rediction|
|||Start Poi<br>Start|nt<br>Predicting|


|Driving Direction|Col2|Col3|Col4|
|---|---|---|---|
|||10 Steps|Prediction|
|||Start Point<br>Start Predicting|Ground Truth<br>Ours (Dirichlet)|



-138 -137.5 -137 -136.5
East (m)


(c) Turning pred. with inertial data in the
good case


(f) Turning pred. with inertial data in the bad

case



180


175


170


165


160



278 280 282 284 286 288 290 292
East(m)


(a) Line prediction in the good case



-139 -138.5 -138 -137.5 -137 -136.5

East (m)


(b) Turning pred. w/o inertial data in the good

case



52


50


48


46


44


42


40


38


36






|Col1|Start Point|
|---|---|
|Start Predicti|ng|
|10 Steps Prediction||
||Driving Direction|
|||
||Ground Truth|
||Ours (Dirichlet)<br>Ours (Deterministic)|
||LSTM 1-Layer<br>LSTM 2-Layers|



92 94 96 98 100 102 104 106 108
East (m)


(d) Line prediction in the bad case (e) Turning pred. w/o inertial data in the bad

case



Fig. 6: For future poses prediction without observations, our Dirichlet based model clearly outperforms others when predicting
the straight driving. In both (a) the good case and (d) the bad case, the predicted locations in future steps from our proposed
Dirichlet based DynaNet model are closer to the groundtruth, compared with other baselines. In turning, the future poses are
estimated in a tangent direction with or without the aid of inertial data (b) (c) in good case and (e) (f) bad case.



10


8





6


4


2


0

|Ours (Dirichlet)<br>Ours (Deterministic)|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||


2 5 8 11 14
Timestep


Fig. 7: The error bar of line predictions


IV, it is clear that our proposed models perform better than
both LSTM based and “LSTM+Attention” based models in

visual egomotion prediction. Especially, our Dirichlet model
outperforms others by a large margin. This is because the
resampled transition matrix from the Dirichlet distribution
ensures the learned dynamical model to be stable, and hence
gives rise to long-term prediction in higher accuracy.

We then demonstrate the qualitative results of our methods.
As straight driving routes dominate driving behaviours for
autonomous vehicles, we thus begin our performance report
with them first. Figure 6a and 6d show the best case and worst
case of line prediction. Clearly, in both cases, the predicted



trajectories from our proposed Dirichlet model are much closer
to the groundtruth trajectories. Without observations, in the best
case it can even provide the same predictions (the small circles
in Figure 6a) as groundtruth values (the small crosses). Figure
7 plots the error bar of predicted locations for straight driving,
with the best and worst results among all tested segments to
show the variance of the model predictions. In both cases,
our proposed Dirichlet based model consistently outperforms
other baselines by providing more accurate and robust location
predictions.

Figure 6b and 6e further show the prediction performance in
turning. As we can see, without timely observation, it is hard for
deep neural networks to estimate accurate orientation changes,
but they predict the future poses in a tangent direction in both
good and bad cases. In the bad case, the motion predictions are
relatively more far away from the ground-truth. We will soon
show how to integrate inertial information to aid the turning
prediction in VIO pose prediction.

_2) VIO Pose Prediction:_ We also evaluate our models
on pose prediction using visual and inertial sensor data.
This experiment is conducted in scenarios of prediction with
visual-only observations, inertial-only observations and no
observations, in which all models are given a sequence of
5 images for initialisation and need to predict the next 5 poses.

As can be seen in Table III, our models including deterministic and Dirichlet approaches, greatly outperform the


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 10





Fig. 8: Sample images from the generated sub-sequences degraded with increasing size of blanked block in the interpretability
experiment.



1 2 3 4 5 6 7 8 9 10 11 12 13 14


Time Step


(b) Observation Noise


1 2 3 4 5 6 7 8 9 10 11 12 13 14


Time Step


(d) Residual Noise



5.9


5.7


5.5


5.3


5.1


4.9


4.7


29


27


25


23


21


19


17



With Degradation Without Degradation


1 2 3 4 5 6 7 8 9 10 11 12 13 14


Time Step


(a) Kalman Gain



12


10


8


6


4


2


13

12

11

10

9

8

7

6

5





Fig. 9: Model Interpretation. The Kalman gain (a) reflects the measurement qualities which decreases with the rising degree of
data corruption. The increasing observation noise (b) and residual noise (d) reflect the rise of observation uncertainty. The
decreasing process noise (d) indicates that the model places more trust in process prediction when observations are uncertain.



no input is available, although attention mechanism improves
the prediction performance over LSTM, our DynaNet still
outperforms this “LSTM+attention” baseline. Specifically, the
Dirichlet model shows better performance than all other
approaches. This is consistent with the results in visual pose
prediction, and validates that the model stability will allow
more accurate long term prediction.


Figure 6c and 6f plot the predicted trajectories with only
inertial data, when no visual observation is given. Clearly, in
the good case our models are capable of predicting future pose
evolution accurately and robustly. We note that robust fusion
is important for safe operation with missing sensor inputs e.g.
for self-driving cars. In the bad case, though the results are
not desirable, the motion predictions still indicate the tendency



of turning.


_E. Towards Model Interpretability_


We are now in a position to discuss model interpretability.
Recall that in the update process (see Section III-C ), the Kalman
gain is an adaptive weight that balances the observations
and the model predictions. If there is a high confidence in
measurements, the Kalman gain will increase to selectively
upweight measurement innovation and vice versa. This property
gives us a unique opportunity to analyse model behaviours
from the value changes of the Kalman Gain.
To this end, we deliberately fed our Dirichlet model with
degraded images and use Kalman gain to capture the belief
in measurements. This experiment generated 113 sequences
with 15 frames of images from Sequence 09 in KITTI dataset.


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 11



For data degradation, a block was blanked on a sequence
of 15 frames of images. The size of blank blocks gradually
increases as time evolves, until all pixels on an image are
blank. Specifically, as shown in Figure 8, in each sequence,
the images are corrupted with an increasing size of blanked
blocks on the timesteps 1-5 (no blanked block), 6-7 (a blanked
block with 192 pixels _×_ 192 pixels), 8-9 (a blanked block with
192 pixels _×_ 320 pixels), 10-11 (a blanked block with 192
pixels _×_ 480 pixels), 12-13 (a blanked block with 192 pixels
_×_ 520 pixels), 14-15 (a blanked block with 192 pixels _×_ 640
pixels). The position of the blanked block is randomly selected
on each image. We then test our model with this modified
dataset in the same fashion as the VO experiment described in
Section IV-B.

The Frobenius norm (2-norm) of the Kalman gain matrix is
calculated, and averaged across all sequences as an aggregated
indicator of changes in Kalman gain matrix. Figure 9a shows
that compared to the case with no degradation, this indicator
gradually decreases with growing data corruption. It implies that
our model can adaptively place more trust on model predictions
when observing low-quality data and signal to higher control
layers that estimation is becoming more uncertain. Similarly, we
further visualize other explicit parameters inside our DynaNet
model, i.e. the observation noise matrix **R**, process noise matrix
**Q** and residual noise matrix **r** . As shown in Figure 9b, 9c
and 9d, these parameters are also able to capture and reflect
the model uncertainty: with respect to the increasing data
corruption, the observation noise rises up as well, indicating
that the model are more uncertain about the observations. On

the contrary, the process noise goes down, showing that the
model has to place more belief in prediction process when the
observations are uncertain. Last but not least, the increasing
residual noise also aligns with the uncertainty in observations.
_It is critical to note that data corrupted in this way have never_
_been seen in the training phase._


V. C ONCLUSION


DynaNet, a neural Kalman dynamical model was introduced
in this paper to learn temporary linear-like structure on latent
states. Through deeply coupled DNNs and SSMs, DynaNet can
scale to high-dimensional data as well as model very complex
motion dynamics in real world. By using Kalman filter on
feature space, DynaNet is able to reason about latent system
states, allowing reliable inference and predictions even with
missing observations. Furthermore, the transition matrix in our
model is sampled from the Dirichlet distribution learned by a
RNN, which ensures system stability in the long run. DynaNet
is evaluated on a variety of challenging motion-estimation tasks,
including single-modality estimation under data corruption,
multiple sensor fusion under data absence, and future motion
prediction. Experimental results demonstrate the superiority of
our approach in accuracy, robustness and interpretability.
In the future work, it would be interesting to further explore
the application of proposed DynaNet model in motion prediction, e.g. evaluating and visualizing future steps predictions
in various environments, and measuring the robustness and
reliability of a learned stable dynamical model compared with
an unstable model.



A PPENDIX A

T HE IMPLEMENTATION DETAILS OF D YNA N ET MODELS


This appendix illustrates the implementation details of the
experiments in visual odometry and visual-inertial odometry.
All the models are trained with 100 epochs.
**Visual Odometry:** Table V reports the framework for the
visual odometry, consisting of Visual Encoder to extract features
**a** and observation noise matrix **Q**, Neural Transition Models
(Deterministic or Resampled) to generate transition matrix
**A** and process noise matrix **R**, the Kalman Filter pipeline to
predict and update system states **z**, and Pose Predictor to output
6-states poses ([Translation, Euler angles]) from systems states.
Specifically, our visual encoder used the encoder structure of
the FlowNetS architecture.


Visual Encoder

[ input ] Two stacked images: _B ×_ 640 _×_ 192 _×_ 6

[ layer 1 ] Conv. 7 [2], Stride 2 [2], Padding 3, LeakyReLU activ.

[ layer 2 ] Conv. 5 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 3 ] Conv. 5 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 3 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 4 ] Conv. 3 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 4 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 5 ] Conv. 3 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 5 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 6 ] Conv. 3 [2], Stride 2 [2], Padding 1

[ layer 7 1 ] FC 128

[ layer 7 2 ] FC 128, ReLU

[ output1 ] Observation **a** : _B ×_ 128 (layer 7 1)

[ output2 ] Observation Noise Covariance **R** : _B ×_ 128 (layer 7 2)


Neural Transition Generation - Deterministic

[ input ] Latent States _z_ : _B ×_ 1 _×_ 128

[ layer 1 ] LSTM, 1-layer, hidden size 128

[ layer 2 1 ] FC 128

[ layer 2 2 ] FC 128, ReLU

[ output 1 ] Transition Matrix **A** : _B ×_ 128 (layer 2 1)

[ output 2 ] Process Noise Covariance **Q** : _B ×_ 128 (layer 2 2)


Neural Transition Generation - Resampled

[ input ] Latent States z: _B ×_ 1 _×_ 128

[ layer 1 ] LSTM, 1-layer, hidden size 128

[ layer 2 1 ] FC 128, ReLU

[ layer 2 2 ] FC 128, ReLU

[ layer 3 ] Dirichlet Distribution

[ output 1 ] Transition Matrix **A** : _B ×_ 128 (layer3)

[ output 2 ] Process Noise Covariance **Q** : _B ×_ 128 (layer 2 2)


Kalman Filter Pipeline

[ input1 ] Previous Latent States **z** _t−_ 1 : _B ×_ 128

[ input2 ] Previous State Covariance **P** _t−_ 1 : _B ×_ 128

[ input3 ] Transition **A** and Process Noise **Q** : _B ×_ 128, _B ×_ 128

[ input4 ] Observation **a** and Observation Noise **R** : _B ×_ 128, _B ×_ 128

[ layer 1 ] Kalman Filter

[ output 1 ] Current Latent States **z** _t_ : _B ×_ 128

[ output 2 ] Current State Covariance **P** _t_ : _B ×_ 128


Pose Predictor

[ input ] Current Latent States **z** _t_ : _B ×_ 128

[ layer 1 1 ] FC 3

[ layer 1 2 ] FC 3

[ layer 4 ] (layer 1 1) _⊕_ (layer 1 2)

[ output ] Poses: _B ×_ 6


TABLE V: Implementation details for the Visual Odometry
Experiment.


**Visual-Inertial Odometry** As can be seen in Table VI, the
framework for visual-inertial odometry used the same Neural
Transition Model, Kalman Filter, and Pose Predictor as in visual


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 12



odometry, except the encoders and sensor fusion part. Table 2
shows that the 64-dimensional visual and inertial features are

extracted from data by visual and inertial encoders respectively,
and concatenated as a 128-dimensional observation in Sensor

Fusion. The inertial encoder used 1-layer Bi-directional LSTM
to process inertial data.


Visual Encoder

[ input ] Two stacked images: _B ×_ 640 _×_ 192 _×_ 6

[ layer 1 ] Conv. 7 [2], Stride 2 [2], Padding 3, LeakyReLU activ.

[ layer 2 ] Conv. 5 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 3 ] Conv. 5 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 3 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 4 ] Conv. 3 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 4 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 5 ] Conv. 3 [2], Stride 2 [2], Padding 2, LeakyReLU activ.

[ layer 5 1 ] Conv. 3 [2], Stride 1 [2], Padding 1

[ layer 6 ] Conv. 3 [2], Stride 2 [2], Padding 1

[ layer 7 1 ] FC 64

[ layer 7 2 ] FC 64, ReLU

[ output1 ] Visual Feature **a** _v_ : _B ×_ 64 (layer 7 1)

[ output2 ] Visual Noise Covariance **R** : _B ×_ 64 (layer 7 2)


Inertial Encoder

[ input ] IMU sequence: _B ×_ 10 _×_ 6

[ layer 1 ] FC 32

[ layer 2 ] B-LSTM, 2-layers, hidden size 32

[ layer 2 1 ] FC 64

[ layer 2 2 ] FC 64, ReLU

[ output1 ] Inertial Feature **a** _i_ : _B ×_ 64 (layer 2 1)

[ output2 ] Inertial Noise Covariance **R** : _B ×_ 64 (layer 2 2)


Sensor Fusion

[ input1 ] Visual Feature **a** _v_ : _B ×_ 64

[ input2 ] Inertial Feature **a** _i_ : _B ×_ 64

[ layer 1 ] (input1 _⊕_ input2)

[ output ] Observation **a** : _B ×_ 128


TABLE VI: Implementation details for Visual Inertial Odometry.
_⊕_ denotes a concatenation operation.


R EFERENCES


[1] N. Sunderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, ¨
B. Upcroft, P. Abbeel, W. Burgard, M. Milford, and P. Corke, “The
Limits and Potentials of Deep Learning for Robotics,” _International_
_Journal of Robotics Research_, vol. 37, no. 4-5, pp. 405–420, 2018.

[2] D. Nister, O. Naroditsky, and J. Bergen, “Visual Odometry,” in _IEEE_
_Conference on Computer Vision and Pattern Recognition (CVPR)_, vol. 1,
2004, pp. I–652–I–659 Vol.1.

[3] J. Engel, J. Sturm, and D. Cremers, “Semi-Dense Visual Odometry for
a Monocular Camera,” in _IEEE International Conference on Computer_
_Vision (ICCV)_, 2013, pp. 1449–1456.

[4] C. Forster, M. Pizzoli, and D. Scaramuzza, “SVO: Fast Semi-Direct
Monocular Visual Odometry,” in _IEEE International Conference on_
_Robotics and Automation (ICRA)_, 2014, pp. 15–22.

[5] R. E. Kalman, “A New Approach to Linear Filtering and Prediction
Problems,” _Journal of Basic Engineering_, vol. 82, no. 1, p. 35, 1960.

[6] S. G. Mohinder and P. A. Angus, “Applications of Kalman Filtering in
Aerospace 1960 to the Present,” _IEEE Control Systems Magazine_, pp.
69–78, 2010.

[7] J. S. Liu and R. Chen, “Sequential monte carlo methods for dynamic
systems,” _Journal of the American Statistical Association_, vol. 93, no.
443, pp. 1032–1044, 1998.

[8] R. Kummerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard, “g2o: ¨
A General Framework for Graph Optimization,” in _IEEE International_
_Conference on Robotics and Automation (ICRA)_, 2011.




[9] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen, “VidLoc: A
Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization,” in
_IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_,
2017.

[10] S. Wang, R. Clark, H. Wen, and N. Trigoni, “DeepVO : Towards Endto-End Visual Odometry with Deep Recurrent Convolutional Neural
Networks,” in _IEEE International Conference on Robotics and Automa-_
_tion (ICRA)_, 2017.

[11] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised Learning
of Depth and Ego-Motion from Video,” in _IEEE Conference on Computer_
_Vision and Pattern Recognition (CVPR)_, 2017.

[12] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “VINet:
Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem,”
in _Association for the Advancement of Artificial Intelligence (AAAI)_,
2017, pp. 3995–4001.

[13] P. Mirowski, M. K. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin, K. Simonyan, K. Kavukcuoglu, A. Zisserman, and
R. Hadsell, “Learning to Navigate in Cities Without a Map,” in _Advances_
_in Neural Information Processing Systems (NIPS)_, 2018.

[14] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison,
“CodeSLAM — Learning a Compact, Optimisable Representation for
Dense Visual SLAM,” in _IEEE Conference on Computer Vision and_
_Pattern Recognition (CVPR)_, 2018.

[15] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz, “Geometry-Aware
Learning of Maps for Camera Localization,” in _IEEE Conference on_
_Computer Vision and Pattern Recognition (CVPR)_, 2018, pp. 2616–2625.

[16] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. Reid,
“Unsupervised Learning of Monocular Depth Estimation and Visual
Odometry with Deep Feature Reconstruction,” in _IEEE Conference on_
_Computer Vision and Pattern Recognition (CVPR)_, 2018, pp. 340–349.

[[Online]. Available: http://arxiv.org/abs/1803.03893](http://arxiv.org/abs/1803.03893)

[17] Z. Yin and J. Shi, “GeoNet: Unsupervised Learning of Dense Depth,
Optical Flow and Camera Pose,” in _IEEE Conference on Computer Vision_
_and Pattern Recognition (CVPR)_, 2018.

[18] D. G. Dudley, “Dynamic system identification experiment design and
data analysis,” _Proceedings of the IEEE_, vol. 67, no. 7, pp. 1087–1087,
July 1979.

[19] J. Yu, S. Wang, and L. Li, “Incremental design of simplex basis function
model for dynamic system identification,” _IEEE transactions on neural_
_networks and learning systems_, vol. 29, no. 10, pp. 4758–4768, 2017.

[20] J. Kocijan, A. Girard, B. Banko, and R. Murray-Smith, “Dynamic Systems
Identification with Gaussian Processes,” _Mathematical and Computer_
_Modelling of Dynamical Systems_, vol. 11, no. 4, pp. 411–424, 2005.

[21] Z. Ghahramani and S. T. Roweis, “Learning Nonlinear Dynamical
Systems Using an EM Algorithm,” in _Advances in Neural Information_
_Processing Systems (NIPS)_, vol. 11, no. 1, 1999, pp. 431–437.

[22] X. Wang and Y. Huang, “Convergence study in extended kalman filterbased training of recurrent neural networks,” _IEEE Transactions on_
_Neural Networks_, vol. 22, no. 4, pp. 588–600, 2011.

[23] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF: Learning
Discriminative Deterministic State Estimators,” in _Advances in Neural_
_Information Processing Systems (NIPS)_, 2016. [Online]. Available:
[http://arxiv.org/abs/1605.07148](http://arxiv.org/abs/1605.07148)

[24] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable Particle
Filters: End-to-End Learning with Algorithmic Priors,” in _RSS_, 2018.

[25] P. Karkus, D. Hsu, and W. S. Lee, “Particle filter networks with
application to visual localization,” _arXiv preprint arXiv:1805.08975_,
2018.

[26] R. G. Krishnan, U. Shalit, and D. Sontag, “Structured Inference Networks
for Nonlinear State Space Models,” in _Association for the Advancement_
_of Artificial Intelligence (AAAI)_, no. Dmm, 2017, pp. 1–21.

[27] M. Fraccaro, S. K. Sønderby, U. Paquet, and O. Winther, “Sequential Neural Models with Stochastic Layers,” in _Advances in Neural Information_
_Processing Systems (NIPS)_, 2016.

[28] M. Fraccaro, S. Kamronn, U. Paquet, and O. Winther, “A Disentangled
Recognition and Nonlinear Dynamics Model for Unsupervised Learning,”
in _Advances in Neural Information Processing Systems (NIPS)_, 2017.

[29] M. Karl, M. Soelch, J. Bayer, and P. van der Smagt, “Deep Variational
Bayes Filters: Unsupervised Learning of State Space Models from Raw
Data,” in _International Conference on Learning Representations (ICLR)_,
[2017. [Online]. Available: http://arxiv.org/abs/1605.06432](http://arxiv.org/abs/1605.06432)

[30] J. Umlauft and S. Hirche, “Learning Stable Stochastic Nonlinear
Dynamical Systems,” in _International Conference on Machine Learning_
_(ICML)_, 2017, pp. 3502—-3510.

[31] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “MonoSLAM:
Real-Time Single Camera SLAM,” _IEEE Transactions on Pattern_
_Analysis and Machine Intelligence_, vol. 29, no. 6, pp. 1052–1067, 2007.


IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, SEP 2021 13




[32] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “DTAM: Dense
Tracking and Mapping in Real-Time,” in _IEEE International Conference_
_on Computer Vision (ICCV)_, 2011, pp. 2320–2327.

[33] J. Engel, T. Schops, and D. Cremers, “LSD-SLAM: Large-Scale Direct ¨
Monocular SLAM,” in _European Conference on Computer Vision (ECCV)_,
2014.

[34] R. Mur-Artal, J. Montiel, and J. D. Tardos, “ORB-SLAM : A Versatile
and Accurate Monocular SLAM System,” _IEEE Transactions on Robotics_,
vol. 31, no. 5, pp. 1147–1163, 2015.

[35] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza, “IMU
Preintegration on Manifold for Efficient Visual-Inertial Maximum-aPosteriori Estimation,” in _Robotics: Science and Systems_, 2015. [Online].
[Available: http://www.roboticsproceedings.org/rss11/p06.pdf](http://www.roboticsproceedings.org/rss11/p06.pdf)

[36] M. Li and A. I. Mourikis, “High-Precision, Consistent EKF-Based VisualInertial Odometry,” _The International Journal of Robotics Research_,
vol. 32, no. 6, pp. 690–711, 2013.

[37] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust Visual Inertial
Odometry Using a Direct EKF-Based Approach,” in _IEEE International_
_Conference on Intelligent Robots and Systems_, vol. 2015-Decem, 2015,
pp. 298–304.

[38] S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale,
“Keyframe-Based Visual–Inertial Odometry Using Nonlinear Optimization,” _The International Journal of Robotics Research_, vol. 34, no. 3, pp.
314–334, 2015.

[39] T. Qin, P. Li, and S. Shen, “VINS-Mono: A Robust and Versatile
Monocular Visual-Inertial State Estimator,” _IEEE Transactions on_
_Robotics_, vol. 34, no. 4, pp. 1004–1020, 2018.

[40] C. Tang and P. Tan, “BA-Net: Dense Bundle Adjustment Networks,” in
_International Conference on Learning Representations (ICLR)_, 2019.

[41] H. J. Kashyap, C. C. Fowlkes, and J. L. Krichmar, “Sparse representations for object-and ego-motion estimations in dynamic scenes,” _IEEE_
_Transactions on Neural Networks and Learning Systems_, 2020.

[42] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino,
M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, D. Kumaran, and
R. Hadsell, “Learning to Navigate in Complex Environments,” in
_International Conference on Learning Representations (ICLR)_, 2017.

[[Online]. Available: http://arxiv.org/abs/1611.03673](http://arxiv.org/abs/1611.03673)

[43] J. F. Henriques and A. Vedaldi, “MapNet: An Allocentric Spatial Memory
for Mapping Environments,” in _IEEE Conference on Computer Vision_
_and Pattern Recognition (CVPR)_, 2018, pp. 8476–8484.

[44] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik, “Recurrent network
models for human dynamics,” in _Proceedings of the IEEE International_
_Conference on Computer Vision_, 2015, pp. 4346–4354.

[45] J. Martinez, M. J. Black, and J. Romero, “On human motion prediction
using recurrent neural networks,” in _Proceedings of the IEEE Conference_
_on Computer Vision and Pattern Recognition_, 2017, pp. 2891–2900.

[46] Y. Tang, L. Ma, W. Liu, and W.-S. Zheng, “Long-term human motion
prediction by modeling motion context and enhancing motion dynamic,”
in _Proceedings of the 27th International Joint Conference on Artificial_
_Intelligence_, 2018, pp. 935–941.

[47] X. Shu, L. Zhang, G.-J. Qi, W. Liu, and J. Tang, “Spatiotemporal coattention recurrent neural networks for human-skeleton motion prediction,”
_IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.

[48] T. C¸ imen, _State-Dependent Riccati Equation (SDRE) Control: A survey_ .
IFAC, 2008, vol. 17, no. 1 PART 1.

[49] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
_International Conference on Learning Representations (ICLR)_, 2014.

[50] S. S. Rangapuram, M. Seeger, J. Gasthaus, L. Stella, Y. Wang, and
T. Januschowski, “Deep State Space Models for Time Series Forecasting,”
_Advances in Neural Information Processing Systems_, no. NeurIPS, pp.
7795–7804, 2018.

[51] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” _Neural_
_computation_, vol. 9, no. 8, pp. 1735–1780, 1997.

[52] K. Greff, R. K. Srivastava, J. Koutn ´ ık, B. R. Steunebrink, and J. Schmidhuber, “Lstm: A search space odyssey,” _IEEE transactions on neural_
_networks and learning systems_, vol. 28, no. 10, pp. 2222–2232, 2016.

[53] J. Schulman, N. Heess, T. Weber, and P. Abbeel, “Gradient Estimation Using Stochastic Computation Graphs,” in _Advances in Neural Information_
_Processing Systems (NIPS)_, 2015, pp. 1–13.

[54] M. Jankowiak and F. Obermeyer, “Pathwise Derivatives Beyond the
Reparameterization Trick,” in _International Conference on Machine_
_Learning (ICML)_ [, 2018. [Online]. Available: http://arxiv.org/abs/1806.](http://arxiv.org/abs/1806.01851)
[01851](http://arxiv.org/abs/1806.01851)

[55] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision Meets Robotics:
The KITTI Dataset,” _The International Journal of Robotics Research_,
vol. 32, no. 11, pp. 1231–1237, 2013.




[56] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and I. Reid,
“Unsupervised scale-consistent depth and ego-motion learning from
monocular video,” in _Advances in neural information processing systems_,
2019, pp. 35–45.

[57] A. Geiger, J. Ziegler, and C. Stiller, “Stereoscan: Dense 3d reconstruction
in real-time,” in _2011 IEEE intelligent vehicles symposium (IV)_ . Ieee,
2011, pp. 963–968.


**Changhao Chen** is currently a Lecturer at College of
Intelligence Science, National University of Defense
Technology (China). Before that, he obtained his
Ph.D. degree at University of Oxford (UK), M.Eng.
degree at National University of Defense Technology
(China), and B.Eng. degree at Tongji University
(China). His research interest lies in robotics, machine
learning and cyberphysical systems.


**Chris Xiaoxuan Lu** is currently an Assistant Professor at School of Informatics, University of Edinburgh.
Before that, he obtained his Ph.D degree at University
of Oxford, and MEng degree at Nanyang Technology
University, Singapore. His research interest lies in
Cyber Physical Systems, which use networked smart
devices to sense and interact with the physical world.


**Bing Wang** is currently PhD student at Department
of Computer Science, University of Oxford. Before
that, he obtained his BEng Degree at Shenzhen
University, China. His research interest lies in camera
localization, feature detection, description & matching, and cross-domain representation learning.


**Niki Trigoni** is a Professor at the Department of
Computer Science, University of Oxford. She is
currently the director of the EPSRC Centre for Doctoral Training on Autonomous Intelligent Machines
and Systems, and leads the Cyber Physical Systems
Group. Her research interests lie in intelligent and
autonomous sensor systems with applications in
positioning, healthcare, environmental monitoring
and smart cities.


**Andrew Markham** is an Associate Professor at
the Department of Computer Science, University of
Oxford. He obtained his BSc (2004) and PhD (2008)
degrees from the University of Cape Town, South
Africa. He is the Director of the MSc in Software Engineering. He works on resource-constrained systems,
positioning systems, in particular magneto-inductive
positioning and machine intelligence.


