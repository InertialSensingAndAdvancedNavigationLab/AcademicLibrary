Personal use of this material is permitted. Permission from the author(s) and/or copyright holder(s), must be obtained for all other uses. Please contact us and provide details if
you believe this document breaches copyrights.

## DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D Visual-Inertial Navigation based on IMU-Vision-Net


Khashayar Ghanizadegan and Hashim A. Hashim



_**Abstract**_ **—This paper addresses the challenge of estimating**
**the orientation, position, and velocity of a vehicle operating in**
**three-dimensional (3D) space with six degrees of freedom (6-**
**DoF). A Deep Learning-based Adaptation Mechanism (DLAM)**
**is proposed to adaptively tune the noise covariance matrices**
**of Kalman-type filters for the Visual-Inertial Navigation (VIN)**
**problem, leveraging IMU-Vision-Net. Subsequently, an adaptively**
**tuned Deep Learning Unscented Kalman Filter for 3D VIN**
**(DeepUKF-VIN) is introduced to utilize the proposed DLAM,**
**thereby robustly estimating key navigation components, including**
**orientation, position, and linear velocity. The proposed DeepUKF-**
**VIN integrates data from onboard sensors, specifically an inertial**
**measurement unit (IMU) and visual feature points extracted**
**from a camera, and is applicable for GPS-denied navigation.**
**Its quaternion-based design effectively captures navigation non-**
**linearities and avoids the singularities commonly encountered**
**with Euler-angle-based filters. Implemented in discrete space, the**
**DeepUKF-VIN facilitates practical filter deployment. The filter’s**
**performance is evaluated using real-world data collected from**
**an IMU and a stereo camera at low sampling rates. The results**
**demonstrate filter stability and rapid attenuation of estimation**
**errors, highlighting its high estimation accuracy. Furthermore,**
**comparative testing against the standard Unscented Kalman**
**Filter (UKF) in two scenarios consistently shows superior perfor-**
**mance across all navigation components, thereby validating the**
**efficacy and robustness of the proposed DeepUKF-VIN.**


_**Index Terms**_ **—Deep Learning, Unscented Kalman Filter, Adap-**
**tive tuning, Estimation, Navigation, Unmanned Aerial Vehicle,**
**Sensor-fusion.**


[For video of navigation experiment visit: link](https://youtu.be/japfpySxilA)


I. I NTRODUCTION


_A. Motivation_
# N AVIGATION is a fundamental component in the success-ful operation of a wide array of applications, spanning

fields such as robotics, aerospace, and mobile technology. At
its core, navigation involves estimating an object’s position,
orientation, and velocity, a task that becomes particularly critical and challenging in environments where Global Navigation
Satellite Systems (GNSS), like GPS, BeiDou, and GLONASS,
are unavailable (e.g., indoor environments) or unreliable (e.g.,
urban settings with obstructed satellite signals due to tall


This work was supported in part by National Sciences and Engineering
Research Council of Canada (NSERC), under the grants RGPIN-2022-04937
and DGECR-2022-00103.
K. Ghanizadegan and H. A. Hashim are with the Department of Mechanical
and Aerospace Engineering, Carleton University, Ottawa, Ontario, K1S-5B6,
Canada (e-mail: hhashim@carleton.ca).



buildings) [1], [2]. Similar challenges are encountered in
underwater navigation, where robots must operate in deep,
GNSS-denied environments [3]. Unmanned ground vehicles
(UGVs) and unmanned aerial vehicles (UAVs) have shown
immense potential in various sectors. For example, UGVs and
UAVs are increasingly used in care facilities to assist with
monitoring and delivery tasks [4], in logistical services for
autonomous package delivery [5], and in surveillance of hardto-access locations [1]. These include monitoring forests for
early fire detection [6], tracking icebergs in the Arctic [7], and
conducting surveys in other remote areas. The effectiveness
of these applications hinges on the precision and reliability of
their navigation systems. In the realm of mobile technology,
accurate navigation is essential for enhancing user experiences,
particularly in smartphone applications that rely on real-time
positional data, such as augmented reality (AR) platforms
and wayfinding tools [8]. Similarly, in aerospace applications,
obtaining precise positional and orientation data is vital for
the accurate analysis and interpretation of observational information [9], [10].


_B. Related Work_


One of the primary approaches to addressing the challenge
of navigation in GPS-denied environments involves utilizing
ego-acceleration measurements from onboard accelerometers
to estimate a vehicle’s pose relative to its previous position.
This technique, known as Dead Reckoning (DR), integrates
acceleration data to derive positional information [1]. DR offers a straightforward, cost-effective solution, particularly with
low-cost sensors, making it accessible for many applications

[11]. To enhance the accuracy of Dead Reckoning, a gyroscope
is often incorporated to measure the vehicle’s angular velocity.
This integration provides additional orientation data, improving the overall pose estimation. However, a significant drawback of this method is its susceptibility to cumulative errors or
drift over time. Without supplementary sensors or correction
mechanisms, these errors can accumulate rapidly, leading
to inaccurate navigation results, especially during prolonged
use. In controlled environments like harbors, warehouses, or
other predefined spaces, ultra-wideband (UWB) technology
can significantly enhance navigation accuracy. UWB systems
measure distances between the vehicle and fixed reference
points, known as anchors, providing highly accurate and robust
localization data [12]. This approach is widely adopted in
applications where precision is paramount, such as robotic



**K. Ghanizadegan and H. A. Hashim, ”DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D Visual-Inertial Navigation based on**
**IMU-Vision-Net,” Expert Systems With Applications, vol. 271, pp. 126656, 2025.** [doi: 10.1016/j.eswa.2025.126656](https://doi.org/10.1016/j.eswa.2025.126656)


operations within structured environments and object-tracking
systems like Apple’s AirTag [13]. When used alongside an Inertial Measurement Unit (IMU), accelerometer, and gyroscope
within a DR framework, UWB can serve as an additional
sensor to correct positional errors and mitigate drift [12].
However, this solution has limitations since UWB requires
the installation of anchors in the environment, which confines
its applicability to pre-configured spaces. As a result, it may
not be suitable for dynamic or unstructured environments,
reducing the system’s flexibility and immediate usability out of
the box [14]. Additionally, UWB is susceptible to high levels
of noise, which can degrade estimation accuracy [12].
With the development of advanced point cloud registration
algorithms such as Iterative Closest Point (ICP) [15] and
Coherent Point Drift (CPD) [16], sensors capable of capturing two-dimensional (2D) points from three-dimensional
(3D) space have emerged as promising candidates to complement IMUs without requiring prior environmental knowledge. Sound Navigation and Ranging (SONAR) is one such
sensor, widely adopted in marine applications due to its
effectiveness in underwater environments, where mechanical
waves propagate efficiently [17]. Similarly, Light Detection
and Ranging (LiDAR) employs electromagnetic waves instead of mechanical waves and has demonstrated utility in
aerospace applications, where sound propagation is limited,
but light transmission is effective [18]. However, both SONAR
and LiDAR exhibit significant limitations in complex indoor
and outdoor environments, as they rely solely on structural
properties and cannot capture texture or color information.
In contrast, recent advancements in low-cost, high-resolution
cameras designed for navigation applications, combined with
robust fusion between IMU and feature detection [2], [19].
Popular tracking feature detection-based algorithms include
Scale-Invariant Feature Transform (SIFT) [20], Good Features
to Track (GFTT) [21], and the Kanade-Lucas-Tomasi (KLT)
algorithm have facilitated the widespread adoption of cameras
as correction sensors alongside IMUs [1], [22]–[24].


_C. Persistent Challenges and Potentials_


To integrate the aforementioned sensor data, Kalman-type
filters are widely employed in navigation due to their stochastic
framework and ability to handle noisy measurements [25]–

[28]. The Kalman Filter (KF) provides a maximum likelihood estimate of the system’s state vector based on available
measurement data; however, it operates optimally only within
linear systems. To overcome this limitation, the Extended
Kalman Filter (EKF) was developed. The EKF linearizes the
system around the current estimated state vector and applies
the KF framework to this linearized model. Its intuitive struc
ture, ease of implementation, and computational efficiency
have established the EKF as a standard choice for navigation
applications [16], [23], [29]. However, the EKF’s performance
degrades with increasing system nonlinearity. To address the
EKF’s limitations, the Unscented Kalman Filter (UKF) was
introduced. The UKF effectively captures the propagation
of mean and covariance through a nonlinear transformation
up to the second order, offering improved accuracy while



2


maintaining comparable computational complexity to the EKF

[26], [30]. Nevertheless, Kalman-type filters rely on accurate
modeling of system and measurement noise. While it is
standard to assume these noise components are zero-mean,
their covariance matrices serve as critical tuning parameters,
and the performance of these filters is sensitive to inaccuracies
in their specification [31]. In practice, determining the values
of covariance matrices is challenging and typically involves
an iterative process of trial and error, which can be both timeconsuming and effort-intensive.
Deep learning techniques have shown significant promise
in adaptively tuning the covariance matrices of Kalman-based
filters, addressing a critical challenge in achieving accurate
state estimation [32]–[35]. These methods offer an efficient
alternative to traditional manual tuning, leveraging data-driven
models to dynamically estimate noise parameters based on
observed system behavior. For instance, Brossard et al. [32]
utilized Convolutional Neural Networks (CNNs) to predict
measurement noise parameters for the DR of ground vehicles
using an Invariant EKF (IEKF). This approach improved noise
estimation by learning from raw sensor data, enhancing overall
navigation accuracy. Similarly, Or et al. [34] applied deep
learning to model trajectory uncertainty by extracting features
such as vehicle speed and path curvature demonstrating the
potential to enhance state predictions by accurately capturing
the system’s dynamic characteristics. Furthermore, Yan et al.

[35] proposed a multi-level framework where the state vector
estimates and covariance predictions from traditional filters
serve as inputs to deep learning architectures. Therefore, deep
learning can be employed to iteratively refine the covariance estimates, improving robustness in complex scenarios
allowing Kalman-based filters to dynamically adjust varying
noise conditions, reducing dependency on intensive manual
tuning and significantly improving performance in real-world
applications.


_D. Contributions_


Motivated by the above discussion, the key contributions
of this work are as follows: (1) The proposed approach
employs singularity-free quaternion dynamics to represent ego
orientation, ensuring robust handling of orientation estimation and avoiding singularities typically encountered with
Euler-angle-based representations. (2) A quaternion-based,
adaptively-tuned Deep Learning Unscented Kalman Filter for
3D Visual-Inertial Navigation (DeepUKF-VIN) based on Deep
Learning-based Adaptation Mechanism (DLAM) is formulated
in discrete form. This approach accurately models the true
navigation kinematics, simplifies the implementation process,
and dynamically estimates the covariance matrices, thereby
enhancing the overall performance of Kalman-type filters.
(3) A novel deep learning-based adaptation mechanism is
introduced to dynamically estimate the covariance matrices associated with the measurement noise vectors in the UKF. This

adaptive approach enhances the filter’s estimation performance
by reducing dependency on manual tuning. (4) The proposed
DeepUKF-VIN demonstrates superior performance compared
to the standard UKF across various scenarios. DeepUKF-VIN


3



TABLE I: Nomenclature


_{B}_ / _{W}_ : Fixed body-frame / fixed world-frame


SO (3) : Special Orthogonal Group of order 3


S [3] : Three-unit-sphere


_q_ _k_ _,_ ˆ _q_ _k_ : True and estimated quaternion at step _k_


_p_ _k_ _,_ ˆ _p_ _k_ : True and estimated position at step _k_


_v_ _k_ _,_ ˆ _v_ _k_ : True and estimated linear velocity at step _k_


_r_ _e,k_, _p_ _e,k_, _v_ _e,k_ : Attitude, position, and velocity estimation error


_a_ _k_ _, a_ _m,k_ : True and measured acceleration at step _k_


_ω_ _k_ _, ω_ _m,k_ : True and measured angular velocity at step _k_


_η_ _ω,k_ _, η_ _a,k_ : Angular velocity and acceleration measurements
noise


_b_ _ω,k_ _, b_ _a,k_ : Angular velocity and acceleration measurements
bias


_C_ _×_ : Covariance matrix of _n_ _×_ .


_l_ _b,k_ _, l_ _b,w_ : landmark coordinates in _{B}_ and _{W}_ .


_x_ _k_, _x_ _[a]_ _k_ [,] _[ u]_ _[k]_ : The state, augmented state, and input vectors at
the _k_ th time step


_z_ ˆ _k_ _, z_ _k_ : Predicted and true measurement



(1)


The orientation resulting from two subsequent rotations _q_ 1 =

[ _q_ _w_ 1 _, q_ _v_ 1 ] _[⊤]_ _∈_ S [3] and _q_ 2 = [ _q_ _w_ 2 _, q_ _v_ 2 ] _[⊤]_ _∈_ S [3] is defined through
quaternion multiplication, denoted by the _⊗_ operator [36]:


_q_ 3 = _q_ 1 _⊗_ _q_ 2



_A. Preliminary_


The matrix _R ∈_ R [3] _[×]_ [3] represents the vehicle’s orientation,
provided it belongs to the Special Orthogonal Group of order
3, denoted SO(3), which is defined by:


SO(3) := � _R ∈_ R [3] _[×]_ [3] [��] _det_ ( _R_ ) = +1 _, RR_ _[⊤]_ = **I** 3 �


A quaternion vector _q_ is defined in the scalar-first format
as _q_ = [ _q_ _w_ _, q_ _x_ _, q_ _y_ _, q_ _z_ ] _[⊤]_ = [ _q_ _w_ _, q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] [ with] _[ q]_ _[v]_ _[∈]_ [R] [3] [,]
_q_ _w_ _∈_ R, and S [3] := _{_ _q ∈_ R [4] [��] _||q||_ = 1 _}_ . To obtain the
quaternion representation corresponding to a rotation matrix







, the mapping _q_ _R_ : SO(3) _→_




_R_ =







_R_ (1 _,_ 1) _R_ (1 _,_ 2) _R_ (1 _,_ 3)

 _R_ (2 _,_ 1) _R_ (2 _,_ 2) _R_ (2 _,_ 3)

 _R_ (3 _,_ 1) _R_ (3 _,_ 2) _R_ (3 _,_ 3)



S [3] is defined as [36]:



12 ~~�~~ 1 + _R_ (1 _,_ 1) + _R_ (2 _,_ 2) + _R_ (3 _,_ 3)
1
4 _q_ _w_ [(] _[R]_ [(3] _[,]_ [2)] _[ −]_ _[R]_ [(2] _[,]_ [3)] [)]
1
4 _q_ _w_ [(] _[R]_ [(1] _[,]_ [3)] _[ −]_ _[R]_ [(3] _[,]_ [1)] [)]
1
4 _q_ _w_ [(] _[R]_ [(2] _[,]_ [1)] _[ −]_ _[R]_ [(1] _[,]_ [2)] [)]


















=




_q_ _R_ ( _R_ ) =









_q_ _w_

_q_ _x_

_q_ _y_

_q_ _z_



_{χ_ _i|j_ _}_ _ν_,
_{χ_ _[a]_ _i|j_ _[}]_ _[ν]_ [,]
_{ζ_ _i|j_ _}_ _ν_



: Sigma points of state, augmented state, and

measurements



_∈_ S [3] (2)
�



effectiveness is validated using real-world data collected from
low-cost sensors operating at low sampling rates. To the best of
the authors’ knowledge, no deep learning-enhanced Kalmantype filter based on inertial measurement and vision units has
been proposed for VIN.


_E. Structure_


The structure of the paper is organized as follows: Section
II introduces the preliminary concepts and mathematical foundations. Section III defines the nonlinear navigation kinematics problem. Section IV presents the quaternion-based UKF
framework tailored for navigation kinematics. Section V provides a detailed description of the deep learning architecture
for adaptive tuning. Section VI outlines the training process
and implementation methodology of the proposed DeepUKFVIN. Section VII evaluates the performance of the DeepUKFVIN algorithm using a real-world dataset. Finally, Section VIII
offers concluding remarks.


II. P RELIMINARIES


_Notation:_ In this paper, the set of _d_ 1 -by- _d_ 2 matrices of
real numbers is denoted by R _[d]_ [1] _[×][d]_ [2] . A vector _v ∈_ R _[d]_ is said
to lie on the _d_ -dimensional sphere S _[d][−]_ [1] _⊂_ R _[d]_ when its norm,
denoted as _∥m∥_ = _√m_ _[⊤]_ _m ∈_ R, is equal to one. The identity

matrix of dimension _d_ is denoted by **I** _d_ _∈_ R _[d][×][d]_ . The world
frame _{W}_ and the body frame _{B}_ refer to the coordinate
systems attached to the Earth and the vehicle, respectively.
Table I lists a summary of notations heavily used in this paper.







= _q_ _w_ 1 _q_ _w_ 2 _−_ _q_ _v_ _[⊤]_ 1 _[q]_ _[v]_ [2]
� _q_ _w_ 1 _q_ _v_ 2 + _q_ _w_ 2 _q_ _v_ 1 + [ _q_ _v_ 1 ] _×_ _q_ _v_ 2



(6)
1
_u_ _R_ ( _R_ ) =
sin( _θ_ _R_ ( _R_ )) [vex(] _[P]_ _[a]_ [(] _[R]_ [))] _[ ∈]_ [S] [2]



The orientation identical in terms of unit quaternion is _q_ _I_ =

[1 _,_ 0 _,_ 0 _,_ 0] _[⊤]_ . For _q_ = [ _q_ _w_ _, q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] [, the inverse of] _[ q]_ [ is given]
by _q_ _[−]_ [1] = [ _q_ _w_ _, −q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] [. It is worth noting that] _[ q][ ⊗]_ _[q]_ _[−]_ [1] [ =]
_q_ _I_ . For _m ∈_ R [3], the skew-symmetric matrix [ _m_ ] _×_ is defined

as:



_m_ 1

 _m_ 2

 _m_ 3



 _∈_ so(3) _,_ _m_ =












[ _m_ ] _×_ =



0 _−m_ 3 _m_ 2

 _m_ 3 0 _−m_ 1

 _−m_ 2 _m_ 1 0



The mapping from quaternion _q_ = [ _q_ _w_ _, q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] [ to rotation]
matrix _R_ _q_ _∈_ SO(3) is defined by [36]:


_R_ _q_ ( _q_ ) = ( _q_ _w_ [2] _[−∥][q]_ _[v]_ _[∥]_ [2] [)] _[I]_ [3] [+ 2] _[q]_ _[v]_ _[q]_ _v_ _[⊤]_ [+ 2] _[q]_ _[w]_ [[] _[q]_ _[v]_ []] _[×]_ (3)


The inverse of the skew-symmetric matrix functionis given by:


vex([ _m_ ] _×_ ) = _m ∈_ R [3] (4)


Let _P_ _a_ ( _·_ ) : R [3] _[×]_ [3] _→_ so(3) be anti-symmetric projection
operator where

_P_ _a_ ( _M_ ) = [1] (5)

2 [(] _[M][ −]_ _[M]_ _[ ⊤]_ [)] _[ ∈]_ [so][(3)] _[,][ ∀][M][ ∈]_ [R] _[m][×][m]_


The orientation of a rigid body can also be represented by
a rotation angle _θ ∈_ R around a unit vector _u ∈_ S [2] _⊂_ R [3]

with S [2] := _{_ _u ∈_ R [3] [��] _||u||_ = 1 _}_ . Angle-axis parametrization
is obtained from the rotation matrix _R ∈_ SO(3), where [36]:
 _θ_ _R_ ( _R_ ) = arccos Tr( _R_ ) _−_ 1 _∈_ R

� 2 �

(6)

 1



Tr( _R_ ) _−_ 1
_θ_ _R_ ( _R_ ) = arccos
� 2



_∈_ R
�



2


4



with Tr( _·_ ) denoting the trace function. Let the rotation vector
_r_ be described via the angle-axis parametrization as follows:


_r_ = _r_ _θ,u_ ( _θ, u_ ) = _θu ∈_ R [3] _,_ _∀θ ∈_ R _, u ∈_ S [2] (7)


The rotation matrix associated with a rotation vector is given
by [36]:


_R_ _r_ ( _r_ ) = exp([ _r_ ] _×_ ) _∈_ SO(3) (8)


The mapping from rotation vector representation to quaternion
representation is found by utilizing (1), (6), and (7) such that
_q_ _r_ : R [3] _→_ S [3] :


_q_ _r_ ( _r_ ) = _q_ _R_ ( _R_ _r_ ( _r_ )) _∈_ S [3] (9)


The rotation vector corresponding to a rotation represented by
a quaternion is found in light of (3), (6), and (7) by _r_ _q_ : S [3] _→_
R [3] such that


_r_ _q_ ( _q_ ) = _r_ _θ,u_ ( _θ_ _R_ ( _R_ _q_ ( _q_ )) _, u_ _R_ ( _R_ _q_ ( _q_ ))) (10)


To facilitate addition ⊞ and subtraction ⊟ between a rotation
vector _r ∈_ R [3] and a quaternion _q ∈_ S [3], using the definitions
in (4), (7), and (9), the following operations are defined:


_q_ ⊞ _r_ := _q_ _r_ ( _r_ ) _⊗_ _q ∈_ S [3] (11)

_q_ ⊟ _r_ := _q_ _r_ ( _r_ ) _[−]_ [1] _⊗_ _q ∈_ S [3] (12)


In light of (3), the subtraction of two quaternions _q_ 1 _, q_ 2 _∈_ S [3]

is given by:


_q_ 1 ⊟ _q_ 2 := _r_ _q_ ( _q_ 1 _⊗_ _q_ 2 _[−]_ [1] [)] _[ ∈]_ [R] [3] (13)


Consider a set of quaternions _Q_ = _{q_ _i_ _∈_ S [3] _}_ and their
corresponding weights _W_ = _{w_ _i_ _∈_ R _}_ . To compute the
weighted average of these quaternions, the matrix _E_ is first
constructed as:


_E_ = � _w_ _i_ _q_ _i_ _q_ _i_ _[⊤]_ _[∈]_ [R] [4] _[×]_ [4]


Next, the quaternion weighted mean QWM( _Q, W_ ) is the
eigenvector corresponding to the largest magnitude eigenvalue
of _E_ such that:


QWM( _Q, W_ ) = EigVector( _E_ ) _i_ _∈_ S [3] (14)


where _i_ = argmax( _|_ EigValue( _E_ ) _i_ _|_ ) _∈_ R. A _d_ -dimensional
random variable (RV) _h ∈_ R _[d]_ drawn from a Gaussian
distribution with a mean _h ∈_ R _[d]_ and a covariance matrix
_C_ _h_ _∈_ R _[d][×][d]_ is represented by the following:


_h ∼N_ ( _h, C_ _h_ )


Note that the expected value of _h_, denoted by E( _h_ ), is equal
to _h_ . The Gaussian (Normal) probability density function of
_h_ is formulated below:


P( _h_ ) = _N_ ( _h|h, C_ _h_ )



where _M_ _k_ _[c]_ _−_ 1 [=] _[ M]_ _[ c]_ [(] _[q]_ _[k][−]_ [1] _[, ω]_ _[k][−]_ [1] _[, a]_ _[k][−]_ [1] [)][ and] _[ dT]_ [ denote a]
sample time.


_B. Measurement Model and Setup_


The IMU measurements at time step _k_ (angular velocity
_ω_ _m,k_ _∈_ R [3] and acceleration _a_ _m,k_ _∈_ R [3] ) and the related bias
in readings ( _b_ _ω,k_ and _b_ _a,k_ ) are as follows [27], [28]:
IMU � _ωa_ _m,km,k_ = = _ω a_ _kk_ + + _b b_ _a,kω,k_ + + _η η_ _a,kω,k_

(18)

 _b_ _b_



III. P ROBLEM F ORMULATION


In this section, the kinetic and measurement models are
introduced. After defining the state vector, a state transition
function is established to define the relation between navigation state and the input data. Moreover, the interdependence
between the state and measurements vector is formulated,
which is essential for the proposed DeepUKF-VIN perfor
mance.


_A. Navigation Model in 3D_


The true navigation kinematics of a vehicle travelling in 3D
space are represented by [2], [19]:
 _q_ ˙ = [1] 2 [Γ(] _[ω]_ [)] _[q][ ∈]_ [S] [3]


˙

 _p_ = _v ∈_ R [3] (15)



_q_ ˙ = [1]



2 [Γ(] _[ω]_ [)] _[q][ ∈]_ [S] [3]



˙
_p_ = _v ∈_ R [3]



(15)







˙
_v_ = _g_ + _R_ _q_ ( _q_ ) _a ∈_ R [3]



with
0 _−ω_ _[⊤]_
Γ( _ω_ ) = � _ω_ _−_ [ _ω_ ] _×_



_∈_ R [4] _[×]_ [4]
�



where _q_ describe vehicle’s orientation with respect to quaternion, _ω ∈_ R [3] and _a ∈_ R [3] denote angular velocity and
acceleration, respectively, while _p ∈_ R [3] and _v ∈_ R [3] refer
to vehicle’s position and linear velocity, respectively, with
_q, ω, a ∈{B}_ and _p, v ∈{W}_ . In light of [2], the kinematics
in (15) is equivalent to:

_q_ ˙˙ 12 [Γ(] _[ω]_ [)] _[q]_ 0 0 0 _q_

     
















1 0 0 0
2 [Γ(] _[ω]_ [)] _[q]_
0 0 **I** 3 0
0 0 0 _g_ + _R_ _q_ ( _q_ ) _a_
0 0 0 0







=








 (16)










_q_

_p_

_v_

1







_q_ ˙
_p_ ˙
_v_ ˙

0



~~�~~ ~~�~~ � ~~�~~
_M_ _[c]_ ( _q,ω,a_ )


Since the onboard data processor operating in discrete space
and the sensor data are updated at discrete instances, the
continuous kinematics in (16) need to be discretized. The true
discrete value at the _k_ th time-step of _q ∈_ S [3], _ω ∈_ R [3], _a ∈_ R [3],
_p ∈_ R [3], and _v ∈_ R [3] is defined by _q_ _k_ _∈_ S [3], _ω_ _k_ _∈_ R [3], _a_ _k_ _∈_ R [3]

_p_ _k_ _∈_ R [3], and _v_ _k_ _∈_ R [3], respectively. The equivalent discretized
kinematics of the expression in (16) is [2]


_q_ _k_ _q_ _k−_ 1

   







= exp( _M_ _ck−_ 1 _[dT]_ [)]








 (17)







_q_ _k_

_p_ _k_

_v_ _k_

1









_q_ _k−_ 1

_p_ _k−_ 1

_v_ _k−_ 1

1



(18)




[1] 2 [(] _[h][ −]_ _[h]_ [)] _[⊤]_ _[C]_ _h_ _[−]_ [1] [(] _[h][ −]_ _[h]_ [)] �



_−_ [1]
� 2
= [exp]



_∈_ R
(2 _π_ ) _[d]_ det( _C_ _h_ )



IMU



_ω_ _m,k_ = _ω_ _k_ + _b_ _ω,k_ + _η_ _ω,k_
� _a_ _m,k_ = _a_ _k_ + _b_ _a,k_ + _η_ _a,k_



~~�~~



_b_ _ω,k_ = _b_ _ω,k−_ 1 + _η_ _bω,k_
� _b_ _a,k_ = _b_ _a,k−_ 1 + _η_ _ba,k_



where P( _h_ ) is the probability density of _h_ .







Bias


where _η_ _ω,k_ and _η_ _a,k_ refer to gyroscope and accelerometer
additive zero-mean white noise, respectively, while the zeromean white noise terms _η_ _ba,k−_ 1, and _η_ _bω,k−_ 1 _∈_ R [3] correspond
to _b_ _a,k_ and _b_ _ω,k_ _∈_ R [3] . In other words
 _ηη_ _ω,ka,k_ _∼N ∼N_ (0(0 33 _, C, C_ _ηη_ _ω,ka,k_ )) (19)




_η_ _ω,k_ _∼N_ (0 3 _, C_ _η_ _ω,k_ )



_η_ _a,k_ _∼N_ (0 3 _, C_ _η_ _a,k_ )



_η_ _ba,k_ _∼N_ (0 3 _, C_ _η_ _bω,k_ )



(19)







_η_ _bω,k_ _∼N_ (0 3 _, C_ _η_ _ba,k_ )



Note that if the noise vectors in (19) are assumed to be
uncorrelated, their covariance matrices will be diagonal with
positive entries, such that [27], [28]:
 _C_ _η_ _ω,k_ = diag( _c_ [2] _η_ _ω,k_ [)]

_C_ _η_ _a,k_ = diag( _c_ [2] _η_ _a,k_ [)]

(20)

 _C_ [2]



_C_ _η_ _ω,k_ = diag( _c_ [2] _η_ _ω,k_ [)]
_C_ _η_ _a,k_ = diag( _c_ [2] _η_ _a,k_ [)]
_C_ _η_ _bω,k_ = diag( _c_ [2] _η_ _bω,k_ [)]
_C_ _η_ _ba,k_ = diag( _c_ [2] _η_ _ba,k_ [)]



(20)



5


where _d_ _z,k_ = 3 _d_ _l,k_ represents the dimension of the measurement vector _z_ _k_ = _l_ _b_ _, k_ . The _i_ th measurement function
h i : R _[d]_ _[x]_ _×_ R [3] _→_ R [3] is defined as:


h i ( _x_ _k_ _, l_ _w,i_ ) = _R_ _q_ ( _q_ _k_ ) _[⊤]_ ( _l_ _w,i_ _−_ _p_ _k_ ) _∈_ R [3] (25)


where _q_ _k_ _, p_ _k_ _⊂_ _x_ _k_ (see (21)). The measurement function h :
R _[d]_ _[x]_ _×_ R _[d]_ _[z,k]_ _→_ R _[d]_ _[z,k]_ is given by:

h( _x_ _k_ _, l_ _w_ ) = h i ( _x_ _k_ _, l_ _w,_ 0 ) _[⊤]_ _, . . .,_ h i ( _x_ _k_ _, l_ _w,d_ _l_ ) _[⊤]_ [�] _[⊤]_ _∈_ R _[d]_ _[z,k]_
�

(26)
The measurement function in (26) is used to find the measurement vector _z_ _k_ at each time step _k_ such that


_z_ _k_ = h( _x_ _k_ _, l_ _w_ ) + _η_ _l,k_ _∈_ R _[d]_ _[z,k]_ (27)


where _η_ _l,k_ _∼N_ (0 _d_ _z,k_ _, C_ _η_ _l_ _,k_ ) is the measurement additive
white noise. The covariance matrix _C_ _η_ _l_ _,k_ _∈_ R _[d]_ _[z,k]_ _[×][d]_ _[z,k]_ is
defined by:
_C_ _η_ _l_ _,k_ = _c_ [2] _η_ _l_ _,k_ **[I]** _[d]_ _z,k_ (28)


where _c_ _η_ _l_ _,k_ _∈_ R is a scalar. This definition is particularly
useful since _d_ _z,k_ may vary at each time step _k_ .


IV. Q UATERNION      - BASED UKF-VIN


This section provides a detailed description of the
quaternion-based Unscented Kalman Filter for 3D VisualInertial Navigation (UKF-VIN) design which will be subsequently tightly-coupled with the proposed Deep Learningbased Adaptation Mechanism (DLAM) for adaptive tuning of
UKF-VIN covariance matrices. The proposed approach builds
upon the standard UKF [26], incorporating specific modifications to address challenges inherent in navigation tasks.
These adaptations ensure the UKF operates effectively within
the quaternion space S [3], preserving the physical validity of
orientation estimation. Furthermore, the design accommodates
the intermittent nature of vision data, which is not available
at every time step, while consistently integrating IMU data.


_A. Initialization_


he filter is initialized with the initial state vector estimate
_x_ ˆ 0 _|_ 0 _∈_ R _[d]_ _[x]_, and its associated covariance estimate _P_ 0 _|_ 0 _∈_
R [(] _[d]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[d]_ _[x]_ _[−]_ [1)] which represents the confidence in the initial
state estimate. The reduced dimensionality of the covariance
matrix arises from the fact that the quaternion in the state
vector has three degrees of freedom, despite having four
components [23].


_B. Aggregate Predict_


At each time step _k_ where image data is available, the
current state vector is predicted using the last _d_ _b_ input vectors
_u_ _k−_ 1 _−d_ _b_ : _k−_ 1 _∈_ R _[d]_ _[b]_ _[×][d]_ _[u]_, along with the previous state vector
estimate ˆ _x_ _k−_ 1 _−d_ _b_ _|k−_ 1 _∈_ R _[d]_ _[x]_ and the covariance matrix
_P_ _k−_ 1 _−d_ _b_ _|k−_ 1 _∈_ R [(] _[d]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[d]_ _[x]_ _[−]_ [1)], the current state vector is
predicted. Here _d_ _b_ represents the number of measurements
received from the IMU between the current and the last
instance when image data was available. For each _j_ = _{k−_ 1 _−_
_d_ _b_ _, . . ., k −_ 1 _}_, the following steps are executed sequentially.







where _c_ _η_ _ω,k_, _c_ _η_ _a,k_, _c_ _η_ _bω,k_, and _c_ _η_ _ba,k_ _∈_ R [3] represent the square
roots of the diagonal elements of their respective covariance
matrices. Let us define the state vector _x_ _k_ _∈_ R _[d]_ _[x]_ and the
augmented state vector _x_ _[a]_ _k_ _[∈]_ [R] _[d]_ _[a]_ [ such that]
 _x_ _k_ = � _q_ _k_ _[⊤]_ _p_ _[⊤]_ _k_ _v_ _k_ _[⊤]_ _b_ _[⊤]_ _ω,k_ _b_ _[⊤]_ _a,k_ � _⊤_ _∈_ R _[d]_ _[x]_

(21)





_⊤_
_x_ _k_ = � _q_ _k_ _[⊤]_ _p_ _[⊤]_ _k_ _v_ _k_ _[⊤]_ _b_ _[⊤]_ _ω,k_ _b_ _[⊤]_ _a,k_ � _∈_ R _[d]_ _[x]_



(21)
_x_ _[a]_ _k_ = � _x_ _[⊤]_ _k_ _η_ _x,k_ _[⊤]_ � _∈_ R _[d]_ _[a]_







with _d_ _x_ = 16 and _d_ _a_ = 22 representing the dimensions of the
state vector and the augmented state vector, respectively, and
_η_ _x,k_ representing the augmented noise vector, such that

_η_ _x,k_ = � _η_ _ω,k⊤_ _η_ _a,k_ _[⊤]_ � _⊤_ _∈_ R _d_ _ηx_ (22)


where _d_ _η_ _x_ = 6 is the dimension of the augmented noise.
Consider formulating the additive noise vector such that:

_η_ _w,k_ = �0 _⊤_ 10 _n_ _[⊤]_ _bω,k_ _n_ _[⊤]_ _ba,k_ � _⊤_ _∈_ R _d_ _x_ (23)


Then, the expression in (17), using (18), (21), (22), and (23)
can be written in form of state transition function f : R _[d]_ _[a]_ _→_
R _[d]_ _[x]_ such that


_x_ _k_ = f( _x_ _[a]_ _k−_ 1 _[, u]_ _[k][−]_ [1] [) +] _[ η]_ _[w,k][−]_ [1] (24)


with the input vector being defined as _u_ _k−_ 1 =

[ _ω_ _m,k_ _[⊤]_ _−_ 1 _[, a]_ _[⊤]_ _m,k−_ 1 []] _[⊤]_ _[∈]_ [R] _[d]_ _[u]_ [ and] _[ d]_ _[u]_ [ = 6][ being the dimension]
of the input vector. Let the landmark coordinates in _{W}_
be represented as _{l_ _w,k,i_ _∈_ R [3] _}_ _i_, where these coordinates
are either known from prior information or obtained from
a series of stereo camera measurements. Similarly, let the
corresponding coordinates measured by the latest stereo
camera data in _{B}_ be denoted as _{l_ _b,k,i_ _∈_ R [3] _}_ _i_, where
_i_ = _{_ 1 _, . . ., d_ _l,k_ _}_ represents the index of each measured
landmark, and _d_ _l,k_ _∈_ R denotes the number of landmarks at
each measurement step. Note that _d_ _l,k_ is not constant and
may change at each step. We then construct the concatenated
vectors _l_ _w,k_ _∈_ R _[d]_ _[z,k]_, and _l_ _b,k_ _∈_ R _[d]_ _[z,k]_ such that:
 _l_ _w,k_ = � _l_ _w,k,_ _[⊤]_ 1 _[, . . ., l]_ _w,k,d_ _[⊤]_ _l,k_ � _⊤_ _∈_ R _[d]_ _[z,k]_

_⊤_





_⊤_
_l_ _w,k_ = � _l_ _w,k,_ _[⊤]_ 1 _[, . . ., l]_ _w,k,d_ _[⊤]_ _l,k_ � _∈_ R _[d]_ _[z,k]_



_⊤_
_l_ _b,k_ = � _l_ _b,k,_ _[⊤]_ 1 _[, . . ., l]_ _b,k,d_ _[⊤]_ _l_ � _∈_ R _[d]_ _[z,k]_






6



_1) Augmentation:_ The augmented state vector _x_ _[a]_ _j_ _[∈]_ [R] _[d]_ _[a]_ [,]
and the augmented covariance matrix _P_ _j_ _[a]_ _|j_ _[∈]_ [R] [(] _[d]_ _[a]_ _[−]_ [1)] _[|]_ [(] _[d]_ _[a]_ _[−]_ [1)]

are constructed as follows:


_⊤_
_x_ ˆ _[a]_ _j|j_ [=] � _x_ ˆ _[⊤]_ _j|j_ _[,]_ [ 0] _m_ _[⊤]_ _nx_ _×_ 1 � _∈_ R _[m]_ _[a]_ (29)

_P_ _j_ _[a]_ _|j_ [= diag(] _[P]_ _[j][|][j]_ _[, C]_ _[η]_ _x_ _[,k][−]_ [1] [)] _[ ∈]_ [R] [(] _[m]_ _[a]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[a]_ _[−]_ [1)] (30)


where _C_ _η_ _x_ _,k_ = diag( _C_ _η_ _w,k_ _, C_ _η_ _a,k_ ) _∈_ R _[η]_ _[x]_ is the covariance
matrix of _η_ _x,k_ as defined in (22).
_2) Sigma Points Construction:_ Using the augmented estimate of the state vector and its covariance, while accounting
for the reduced dimensionality of the quaternions and applying
the unscented transform [26], the sigma points representing the
prior distribution are computed as follows:

 _χ_ _[a]_ _j|j,_ 0 [= ˆ] _[x]_ _[a]_ _j|j_ _[∈]_ [R] _[d]_ _[a]_
 _χ_ _[a]_ _j|j,ν_ [= ˆ] _[x]_ _[a]_ _j|j_ [⊞] _[δ][x]_ [ˆ] _ν_ _[a]_ _[∈]_ [R] _[d]_ _[a]_ _ν_ = _{_ 1 _, . . .,_ 2( _d_ _a_ _−_ 1) _}_ _ν_



with _χ_ _j_ +1 _|j,ν,q_ _∈_ S [3] and _χ_ _j_ +1 _|j,ν,−_ _∈_ R _[d]_ _[x]_ _[−]_ [4] representing the
quaternion and non-quaternion components of _χ_ _j_ +1 _|j,ν_ _∈_ R _[d]_ _[x]_,
respectively. Note that in (35), the quaternion weighted average
(14) is used for quaternion components of the propagated
sigma point vectors and the straightforward weighted average
for non-orientation components of the propagated sigma point
vector. The weights _{w_ _ν_ _[m]_ _[}]_ _[ν]_ [and] _[ {][w]_ _ν_ _[c]_ _[}]_ _[ν]_ [in (][35][) and (][36][) are]
derived from:
 _ww_ 0 _[m]_ 0 _[c]_ [=][=] _λ_ + ( _dλλ_ _a_ _−_ 1) _[∈]_ [R]

_λ_ + ( _d_ _a_ _−_ 1) [+ 1] _[ −]_ _[α]_ [2] [ +] _[ β][ ∈]_ [R] (37)





_λ_
_w_ 0 _[m]_ [=]
_λ_ + ( _d_ _a_ _−_ 1) _[∈]_ [R]



_λ_
_w_ 0 _[c]_ [=]
_λ_ + ( _d_ _a_ _−_ 1) [+ 1] _[ −]_ _[α]_ [2] [ +] _[ β][ ∈]_ [R]



(37)



1
_w_ _ν_ _[m]_ [=] _[ w]_ _ν_ _[c]_ [=] 2(( _d_ _a_ _−_ 1) + _λ_ ) _[∈]_ [R]



_χ_ _[a]_ _j|j,_ 0 [= ˆ] _[x]_ _[a]_ _j|j_ _[∈]_ [R] _[d]_ _[a]_







_ν_ = _{_ 1 _, . . .,_ 2( _d_ _a_ _−_ 1) _}_ _ν_



 _χ_ _[a]_ _j|j,ν_ [= ˆ] _[x]_ _[a]_ _j|j_ [⊞] _[δ][x]_ [ˆ] _ν_ _[a]_ _[∈]_ [R] _[d]_ _[a]_ _ν_ = _{_ 1 _, . . .,_ 2( _d_ _a_ _−_ 1) _}_ _ν_

 _χ_ _[a]_ _j|j,ν_ + _m_ _a_ [= ˆ] _[x]_ _[a]_ _j|j_ [⊟] _[δ][x]_ [ˆ] _ν_ _[a]_ _[∈]_ [R] _[d]_ _[a]_

(31)
where _δx_ ˆ _[a]_ _j,ν_ [=] ��( _d_ _a_ _−_ 1 + _λ_ ) _P_ _j_ _[a]_ _|j_ � _ν_ _[∈]_ [R] _[m]_ _[a]_ _[−]_ [1] [, with the]



where _α_ and _β ∈_ R are tuning parameters. The ⊟ operator in
(36) is defined in accordance with (13) as follows:







( _d_ _a_ _−_ 1 + _λ_ ) _P_ _j_ _[a]_ _|j_ �



where _δx_ ˆ _[a]_ _j,ν_ [=] ��( _d_ _a_ _−_ 1 + _λ_ ) _P_ _j_ _[a]_ _|j_ � _ν_ _[∈]_ [R] _[m]_ _[a]_ _[−]_ [1] [, with the]

subscript _ν_ representing the _ν_ th column. The operators ⊞ and
⊟ in (31) are defined in accordance with (11) and (12), such
that



_χ_ _j_ +1 _|j,ν_ ⊟ _x_ ˆ _j_ +1 _|j_ = � _χχ_ _jj_ +1+1 _||j,ν,j,ν,q−_ _⊖−_ _xx_ ˆˆ _jj_ +1+1 _||j,qj,−_ � _∈_ R _[d]_ _[x]_ _[−]_ [1] (38)



�

�



where ˆ _x_ _j_ +1 _|j,q_ _∈_ S [3], and ˆ _x_ _j_ +1 _|j,−_ _∈_ R _[d]_ _[x]_ _[−]_ [4] represent the
quaternion and non-quaternion components of ˆ _x_ _j_ +1 _|j_ _∈_ R _[d]_ _[x]_,
respectively. Note that _C_ _η_ _w_ _,k_ = diag(0 _d_ _x_ _−_ 7 _, C_ _η_ _w,k_ _, C_ _η_ _a,k_ ) _∈_
R [(] _[d]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[d]_ _[x]_ _[−]_ [1)] denotes the covariance matrix of _η_ _w,k_ as
defined in (23).
_5) Iterate over batch:_ If _j_ = _k−_ 1, corresponding to the end
of the batch, the predicted state estimate ˆ _x_ _k|k−_ 1, the covariance
_P_ _k|k−_ 1, and the predicted sigma points _{χ_ _j_ +1 _|j,ν_ _}_ _ν_, as defined
in (35), (36), and (34), respectively, are passed to the update
step (see Section IV-C). Otherwise, ˆ _x_ _j_ +1 _|j_ +1 and _P_ _j_ +1 _|j_ +1
are set to ˆ _x_ _j_ +1 _|j_ and _P_ _j_ +1 _|j_, respectively. The aggregate
prediction algorithm then increments _j ←_ _j_ +1 and continues
by returning to Section IV-B1.


_C. Update_

_1) Calculate Measurement Sigma Points And Its Statistics:_
At time step _k_, the predicted sigma points _{χ_ _j_ +1 _|j,ν_ _}_ _ν_ are
passed through the measurement function (26) to calculate the
measurement sigma points � _ζ_ _k,ν_ _∈_ R _[d]_ _[z,k]_ [�] _ν_ [such that]

_ζ_ _k,ν_ = _h_ ( _χ_ _k|k−_ 1 _,ν_ _, l_ _w_ ) _∈_ R _[d]_ _[z,k]_ (39)


Considering (27) and (37), the expected value and covariance
of _{ζ_ _k,ν_ _}_ _ν_, denoted by ˆ _z_ _k_ _∈_ R _[d]_ _[z,k]_ and _P_ _z_ _k_ _∈_ R _[d]_ _[z,k]_ _[×][d]_ _[z,k]_,
respectively, are determined as follows:



_x_ ˆ _[a]_ _j|j_ [⊞] _[δ][x]_ [ˆ] _ν_ _[a]_ [=]


_x_ ˆ _[a]_ _j|j_ [⊟] _[δ][x]_ [ˆ] _ν_ _[a]_ [=]



� _x_ ˆ _x_ ˆ _[a]_ _j_ _[a]_ _j||j,j,q−_ [⊞][+] _[ δ][δ][x][x]_ [ˆ][ˆ] _[a]_ _j,ν,r_ _[a]_ _j,ν,−_

� _x_ ˆ _x_ ˆ _[a]_ _j_ _[a]_ _j||j,j,q−_ [⊟] _[−]_ _[δ][δ][x][x]_ [ˆ][ˆ] _[a]_ _j,ν,r_ _[a]_ _j,ν,−_



_∈_ R _[d]_ _[a]_ (32)


_∈_ R _[d]_ _[a]_ (33)



where ˆ _x_ _[a]_ _j|j,q_ _[∈]_ [S] [3] [ and][ ˆ] _[x]_ _[a]_ _j|j,−_ _[∈]_ [R] _[d]_ _[a]_ _[−]_ [4] [ represent the quaternion]
and non-quaternion components ˆ _x_ _[a]_ _j|j_ _[∈]_ [R] _[d]_ _[a]_ [, respectively,]
and _δx_ ˆ _[a]_ _j,ν,r_ _[∈]_ [R] [3] [ and] _[ δ][x]_ [ˆ] _[a]_ _j,ν,−_ _[∈]_ [R] _[d]_ _[a]_ _[−]_ [4] [ denote the rotation]
vector and non-rotation vector components of _δx_ ˆ _[a]_ _j,ν_ _[∈]_ [R] _[d]_ _[a]_ _[−]_ [1] [,]
respectively. Note that _λ ∈_ R is a tuning parameter that
controls the spread of the sigma points.
_3) Sigma Points Propagation:_ In this step, the sigma points
defined in (31) are propagated through the state transition
function (24) to obtain the predicted sigma points _{χ_ _j_ +1 _|j,ν_ _}_ _ν_
such that


_χ_ _j_ +1 _|j,ν_ = f( _χ_ _[a]_ _j|j,ν_ _[, u]_ _[j]_ [)] _[ ∈]_ [R] _[d]_ _[x]_ _ν_ = _{_ 1 _, . . .,_ 2( _d_ _a_ _−_ 1) _}_ _ν_
(34)
_4) Calculate the Predicted Mean and Covariance:_ The
weighted mean ˆ _x_ _j_ +1 _|j_ _∈_ R _[d]_ _[x]_ and covariance _P_ _j_ +1 _|j_ _∈_
R [(] _[d]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[d]_ _[x]_ _[−]_ [1)] of the predicted sigma points _{χ_ _j_ +1 _|j,ν_ _}_ _ν_
are determined in this step in accordance with (14). These
quantities are calculated as follows:



_z_ ˆ _k_ =


_P_ _z_ _k_ =



2( _d_ _a_ _−_ 1)
� _w_ _ν_ _[m]_ _[ζ]_ _[k,ν]_ (40)


_ν_ =0


2( _d_ _a_ _−_ 1)
� _w_ _ν_ _[c]_ [[] _[ζ]_ _[k,ν]_ _[−]_ _[z]_ [ˆ] _[k]_ [][] _[ζ]_ _[k,ν]_ _[−]_ _[z]_ [ˆ] _[k]_ []] _[⊤]_


_ν_ =0


+ _C_ _η_ _l_ _,k_ (41)



� _w_ _ν_ _[m]_ _[X]_ _j_ +1 _|j,ν,−_


_ν_ =0







_x_ ˆ _j_ +1 _|j_ =









QWM( _{χ_ _j_ +1 _|j,ν,q_ _}_ _ν_ _, {w_ _ν_ _[m]_ _[}]_ _[ν]_ [)]
2( _d_ _a_ _−_ 1)
� _w_ _ν_ _[m]_ _[X]_ _j_ +1 _|j,ν,−_



 _∈_ R _d_ _x_ (35)



Using (37), (38), and (40), the cross covariance matrix
_P_ _x_ _k_ _,z_ _k_ _∈_ R [(] _[d]_ _[x]_ _[−]_ [1)] _[×][d]_ _[z,k]_ is calculated by



_P_ _x_ _k_ _,z_ _k_ =



_P_ _j_ +1 _|j_ =



2( _d_ _a_ _−_ 1)
�


_ν_ =0



� _w_ _ν_ _[c]_ [(] _[χ]_ _j_ +1 _|j,ν_ [⊟] _[x]_ [ˆ] _j_ +1 _|j_ [)(] _[χ]_ _j_ +1 _|j,ν_ [⊟] _[x]_ [ˆ] _j_ +1 _|j_ [)] _[⊤]_ [�]



2( _d_ _a_ _−_ 1)
� _w_ _j_ _[c]_ [[] _[χ]_ _k|k−_ 1 _,ν_ [⊟] _[x]_ [ˆ] _k|k−_ 1 [][] _[ζ]_ _[k,ν]_ _[−]_ _[z]_ [ˆ] _[k]_ []] _[⊤]_ (42)


_ν_ =0



+ _C_ _η_ _w_ _,k−_ 1 _∈_ R [(] _[d]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[d]_ _[x]_ _[−]_ [1)] (36)


Note that the operator ⊟ in (42) is defined in (38).
_2) Calculate The Current State Estimate:_ First, the Kalman
gain _K_ _k_ _∈_ R [(] _[d]_ _[x]_ _[−]_ [1)] _[×][d]_ _[z,k]_, based on (42) and (41), is computed

as
_K_ _k_ = _P_ _x_ _k_ _,z_ _k_ _P_ _z_ _[⊤]_ _k_ _,z_ _k_ _[∈]_ [R] [(] _[d]_ _[x]_ _[−]_ [1)] _[×][d]_ _[z,k]_ (43)


The correction vector _δx_ ˆ _k_ _∈_ R _[d]_ _[x]_ _[−]_ [1] is then derived, using (40)
and (43), as


_δx_ ˆ _k_ = _K_ _k_ ( _z_ _k_ _−_ _z_ ˆ _k_ ) _∈_ R _[d]_ _[x]_ _[−]_ [1] (44)


Representing the rotation and non-rotation components of

ˆ ˆ ˆ
_δx_ _k_ _∈_ R _[d]_ _[x]_ _[−]_ [1] as _δx_ _k,r_ _∈_ S [3] and _δx_ _k,−_ _∈_ R _[d]_ _[x]_ _[−]_ [4], respectively,
the updated state estimate ˆ _x_ _k|k_ is computed as


_x_ ˆ _k|k_ = ˆ _x_ _k|k−_ 1 ⊞ _δx_ ˆ _k_ (45)


Note that the ⊞ operator in (45) has been defined in (32). Finally, the covariance matrix associated with this state estimate
is updated, based on (36), (41), and (43), as


_P_ _k|k_ = _P_ _k|k−_ 1 _−_ _K_ _k_ _P_ _z_ _k_ _,z_ _k_ _K_ _k_ _[⊤]_ (46)


_D. Iterate and Collect IMU Measurements_


Proceed to the next index by setting _k ←_ _k_ + 1. The
IMU measurements _u_ _k−_ 1 _−d_ _b_ : _k−_ 1 _∈_ R _[d]_ _[b]_ _[×][d]_ _[u]_ are collected
until image data becomes available at _z_ _k_, at which point the
algorithm proceeds to IV-B. If image data is not yet available,
the collection of IMU data continues.


V. D EEP L EARNING  - BASED A DAPTATION M ECHANISM

(DLAM)


This section provides a detailed discussion of the design
of the proposed DLAM. This mechanism adaptively updates
the covariance matrices of the noise parameters used by the
quaternion-based UKF-VIN at each time step, leveraging the
input data. The DLAM is designed to enhance the filter’s
performance by dynamically adjusting to changes in noise
characteristics, thereby ensuring more accurate and robust
state estimation. The proposed DLAM is composed of two
neural networks, namely, the IMU-Net (Section V-A) and the
Vision-Net (Section V-B). The IMU-Net processes the last
_d_ GRU _∈_ R measurement vectors (see (24)) as input, while the
Vision-Net takes the current stereo image measurements as
input. Each network produces scaling factors corresponding
to their respective sensor covariance matrices. Given that the
IMU noise model includes 12 unknown terms (see (20)), and
the vision unit covariance matrix contains a single unknown
element (see (28)), the IMU-Net and Vision-Net generate
outputs of size 12 and 1, respectively. This can be formulated

as:


_{γ_ _i,k_ _}_ _i_ = _{_ 1 _,...,_ 12 _}_ = IMUNet( _u_ _k−_ 1 _−d_ GRU : _k−_ 1 _, W_ _IN_ ) _,_ (47)

_γ_ 13 _,k_ = VisionNet(img _l_ _,_ img _r_ _, W_ _V N_ ) _,_ (48)


where _u_ _k−_ 11: _k−_ 1 _∈_ R _[d]_ [GRU] _[×][d]_ _[u]_ represents the last _d_ GRU measurement vectors, img _l_ and img _r_ denote the left and right images, respectively, and _W_ _IN_ and _W_ _V N_ represent the weights
and biases of the IMU-Net and Vision-Net, respectively. To
explain the use of the scaling parameters _{γ_ _i,k_ _}_ _i_ = _{_ 1 _,...,_ 13 _}_



7


generated by IMU-Net and Vision-Net, let us define the
standard deviation vector _c_ _k_ _∈_ R [13] using the square root of
the diagonal elements of the covariance matrices in (20) and
(28), such that:


_⊤_ _⊤_ 13
_c_ _k_ = � _c_ _η_ _ω,k_ _c_ _[⊤]_ _η_ _a,k_ _c_ _[⊤]_ _η_ _bω,k_ _c_ _[⊤]_ _η_ _ba,k_ _c_ _[⊤]_ _η_ _l_ _,k_ � _∈_ R (49)


For each _i_ th element of _c_ _k_, denoted by _c_ _k,i_ _∈_ R, let ¯ _c_ _k,i_ _∈_ R
represent its nominal value, obtained through traditional offline
tuning methods. Then, at each time step _k_, using the scaling
parameters _{γ_ _i,k_ _}_ _i_ = _{_ 1 _,...,_ 13 _}_ from (48) and (47), the standard
deviation vector elements defined in (49) are computed as in

[32], [33]:


_c_ _k,i_ = ¯ _c_ _k,i_ 10 _[υ]_ [ tanh] _[ γ]_ _[i,k]_ _i_ = _{_ 1 _,_ 2 _, . . .,_ 13 _}_ (50)


where _υ ∈_ R specifies the degree to which the predicted _c_ _k,i_
may deviate from the nominal value ¯ _c_ _k,i_ . Considering (50),
(49), (20), and (28), the covariance matrices used by the filter
are found as follows:
 _CC_ _ηη_ _ω,ka,k_ = diag(= diag( _cc_ [2] _k,_ [2] _k,_ 1:34:6 [)][)]
 _C_ _η_ _bω,k_ = diag( _c_ [2] _k,_ 7:9 [)] (51)



_C_ _η_ _ω,k_ = diag( _c_ [2] _k,_ 1:3 [)]
_C_ _η_ _a,k_ = diag( _c_ [2] _k,_ 4:6 [)]
_C_ _η_ _bω,k_ = diag( _c_ [2] _k,_ 7:9 [)]
_C_ _η_ _ba,k_ = diag( _c_ [2] _k,_ 10:12 [)]
_C_ _η_ _l,k_ = _c_ [2] _k,_ 13 **[I]** _[d]_ _z,k_



(51)







where _c_ _k,i_ : _j_ _∈_ R _[j][−][i]_ [+1] denotes the _i_ th to _j_ th components of

_c_ _k_ .









Fig. 1: IMU-Net Architecture Schematics


_A. IMU-Net_


It is assumed that the covariance matrices _C_ _η_ _w,k_ and _C_ _η_ _a,k_
can be optimized for each batch by considering the input
vector, which comprises the last _d_ _b_ IMU measurements and
practically it is a feasible and realizable condition. Recurrent deep learning frameworks, particularly Recurrent Neural
Networks (RNNs) and their advanced variants, have proven
effective in modeling sequential data due to their capacity to


capture dependencies across time steps [37]. While traditional
RNNs are foundational, they often struggle with long-term
dependencies due to challenges such as vanishing gradients

[38], [39], leading to the development of more sophisticated
architectures, such as Long Short-Term Memory (LSTM) networks [40] and Gated Recurrent Units (GRUs) [41]. LSTMs
and GRUs were specifically designed to address the limitations
of standard RNNs by incorporating gating mechanisms that
regulate information flow, enabling more stable long-term
memory retention. In particular, GRUs offer a streamlined
architecture by combining the forget and input gates of LSTMs
into a single update gate, making them computationally more
efficient while retaining the ability to model complex temporal
relationships. GRUs have been shown to outperform LSTMs,
particularly when the dataset is small, while also being less
computationally intensive [41], [42].


For a GRU cell at time step _l_, let _α_ _l_ _∈_ R _[d]_ _[u]_ denote the GRU
cell input vector. Each GRU cell computes its hidden state
_−→_
_h_ _l_ _∈_ R _d_ _h_ by leveraging three key components: the update
gate _[−→]_ _z_ _l_ _∈_ R _[d]_ _[h]_, reset gate _[−→]_ _r_ _l_ _∈_ R _[d]_ _[h]_, and candidate hidden
state _[−→]_ _n_ _l_ _∈_ R _[d]_ _[h]_, where _d_ _h_ represents the dimensionality of
the hidden state. The equations governing these components
are as follows:


_•_ **Update Gate** :

_−→z_ _l_ = _σ_ ( _W_ _[−→]_ _z_ _α_ _l_ + _[−→]_ _U_ _z_ _−→h_ _l−_ 1 + _−→b_ _z_ ) _∈_ R _d_ _h_ (52)

where _[−→]_ _W_ _z_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[u]_ and _[−→]_ _U_ _z_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[h]_ are weight
matrices, and _[−→]_ _b_ _z_ _∈_ R _[d]_ _[h]_ is the bias term. Note that the
function _σ_ : R _[d]_ _[h]_ _→_ R _[d]_ _[h]_ denotes the sigmoid function.
The update gate controls the degree to which the previous
hidden state _[−→]_ _h_ _l−_ 1 is retained.


_•_ **Reset Gate** :


_−→r_ _l_ = _σ_ ( _W_ _[−→]_ _r_ _α_ _l_ + _[−→]_ _U_ _r_ _−→h_ _l−_ 1 + _−→b_ _r_ ) _∈_ R _d_ _h_ (53)

where _[−→]_ _W_ _r_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[u]_, _[−→]_ _U_ _r_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[h]_, and _[−→]_ _b_ _r_ _∈_ R _[d]_ _[h]_ are
the corresponding parameters for the reset gate, which
determines the relevance of the previous hidden state in
computing the candidate hidden state.


_•_ **Candidate Activation** :


_−→_ _−→_
_n_ _l_ = tanh( _W_ _[−→]_ _n_ _α_ _l_ + _[−→]_ _r_ _l_ _◦_ ( _[−→]_ _U_ _n_ _h_ _l−_ 1 ) + _−→b_ _n_ ) (54)


where the _◦_ operator represents element-wise multiplication, and _[−→]_ _W_ _n_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[u]_, _[−→]_ _U_ _n_ _∈_ R _[d]_ _[h]_ _[×][d]_ _[h]_, and _[−→]_ _b_ _n_ _∈_ R _[d]_ _[h]_
are the weight matrices and bias vector associated with
the candidate hidden state. Note that tanh : R _[d]_ _[h]_ _→_ R _[d]_ _[h]_

denotes the hyperbolic tangent function.


_•_ **Hidden State Update** :

_−→_
_h_ _l_ = _−→z_ _l_ _◦−→h_ _l−_ 1 + (1 _−−→z_ _l_ ) _◦−→n_ _l_ (55)


This update equation combines the previous hidden state
_−→_
_h_ _l−_ 1 and the candidate _−→n_ _l_, governed by the update gate
_−→_
_z_ _l_ .



8


Equations (52), (53), (54), and (55) can be adapted to calculate
the backward hidden vector _[←−]_ _h_ _l_ by moving in reverse across
the sequence, computing each hidden state based on the
subsequent hidden vector _[←−]_ _h_ _l_ +1 and the input vector _α_ _l_ . For
each GRU layer, consisting of _d_ GRU GRU cells, both forward
and backward passes are computed. The forward and backward
hidden vectors for each cell are concatenated to form:


_−→_ _←−_ _⊤_
_h_ _l_ = � _h_ _⊤l_ _h_ _⊤l_ � _∈_ R [2] _[d]_ _[h]_ (56)


In summary, equations (52), (53), (54), (55), and (56) collectively define the GRU function GRU : R _[d]_ _[u]_ _×_ R _[d]_ _[h]_ _→_ R [2] _[d]_ _[h]_ as
follows:
_h_ _l_ = GRU( _α_ _l_ _,_ _[−→]_ _h_ _l−_ 1 _,_ _[←−]_ _h_ _l_ +1 )


To design IMU-Net, two layers of GRUs have been stacked
with a fully connected network at the end, with Rectified Linear Unit (ReLU) activation function as the activation function
between the GRUs and the fully connected network, producing
the scaling parameters (see (47)). Note that the input to the
first GRU layer (that is _α_ 1 _, α_ 2 _, . . ., α_ _d_ GRU ) is set to the last
_d_ GRU IMU measurements ( _u_ _k−_ 1 _−d_ GRU : _k−_ 1 _∈_ R _[d]_ [GRU] _[×][d]_ _[u]_ ). This
process is visualized in Fig. 1.


_B. Vision-Net_


The Vision-Net network is designed to adaptively estimate
the measurement covariance matrix based on vision data. The

uncertainty in image measurements can be effectively estimated from the current stereo-vision measurements. In Vision
Net, each image is processed through a 2D convolutional layer,
followed by 2D max pooling, then a second 2D convolutional
layer, and another 2D max pooling layer. The resulting features
are flattened, concatenated, and subsequently passed through
two fully connected layers, ultimately producing the scaling
parameter _γ_ 13 _∈_ R (see (48)). Each convolutional layer is
followed by a ReLU activation function. A visualization of
Vision-Net is provided in Fig. 2.


VI. D EEP UKF-VIN T RAINING AND I MPLEMENTATION


In summary, the DLAM-equipped UKF-VIN algorithm referred to quaternion-based DeepUKF-VIN, is illustrated in
Fig. 3. For IMU-Net, we selected the last _d_ GRU = 10 IMU
measurements as input (see (47)). This choice is based on
the IMU’s 200 Hz sample rate, compared to the image data’s
20 Hz sample rate, meaning there are at least 10 IMU
measurements between each pair of vision measurements.
Although the structure of IMU-Net allows for variable-length
time series input data, using a fixed length of 10 measurements
enhances consistency and predictability. To train the models,
a loss function must be defined. Let the estimated orientation,
position, and velocity at time step _k_ be denoted by ˆ _q_ _k_, ˆ _p_ _k_,
and ˆ _v_ _k_, respectively, with their corresponding estimation errors
denoted by _r_ _e,k_, _p_ _e,k_, and _v_ _e,k_ . These errors are defined as
follows:


ˆ

 _r_ _e,k_ = _q_ _k_ ⊟ _q_ _k_ _∈_ S [3]


ˆ
_p_ _e,k_ = _p_ _k_ _−_ _p_ _k_ _∈_ R [3] (57)









ˆ
_p_ _e,k_ = _p_ _k_ _−_ _p_ _k_ _∈_ R (57)


ˆ
_v_ _e,k_ = _v_ _k_ _−_ _v_ _k_ _∈_ R [3]



ˆ
_r_ _e,k_ = _q_ _k_ ⊟ _q_ _k_ _∈_ S [3]


ˆ
_p_ _e,k_ = _p_ _k_ _−_ _p_ _k_ _∈_ R [3]


9



( **4** _×_ **4** )



**Conv2+ReLU**
( **32** _×_ **5** _×_ **5** )



**MaxPool**
( **4** _×_ **4** )



**Left Image**


**Right Image**



**Conv1+ReLU**
( **16** _×_ **5** _×_ **5** )





**Output**





Fig. 2: Vision-Net Architecture Schematics





































Fig. 3: Summary schematic architecture of quaternion-based DeepUKF-VIN. First, the Aggregate Predict step of the filter is
executed, incorporating the last known state information, aggregated IMU data, and the IMU noise covariance computed by
IMU-Net. Next, the Update step is performed using the predicted state information and the vision covariance matrix estimated
by Vision-Net. Raw IMU and vision data are used as inputs to IMU-Net and Vision-Net, respectively.



For a set of _d_ [mini-batch] _∈_ R estimations in the _i_ -th mini-batch,
the total loss is computed as the weighted sum of the mean



square errors (MSE) of the individual errors defined in (57).


Fig. 4: Training loss convergence over 30 epochs.


The loss function for the mini-batch is given by:



2
� _∥p_ _e,k_ _∥_

_d_ [mini-batch]



(58)



Loss [mini-batch] _i_ = _w_ _q_



2
� _d_ [mini-batch] _∥r_ _e,k_ _∥_ [+] _[ w]_ _[p]_



10


Fig. 5: Matched feature points between the left and right
images of a set of stereo image measurements using EuRoC
dataset [43].


**Algorithm 1** Training Procedure

**Initialization** :

1: Set initial values for ˆ _x_ 0 and _P_ 0 .
2: Create mini-batches of size 32 _u_ _k−_ 11: _k−_ 1, _z_ _k_, and _x_ _k_ .
**For** each _i_ -th epoch:
3: Initialize Gradient _←_ 0.

**For** each _j_ -th mini-batch:
4: Initialize Loss [mini-batch] _←_ 0.
_j_
5: Compute parameter estimates (see (47) and (48)):
_γ_ 1:12 [mini-batch] _←_ IMUNet(mini-batch _, W_ _IN_ )
� _γ_ 13 [mini-batch] _←_ VisionNet(mini-batch _, W_ _V N_ )
6: Calculate covariance scaling (see (50) and (51)):
_Cov_ [mini-batch] _←_ _Cov ·_ 10 _[υ]_ [ tanh(] _[γ]_ [mini-batch] [)]


**For** each data point in mini-batch (Sections IV-B and IV-C):
7: ˆ _x_ _k|k−_ 1 _, P_ _k|k−_ 1 _←_ Predict(DataPoint _, Cov_ 1:12 [DataPoint] )
_x_ ˆ _k|k_ _, P_ _k|k_ _←_ Update(ˆ _x_ _k|k−_ 1 _, z_ _k_ _, Cov_ 13 [DataPoint] )
8: Store current state and covariance estimates.

**End For**

9: Compute loss for the mini-batch see (58)
Loss [mini-batch] _←_ Loss(ˆ _x_ [mini-batch] _, x_ [mini-batch] )
10: Compute and clip gradients:
Gradient [mini-batch] _←_ _[∂]_ [Loss] [mini-batch]

_∂W_ models

�Gradient [mini-batch] _←_ max(Gradient [mini-batch] _,_ 1)

11: Accumulate gradient:
Gradient _←_ Gradient + Gradient [mini-batch]


**End For**

12: Update model weights using ADAM optimizer:
_W_ models _←_ ADAM(Gradient _, W_ models )
13: Reset gradient: Gradient _←_ 0
**End For**


challenges in computing the loss gradient, particularly in step
10 of Algorithm 1. To address these challenges, we employed
an EKF as the filter during the model training phase. This substitution simplifies gradient computation considerably, thereby
enhancing the training efficiency. Despite this modification, we
hypothesize that the model can learn the optimal covariance
matrices corresponding to sensor uncertainties independently
of the filter type used. This hypothesis will be further examined



+ _w_ _v_



2
� _∥v_ _e,k_ _∥_

_d_ [mini-batch]



where _w_ _q_, _w_ _p_, and _w_ _v_ _∈_ R are the weights that determine
the relative importance of each term and are tuned offline.
As the steady-state performance of the filter is of greater
importance than its transient performance, the loss function
in (58) is evaluated starting from the 51st data point onward.
This ensures that the loss function disregards the first 50 data
points, which represent the transient response of the filter.
The V1 02 medium part of the EuRoC dataset [43] has
been utilized for training. This dataset includes IMU measurements, recorded at 200 Hz using the ADIS16448 sensor,
mounted on an MAV. Stereo images are captured as well by
the Aptina MT9V034 global shutter camera at a rate of 20
Hz. Ground truth data is provided at 200 Hz, measured via the
Vicon motion capture system. At each epoch, the whole dataset
will be utilized as a single batch. In other words, given the
initial state estimate and covariance matrix, at each time step,
the filter will estimate the state vector based on the IMU and
landmark measurements, as well as the covariance matrices
found by IMU and Vision-Nets. To manage computational
resources effectively, the data is divided into mini-batches of
size 32. After each mini-batche is processed, the loss value
is found per (58). the gradient of this loss with respect to
the networks weights are found and clipped to one to avoid
gradient explosion. These gradients are accumulated through
mini-batches to find the gradient of the batch. After all the
mini-batches in a batch are processed, the weights are updated
using the Adam optimizer [44]. _L_ 2 regularization [45] has
been performed during the weight training to avoid overfitting.


The IMU and Vision-Networks have 27,276 and 2,901,089
parameters, respectively, with the weights in (58) set to
_w_ _q_ = 1000, _w_ _p_ = 600, and _w_ _v_ = 100. The quaternion-based
UKF involves eigenvalue decomposition in (14) and the use of
singular value decomposition (SVD) for computing the matrix
square root in (31). These operations introduce significant


11


Fig. 6: Validation results of quaternion-based DeepUKF-VIN: The algorithm is evaluated using the V1 02 medium EuRoC
dataset. On the left, the MAV trajectory along with three sample orientations in 3D space is displayed. On the right, the
magnitudes of the orientation (top), position (middle), and velocity (bottom) vectors over time are illustrated.



in Section VII. Given these considerations, the implementation
was carried out using PyTorch for handling the neural network
components and for orientation calculations [46]. The models
were trained over 30 epochs, during which the loss function
converged to its minimum. The convergence behaviour is
illustrated in Fig. 4.
The measurement function is implemented by detecting 2D
feature points in each available vision dataset using the KLT
algorithm. An example of the results of this step is visualized
in Fig. 5. Considering the matched 2D points in the stereo
images and the camera calibration data, the feature points in
the world coordinate frame _{W}_ are computed using triangulation [47]. These computed points serve as the measurement
values in this problem. In summary, the DLAM-equipped
UKF-VIN algorithm, named quaternion-based DeepUKF-VIN,
is illustrated in Fig. 3. The proposed algorithm leverages
IMU-Net and Vision-Net, as described in Sections V-A and
V-B, respectively, to compute the covariance matrices of the
UKF-VIN, as discussed in Section IV. These components are
integrated to enhance the accuracy and performance of the
algorithm. The training algorithm is summarized in Algorithm
1.


VII. E XPERIMENTAL V ALIDATION


To validate the effectiveness of quaternion-based
DeepUKF-VIN, the algorithm is tested using the realworld V1 02 medium EuRoC dataset [43]. For video of the
[experiment, visit the following link. The dataset trajectory, as](https://youtu.be/japfpySxilA)
well as the magnitudes of orientation, position, and velocity
errors defined in (57) over time, are visualized in Fig. 6.
The errors converge rapidly to near-zero values despite



initially high magnitudes, demonstrating quaternion-based
DeepUKF-VIN’s efficacy. To further examine the results, the
individual components of each estimation error are visualized
in Fig. 7. It can be observed that all error components
converge to near-zero values promptly, further underscoring
the effectiveness of the proposed filter.


Fig. 7: Components of the orientation (left), position (middle), and velocity (right) estimation error vectors in the
V1 02 medium EuRoC dataset experiment using quaternionbased DeepUKF-VIN.


To investigate the effectiveness of the proposed IMU and
Vision-Nets, the quaternion-based DeepUKF-VIN was compared to its non-deep counterpart, UKF-VIN, and another
Kalman-type filter with a learning component, the DeepEKF.
The DLAM algroithm were evaluated in two environments,


V1 02 medium and V2 02 medium, which are subsets of
the EuRoC dataset [43]. Note that V2 02 medium dataset
was not utilized during training or validation phases. The
dataset was recorded using an Aptina MT9V034 global shutter
camera, which captured stereo images at a rate of 20 Hz.
Additionally, an ADIS16448 sensor was employed to capture
IMU data at 200 Hz, while ground truth data was recorded
at 200 Hz using the Vicon motion capture system. To ensure
a fair comparison, all filters were configured with the same
nominal covariances as DeepUKF-VIN. Furthermore, in each
environment, all filters were initialized with the same state
vector and covariance matrix. The loss values, as defined in
(58), for the aforementioned filters in both experiments are
presented in Table II. The proposed DeepUKF-VIN outperformed both its non-deep counterpart (UKF-VIN) and the
DeepEKF in terms of loss values across both experiments. The
DeepEKF was exposed to data from the first experiment, while
the DeepUKF-VIN was never trained on either experiment.
Thus, both experiments were entirely novel to the DeepUKFVIN. To further examine these experiments, the MSE values
of the orientation, position, and velocity estimation errors are
tabulated in Table III. It can be observed from Table III that

across both experiments and all components, quaternion-based
DeepUKF-VIN consistently outperformed the non-deep UKFVIN and DeepEKF. Specifically, the DeepUKF-VIN yielded
lower MSE values across all tested experiments and components, demonstrating its superior performance in orientation,
position, and velocity estimation.


TABLE II: Loss Value Comparison of DeepUKF-VIN against
UKF-VIN and DeepEKF.


**Filter** **V1 02 medium** **V2 02 medium**

DeepEKF 1918 834
UKF-VIN 132 251

DeepUKF-VIN 88 250


TABLE III: Components MSEs for the two filters in each
experiment


**Filter** **MSE Element** **V1 02 medium** **V2 02 medium**


Orientation 1.572 0.0544
DeepEKF Position 9.3188 1.1228
Velocity 7.5663 0.9558

Orientation 0.0015 0.0026

UKF-VIN Position 0.0929 0.3070
Velocity 0.0509 0.1319

Orientation 0.0008 0.0080
DeepUKF-VIN Position 0.0806 0.3011
Velocity 0.0282 0.0914


VIII. C ONCLUSION


In this paper, we proposed an adaptively-tuned Deep Learning Unscented Kalman Filter for 3D Visual-Inertial Navigation
(DeepUKF-VIN) to estimate the orientation, position, and
velocity of a vehicle with six degrees of freedom (6-DoF) in
three-dimensional space. By effectively addressing kinematic
nonlinearities through a quaternion-based framework, the algorithm mitigates numerical instabilities commonly associated
with Euler-angle representations. DeepUKF-VIN integrates



12


data from a 6-axis Inertial Measurement Unit (IMU) and stereo
cameras, achieving robust navigation even in GPS-denied environments. The Deep Learning-based Adaptation Mechanism
(DLAM) dynamically adjusts noise covariance matrices based
on sensor data, improving estimation accuracy by responding
adaptively to varying conditions. Evaluated with real-world
data from low-cost sensors operating at low sampling rates,
DeepUKF-VIN demonstrated stability and rapid error attenuation. Comparative testing across two experimental setups
consistently showed that DeepUKF-VIN outperformed the
standard Unscented Kalman Filter (UKF) in all key navigation
components. These results underscore the algorithm’s superior
adaptability, efficacy, and robustness in practical scenarios,
validating its potential for accurate and reliable 3D navigation.
Future work could explore the application of the proposed
DLAM to other Kalman-type and non-Kalman-type filters developed for the VIN problem. Given that the proposed DLAM
was trained using the Extended Kalman Filter (EKF) and
validated with the UKF, it is reasonable to hypothesize that integrating DLAM with other algorithms may yield similar benefits. Furthermore, the vision-based component of the proposed
DLAM could be adapted for alternative sensor inputs, such as
Light Detection and Ranging (LiDAR) and Sound Navigation
and Ranging (SONAR), with minimal modifications. Such
adaptations have the potential to enhance the performance of
algorithms relying on these sensor technologies.


R EFERENCES


[1] H. A. Hashim, “Advances in UAV Avionics Systems Architecture,
Classification and Integration: A Comprehensive Review and Future
Perspectives,” _Results in Engineering_, vol. 25, p. 103786, 2025.

[2] H. A. Hashim, M. Abouheaf, and M. A. Abido, “Geometric Stochastic
Filter with Guaranteed Performance for Autonomous Navigation based
on IMU and Feature Sensor Fusion,” _Control Engineering Practice_, vol.
116, p. 104926, 2021.

[3] Y.-J. Gong, T. Huang, Y.-N. Ma, S.-W. Jeon, and J. Zhang, “Mtrajplanner: A multiple-trajectory planning algorithm for autonomous underwater vehicles,” _IEEE Transactions on Intelligent Transportation Systems_,
vol. 24, no. 4, pp. 3714–3727, 2023.

[4] G. Yang and et al., “Homecare robotic systems for healthcare 4.0:
Visions and enabling technologies,” _IEEE journal of biomedical and_
_health informatics_, vol. 24, no. 9, pp. 2535–2549, 2020.

[5] X. Bai, Y. Ye, B. Zhang, and S. S. Ge, “Efficient package delivery task
assignment for truck and high capacity drone,” _IEEE Transactions on_
_Intelligent Transportation Systems_, vol. 24, no. 11, pp. 13 422–13 435,
2024.

[6] J. Hu, H. Niu, J. Carrasco, B. Lennox, and F. Arvin, “Fault-tolerant
cooperative navigation of networked uav swarms for forest fire monitoring,” _Aerospace Science and Technology_, vol. 123, p. 107494, 2022.

[7] K. N. Braun and C. G. Andresen, “Heterogeneity in ice-wedge permafrost degradation revealed across spatial scales,” _Remote Sensing of_
_Environment_, vol. 311, p. 114299, 2024.

[8] F. Chen and wt al., “Augmented reality navigation for minimally invasive
knee surgery using enhanced arthroscopy,” _Computer Methods and_
_Programs in Biomedicine_, vol. 201, p. 105952, 2021.

[9] R. Korkin, I. Oseledets, and A. Katrutsa, “Multiparticle kalman filter
for object localization in symmetric environments,” _Expert Systems with_
_Applications_, vol. 237, p. 121408, 2024.

[10] S. Wattanarungsan, T. Kuwahara, and S. Fujita, “Magnetometer-based
attitude determination extended kalman filter and optimization techniques,” _IEEE Transactions on Aerospace and Electronic Systems_,
vol. 59, no. 6, pp. 7993–8004, 2023.

[11] X. Hou and J. Bergmann, “Pedestrian dead reckoning with wearable
sensors: A systematic review,” _IEEE Sensors Journal_, vol. 21, no. 1, pp.
143–152, 2021.


[12] H. A. Hashim, A. E. Eltoukhy, and K. G. Vamvoudakis, “UWB Ranging
and IMU Data Fusion: Overview and Nonlinear Stochastic Filter for
Inertial Navigation,” _IEEE Transactions on Intelligent Transportation_
_Systems_, vol. 25, no. 1, pp. 359–369, 2024.

[13] T. M. Roth, F. Freyer, M. Hollick, and J. Classen, “Airtag of the
clones: Shenanigans with liberated item finders,” _2022 IEEE Security_
_and Privacy Workshops (SPW)_, pp. 301–311, 2022.

[14] H. A. Hashim, “Exponentially Stable Observer-based Controller for
VTOL-UAVs without Velocity Measurements,” _International Journal of_
_Control_, vol. 96, no. 8, pp. 1946–1960, 2023.

[15] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
in _Sensor fusion IV: control paradigms and data structures_, vol. 1611.
Spie, 1992, pp. 586–606.

[16] A. Myronenko and X. Song, “Point set registration: Coherent point drift,”
_IEEE transactions on pattern analysis and machine intelligence_, vol. 32,
no. 12, pp. 2262–2275, 2010.

[17] Y. Xu, R. Zheng, S. Zhang, and M. Liu, “Robust inertial-aided underwater localization based on imaging sonar keyframes,” _IEEE Transactions_
_on Instrumentation and Measurement_, vol. 71, pp. 1–12, 2022.

[18] J. A. Christian and S. Cryan, “A survey of lidar technology and its use
in spacecraft relative navigation,” in _AIAA Guidance, Navigation, and_
_Control (GNC) Conference_, 2013, p. 4641.

[19] H. A. Hashim, “GPS-denied Navigation: Attitude, Position, linear Velocity, and Gravity Estimation with Nonlinear Stochastic Observer,” in
_2021 American Control Conference (ACC)_ . IEEE, 2021, pp. 1146–
1151.

[20] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
_International journal of computer vision_, vol. 60, pp. 91–110, 2004.

[21] J. Shi _et al._, “Good features to track,” in _1994 Proceedings of IEEE_
_conference on computer vision and pattern recognition_ . IEEE, 1994,
pp. 593–600.

[22] F. Santoso, M. A. Garratt, and S. G. Anavatti, “Visual–inertial navigation systems for aerial robotics: Sensor fusion and technology,” _IEEE_
_Transactions on Automation Science and Engineering_, vol. 14, no. 1,
pp. 260–275, 2016.

[23] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman
filter for vision-aided inertial navigation,” in _Proceedings 2007 IEEE_
_international conference on robotics and automation_ . IEEE, 2007, pp.
3565–3572.

[24] K. Sun and et al., “Robust stereo visual inertial odometry for fast
autonomous flight,” _IEEE Robotics and Automation Letters_, vol. 3, no. 2,
pp. 965–972, 2018.

[25] A. Odry, R. Fuller, I. J. Rudas, and P. Odry, “Kalman filter for mobilerobot attitude estimation: Novel optimized and adaptive solutions,”
_Mechanical systems and signal processing_, vol. 110, pp. 569–589, 2018.

[26] K. Ghanizadegan and H. A. Hashim, “Quaternion-based Unscented
Kalman Filter for 6-DoF Vision-based Inertial Navigation in GPS-denied
Regions,” _IEEE Transactions on Instrumentation and Measurement_,
vol. 74, no. 1, pp. 1–13, 2025.

[27] H. A. Hashim, L. J. Brown, and K. McIsaac, “Nonlinear Stochastic Attitude Filters on the Special Orthogonal Group 3: Ito and Stratonovich,”
_IEEE Transactions on Systems, Man, and Cybernetics: Systems_, vol. 49,
no. 9, pp. 1853–1865, 2019.

[28] H. A. Hashim, “Systematic Convergence of Nonlinear Stochastic Estimators on the Special Orthogonal Group SO(3),” _International Journal_
_of Robust and Nonlinear Control_, vol. 30, no. 10, pp. 3848–3870, 2020.



13


[29] A. T. Erdem and A. O. Ercan, “Fusing inertial sensor data in an extended
kalman filter for 3d camera tracking,” _IEEE Transactions on Image_
_Processing_, vol. 24, no. 2, pp. 538–548, 2014.

[30] E. A. Wan and R. Van Der Merwe, “The unscented kalman filter,”
_Kalman filtering and neural networks_, pp. 221–280, 2001.

[31] S. Wernitz, E. Chatzi, B. Hofmeister, M. Wolniak, W. Shen, and
R. Rolfes, “On noise covariance estimation for kalman filter-based
damage localization,” _Mechanical Systems and Signal Processing_, vol.
170, p. 108808, 2022.

[32] M. Brossard, A. Barrau, and S. Bonnabel, “Ai-imu dead-reckoning,”
_IEEE Transactions on Intelligent Vehicles_, vol. 5, no. 4, pp. 585–595,
2020.

[33] H. Zhou and et al., “Imu dead-reckoning localization with rnn-iekf
algorithm,” in _2022 IEEE/RSJ International Conference on Intelligent_
_Robots and Systems (IROS)_ . IEEE, 2022, pp. 11 382–11 387.

[34] B. Or and I. Klein, “Learning vehicle trajectory uncertainty,” _Engineer-_
_ing Applications of Artificial Intelligence_, vol. 122, p. 106101, 2023.

[35] S. Yan, Y. Liang, and B. Wang, “Multi-level deep learning kalman
filter,” in _2023 International Conference on Advanced Robotics and_
_Mechatronics (ICARM)_ . IEEE, 2023, pp. 1113–1118.

[36] H. A. Hashim, “Special Orthogonal Group SO(3), Euler Angles, Angleaxis, Rodriguez Vector and Unit-quaternion: Overview, Mapping and
Challenges,” _arXiv preprint arXiv:1909.06669_, 2019.

[37] P. B. Weerakody, K. W. Wong, G. Wang, and W. Ela, “A review of
irregular time series data handling with gated recurrent neural networks,”
_Neurocomputing_, vol. 441, pp. 161–178, 2021.

[38] T. K. Rusch and S. Mishra, “Unicornn: A recurrent model for learning
very long time dependencies,” in _International Conference on Machine_
_Learning_ . PMLR, 2021, pp. 9168–9178.

[39] M. Lechner and R. M. Hasani, “Learning long-term dependencies
in irregularly-sampled time series,” _ArXiv_, vol. abs/2006.04418,
2020. [Online]. Available: [https://api.semanticscholar.org/CorpusID:](https://api.semanticscholar.org/CorpusID:219530825)
[219530825](https://api.semanticscholar.org/CorpusID:219530825)

[40] Y. Cheng, J. Wu, H. Zhu, S. W. Or, and X. Shao, “Remaining useful life
prognosis based on ensemble long short-term memory neural network,”
_IEEE Transactions on Instrumentation and Measurement_, vol. 70, pp.
1–12, 2020.

[41] R. Dey and F. M. Salem, “Gate-variants of gated recurrent unit (gru)
neural networks,” _2017 IEEE 60th International Midwest Symposium_
_on Circuits and Systems (MWSCAS)_, pp. 1597–1600, 2017. [Online].
[Available: https://api.semanticscholar.org/CorpusID:8492900](https://api.semanticscholar.org/CorpusID:8492900)

[42] A. N. Shewalkar, D. Nyavanandi, and S. A. Ludwig, “Performance
evaluation of deep neural networks applied to speech recognition: Rnn,
lstm and gru,” _Journal of Artificial Intelligence and Soft Computing_
_Research_, vol. 9, pp. 235 – 245, 2019.

[43] M. Burri and et al., “The EuRoC micro aerial vehicle datasets,” _The_
_International Journal of Robotics Research_, vol. 35, no. 10, pp. 1157–
1163, 2016.

[44] D. P. Kingma, “Adam: A method for stochastic optimization,” _arXiv_
_preprint arXiv:1412.6980_, 2014.

[45] P. Zhou, X. Xie, Z. Lin, and S. Yan, “Towards understanding convergence and generalization of adamw,” _IEEE Transactions on Pattern_
_Analysis and Machine Intelligence_, 2024.

[46] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson,
and G. Gkioxari, “Accelerating 3d deep learning with pytorch3d,”
_arXiv:2007.08501_, 2020.

[47] R. Hartley and A. Zisserman, _Multiple view geometry in computer vision_ .
Cambridge university press, 2003.


