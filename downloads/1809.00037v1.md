## Estimation for Quadrotors

Stefanie Tellex Andy Brown Sergei Lupashin
Brown University Udacity, Inc. Fotokite


September 5, 2018


This document describes standard approaches for filtering and estimation for quadrotors, created for the
Udacity Flying Cars course. We assume previous knowledge of probability and some knowledge of linear
algebra. We do not assume previous knowledge of Kalman filters or Bayes filters. This document derives an
EKF for various models of drones in 1D, 2D, and 3D. We use the EKF and notation as defined in Thrun
et al. [13]. We also give pseudocode for the Bayes filter, the EKF, and the Unscented Kalman filter [14]. The
motivation behind this document is the lack of a step-by-step EKF tutorial that provides the derivations
for a quadrotor helicopter. The goal of estimation is to infer the drone’s state (pose, velocity, accelleration,
and biases) from its sensor values and control inputs. This problem is challenging because sensors are noisy.
Additionally, because of weight and cost issues, many drones have limited on-board computation so we want
to estimate these values as quickly as possible. The standard method for performing this method is the
Extended Kalman filter, a nonlinear extension of the Kalman filter which linearizes a nonlinear transition
and measurement model around the current state. However the Unscented Kalman filter is better in almost
every respect: simpler to implement, more accurate to estimate, and comparable runtimes.

### 1 Averaging


When performing estimation, the first thing one might think of is averaging the sensor measurements. We
consider averaging for the 1D case, where the range sensor value, z t is directly observing the state (the height
of the drone), x t, both of which are scalars. The notation ˆx t means our estimate of the true state x t . Then
the average is:



xˆ t = [1] 0 [z] [t] (1)

t [Σ] [t]



However this form requires saving all sensor measurements from the beginning of time, requiring memory and
computation that grows linearly in the number of observations. Instead we rewrite the update recursively
in terms of our previous estimate, ˆx t−1, allowing us to transform ˆx t−1 and z t to ˆx:



xˆ t = [1] t [[ˆ][x] [t][−][1] [ ×][ (][t][ −] [1) +][ z] [t] []][ .] (2)



The recursive form, where the next estimate is written in terms of the previous estimate, allows us to perform
a constant time update, and requires us to store a constant amount of information about past sensor values
(only the previous estimate, ˆx t−1 - in other words, our estimate is Markov). However this form updates
slowly when the drone’s state changes. Imagine the drone is stationary for one minute, and then starts
moving. Then [1] t [will be very small, and the estimate will take a long time to move to the new value (longer]

and longer, the longer the drone is running). Instead, we want an average that moves more quickly. One
way to achieve a faster updating average is to simply average the old estimate with the new sensor value:



xˆ t = [x][ˆ] [t][−][1] [ +][ z] [t] (3)

2



A more general formulation is a weighted average of the old value and the new observation:


ˆ
x t = (α)ˆx t−1 + (1 − α)z t (4)


1


This formulation allows the designer to tune the parameter α, which weights how much to weight the old
estimate compared to the new sensor value.

### 2 Bayes Filter


None of the above formulations update the estimate based on the drone’s movements. Intuitively, if we have
told the drone to go up, for instance, then our belief about the drone’s position should also go up. The Bayes
Filter gives us a way to incorporate motion prediction into our state estimate. First we predict the next
state, given a control input, the current state, and a model of how the system evolves over time. We do not
maintain a point estimate but rather a belief or distribution of the estimate. Then, we revise our prediction
with an update from an observation. The update method takes the previous estimate from prediction, and
an observed sensor value. It returns a new distribution that takes into account the sensor value, using a
model of how the sensor works. Below you will see pseudocode for the prediction and update steps for the
filter, following Thrun et al. [13]. In typical use one would call predict after determining the control input,
and update after reading a sensor value. The BayesFilter function is illustrative only; in real life, one
might call predict in the sensor callback for example.


Algorithm 1 General Bayes Filter algorithm. For specific filters such as the Kalman Filter or the particle
filter, the representation for bel and bel [¯] changes, and the corresponding mathematical updates take specific
computational forms.

1: function¯ Predict(bel(x t−1 ), u t, ∆t)

2: bel(x t ) = � p(x t |u t, x t−1 )bel(x t−1 )dx t−1
3: return bel [¯] (x t )

4: function Update( bel [¯] (x t ), z t )
5: bel(x t ) = ηp(z t |x t )bel(x t )

6: return bel(x t )


7: function BayesFilter

8: u¯ t = ComputeControl(bel(x t−1 ))

9: bel(x t ) = Predict(bel(x t−1 ), u t, ∆t)
10: z t = ReadSensor()

11: bel(x t ) = Update( bel [¯] (x t ), z t )

### 3 E(KF)


The Kalman Filter and Extended Kalman Filter make the assumption that the distributions over belief state
are Gaussian, represented as a mean and covariance matrix. Compare to the Bayes’ filter, the distributions
bel and bel [¯] are represented as a mean and covariance matrix. The KF assumes that the transition and
observation models are linear, and can be defined by a matrix. The EKF is the extension to the nonlinear
case, where we take use the Jacobian matrix of the transition and observation functions to compute a pointwise linear estimate, and then do the same updates as the Kalman Filter. We define the Extended Kalman
Filter (EKF) algorithm following Thrun et al. [13]. We refactor it to include separate Predict and Update
methods and to use our notation. We also unify the KF and EKF algorithm pseudocode. The transition
or prediction covariance is Q t ; the measurement covariance is R t . These matrices are often taken to be
constant, but also sometimes people change them over time depending on the sensor model.
The EKF and KF are closely related. For the KF, the matrix G t is constant every iteration of the function
and does not need to be recomputed each time (except for ∆t). Another way to say it is the implementation
of g [′] ignores its state input. Similarly, for the KF, the matrix H t is constant every iteration of the function
and does not need to be recomputed. Another way to say it is the function h [′] ignores its state input. For
the EKF, these matrices change each iteration, because it linearizes around the current state.


2


Algorithm 2 E(KF) algorithm.

1: function Predict(µ t−1, Σ t−1, u t, ∆t)


¯
2: µ t = g(u t, µ t−1 )
3: G t = g [′] (u t, x t, ∆t)

4: ¯Σ t = G t Σ t−1 G [T] t [+][ Q] [t]
5: return ¯µ t, Σ [¯] t

6: function Update(¯µ t, Σ [¯] t, z t )

7: H t = h [′] (¯µ t )

8: K t = Σ [¯] t H t [T] [(][H] [t] [¯Σ] [t] [H] t [T] [+][ R] [t] [)] [−][1]

9: µ t = ¯µ t + K t (z t − h(¯µ t ))

10: Σ t = (I − K t H t )Σ [¯] t
11: return µ t, Σ t


12: function ExtendedKalmanFilter

13: u t = ComputeControl(µ t−1, Σ t−1 )

¯
14: µ t, Σ [¯] t = Predict(µ t−1, Σ t−1, u t, ∆t)

15: z t = ReadSensor()

16: µ t, Σ t = Update(¯µ t, Σ [¯] t, z t )

### 4 Unscented Kalman Filter


The Unscented Kalman Filter [14] is similar to the EKF in that it handles nonlinear transition models g and
measurement models h. However there are no Jacobians! Instead of linearizing around the current estimate,
the Unscented Kalman Filter picks magic “sigma points” which are sample points chosen according to the
current state estimate and covariance. Then these sigma points are passed through the nonlinear transition
or observation function, and the sample mean and sample covariance of the sigma points is used to construct
a new Gaussian µ and σ. The pseudocode here follows Kandepu et al. [9] but removes the augmentation for
tracking moving prediction and measurement covariance, and uses our notation.
Sigma points are computed using the matrix S which is defined from the covariance matrix, Σ t . S i
denotes the ith colum of the matrix S.


S = �Σ t (5)


We define the sigma points X i,t ∈ X t as follows:



X i,t =










= µ t, i = 0
= µ t + γS i, i = 1, . . ., N (6)
= µ t − γS i−N, i = N + 1, . . ., 2N



Note that they are defined using the mean and covariance matrix of the distributions; by picking several
representative points S i away from the mean, we can use a relatively small set of points to represent the
entire distribution. The weights when computing the sample mean, w i [m] are:



w i [m] [=]



= λ i = 0
N +λ
� = 2(N1+λ) [,] i = 1, . . ., 2N (7)



The weights when computing the sample covariance, w i [c] [are:]



w i [c] [=]



= λ i = 0
N +λ [+ (1][ −] [α] [2] [ +][ β][)]
� = 2(N1+λ) [,] i = 1, . . ., 2N (8)



The parameters are defined as:



γ = √



N + λ (9)



λ = α [2] (N + κ) − N (10)


3


See Kandepu et al. [9] for tuning suggestions when implementing the filter.
The pseudocode for the Unscented Kalman Filter is given in Algorithm 3. Compared to the Bayes’ filter,
we use the computed sigma points to represent the distribution bel [¯] . However we use a mean and covariance
to represent the distribution bel at the beginning and end of each iteration of the filter.


Algorithm 3 Unscented Kalman Filter.

1: function ComputeSigmas(µ t, Σ t )
2: return X 0,t, . . ., X 2N,t following Equation 6.

3: function Predict(µ t−1, Σ t−1, u t, ∆t)
4: X t−1 = ComputeSigmas(µ t−1, Σ t−1 )

5: ∀ [2] i=0 [N] [X][¯] [i,t] [ =][ g][(][X] [i,t][−][1] [, u] [t] [,][ ∆][t][)]

6: return X [¯] t

8:7: functionµ¯ t = [�] Update [2] i=0 [N] [(][w] i [m] X( ¯X [¯] i,tt, z) t )

9: ¯Σ t = [�] [2] i=0 [N] [w] i [c] [(][X] [i,t] [ −] [µ][¯] [t] [)(][X] [i,t] [ −] [µ][¯] [t] [)] [T] [ +][ Q] [t]
10: ∀ [2] i=1 [N] [Z] [i,t] [ =][ h][( ¯][X] [i,t] [)]
11: µ [z] = Σ [2] i=0 [N] [w] i [m] [Z] [i,t]
12: Σ [z] t [= Σ] [2] i=0 [N] [w] i [c] [(][Z] [i,t] [ −] [µ] [z] [)(][Z] [i,t] [ −] [µ] [z] [)] [T] [ +][ R] [t]
13: Σ [xz] t = Σ [2] 0 [N] [w] i [c] [( ¯][X] [i,t] [ −] [µ][¯] [t] [)(][Z] [i,t] [ −] [µ] [z] [)] [T]

14: K t = Σ [xz] t [(Σ] [z] t [)] [−][1]

15: µ t = ¯µ t + K t (z t − µ [z] )
16: Σ t = Σ [¯] t − K t Σ [z] t [K] t [T]
17: return µ t, Σ t


18: function UnscentedKalmanFilter
19: u¯ t = ComputeControl(µ t−1, Σ t−1 )

20: X t = Predict(µ t−1, Σ t−1, u t, ∆t)

21: z t = ReadSensor()
22: µ t, Σ t = Update( X [¯] t, z t )

### 5 One Dimensional Quad


To implement the filters on specific vehicles, we need to define the state transition function, g and the
measurement model, h. We will do this three times for increasingly more realistic models of a quadrotor.
First, we define a 1D quad model, where the quadroter is moving in z, but not in x or y. It has one control
input, the downward pointing thrust, and a noisy range sensor. The intention is that this is identical to the
1D quad used in the controls lesson. The state is then the position, z and velocity, ˙z:



z˙
x t =
z
�



(11)
�



We define the control input as directly setting the accelleration, ¨z:


u t = � z¨ � (12)


5.1 Transition Model


Then we define a transition function g(x t, u t, ∆t) which returns a new x t+1 :



g(x t, u t, ∆t) = x t, ˙z + u t,z¨ × ∆t
� x t,z + x t, ˙z × ∆t



(13)
�



1 0
= ∆t 1
�


4



z˙

z
��



∆t
+
0
� �



z¨ � (14)
��


We can rewrite it in terms of the A t and B t matrix, as in a conventional Kalman filter. In this form we see
that the control update is linear because it can be written in this form.


= A t x t + B t u t (15)


Then x t ∈ R [2] and g(x t, u t, ∆t) ∈ R [2] . So g [′] (x t, u t ) is a 2 × 2 matrix, defined as the partial derivative of
g with respect to x t for each component in x t .



�



g [′] (x t, u t, ∆t) =



∂ ∂
∂x t,z ˙ [g] [ ˙][z] [(][x] [t] [, u] [t] [,][ ∆][t][)] ∂x t,z [g] [ ˙][z] [(][x] [t] [, u] [t] [,][ ∆][t][)]
∂ ∂
� ∂x t,z ˙ [g] [z] [(][x] [t] [, u] [t] [,][ ∆][t][)] ∂x t,z [g] [z] [(][x] [t] [, u] [t] [,][ ∆][t][)]



(16)



1 0
= ∆t 1
�



(17)
�

(18)



Since this function is linear, the Jacobian is a constant matrix except for ∆t, and just reduces to the A t
matrix.


5.2 Measurement Model


Next we assume the drone has a range sensor pointed downwards at the ground, at z = 0. Then z t is the
range value, r:


z t = � r � (19)


Then we define a measurement function h(x t ) which returns a new z t :


h(x t ) = � x t,z � (20)



= � 0 1 [��] [z] z [˙]



(21)
�



(22)
�



= C t



z˙

z
�



= C t x t (23)


Finally we define h [′] (x t ), the Jacobian of h with respect to x t . The Jacobian is a 1 × 2 matrix.


h [′] (x t ) = � ∂x∂ t,z ˙ [h] [r] [(][x] [t] [)] ∂x∂ t,z [h] [r] [(][x] [t] [)] � (24)

= � 0 1 [�] (25)


Note that in this case the Jacobian does not depend at all on the input x t . This is because this system
is linear, so the EKF linearization will boil back down to a regular Kalman filter.

### 6 Two Dimensional Quad


We define a 2D quad model where the quadrotor is operating at a fixed height z. It has a range sensor
pointing sideways, and it can move by rotating about its center, the angle φ. Additionally it can move right
and left in y. Then we define the state x t as the state transition function as follows:





(26)




x t =



φ

˙



y
 y



We define the control input as directly setting the angle, φ.


u t = � φ � (27)


5


6.1 Transition Model


Then we define g(x t, u t, ∆t) and returns a new x t+1 :





(28)




g(x t, u t, ∆t) =



u t,φ

 x t, ˙y − sin(x t,φ ) × ∆t

 x t,y + x t, ˙y × ∆t



Then x t ∈ R [3] and g(x t, u t, ∆t) ∈ R [3] . So g [′] (x t, u t ) is a 3 × 3 matrix.
Finally, we take the partial derivative of g with respect to x t for each component in x t .



∂ ∂ ∂
∂x t,φ [g] [φ] ∂x t,y ˙ [g] [φ] ∂x t,y [g] [φ]
∂ ∂ ∂
∂x t,φ [g] [ ˙][y] ∂x t,y ˙ [g] [ ˙][y] ∂x t,y [g] [ ˙][y]
∂ ∂ ∂
∂x t,φ [g] [y] ∂x t,y ˙ [g] [y] ∂x t,y [g] [y]



∂ ∂ ∂
∂x t,φ [u] [t,φ] ∂x t,y ˙ [u] [t,φ] ∂x t,y [u] [t,φ]
∂ ∂ ∂
∂x t,φ [x] [t,][ ˙][y] [ −] [sin(][x] [t,φ] [)][ ×][ ∆][t] ∂x t,y ˙ [x] [t,][ ˙][y] [ −] [sin(][x] [t,φ] [)][ ×][ ∆][t] ∂x t,y [x] [t,][ ˙][y] [ −] [sin(][x] [t,φ] [)][ ×][ ∆][t]
∂ ∂ ∂
∂x t,φ [x] [t,y] [ +][ x] [t,][ ˙][y] [ ×][ ∆][t] ∂x t,y ˙ [x] [t,y] [ +][ x] [t,][ ˙][y] [ ×][ ∆][t] ∂x t,y [x] [t,y] [ +][ x] [t,][ ˙][y] [ ×][ ∆][t]





 (29)





 (30)



g [′] (x t, u t, ∆t) =


=


=













 − cos(x0 t,φ )∆t 01 00

0 ∆t 1






(31)




6.2 Measurement Model


Next we assume the drone has a range sensor pointed sideways at a wall y = 5. Then z t is the range value, r:


z t = � r � (32)


Then we define h(x t ) and returns a new z t :


h(x t ) = � wallcos( y x− t,φ x t,y ) � (33)


Finally we define h [′] (x t ), the Jacobian of h with respect to x t .


h [′] (x t ) = � ∂x∂ t,φ [h] [r] [(][x] [t] [)] ∂x∂ t,y ˙ [h] [r] [(][x] [t] [)] ∂x∂ t,y [h] [r] [(][x] [t] [)] � (34)

= � [wall y cos−x [2] t,y (x] sin( t,φ ) x t,φ ) 0 cos(−x1 t,φ ) � (35)


Using the above math, we have implemented an EKF in Python for this model, and showed it is able to
estimate position and velocity using the drone’s simulated noisy range sensor.

### 7 Three Dimensional Quad


The state will be position and velocity which we will estimate with the GPS. We will use the magnetometer
to estimate yaw. We will represent position and velocity in the global frame, and yaw that goes from body
to local in the state. We would also then use a multirate Kalman Filter [4, 12] to perform updating, with
separate updates for the GPS, the magnetometer, and the IMU (accelerometer plus rate gyro).
We track yaw as the heading from magnetic north. So it is the reading one would get if one reads from

a compass.


6


Variable Description


u t The control input at time t.
x t The state at time t.
z t The observation at time t.

The scalar value of the state vector at the index cor
x t, variable responding to variable. Similar notation for u t and

z t .
∆t The elapsed time between updates in seconds.
Q t Transition model covariance
R t Measurement model covariance
φ Roll
θ Pitch

ψ Yaw
R bg Rotation matrix from body to global
R gy Rotation matrix from global to yaw frame
x [y] the x coordinate in the yaw frame.
x The x coordinate in the global frame.
x [b] The x coordinate in the body frame.
g(x t, u t, ∆t) The transition function.
h(x t ) The observation function.


Table 1: Table of variables.









x t =









x

y

z

x˙

y˙
z˙

ψ



(36)



Then u t is the acceleration in the body frame, where ψ [˙] is global frame yaw.





 (37)



x¨ [b]

y¨ [b]

z¨ [b]
ψ˙



u t =









7.1 Attitude Filter


Markley [10] gives an EKF in quaternions for attitude estimation. Higgins [7] compares complementary
filters to Kalman filters, in a way not specific to quads or attitude estimation. Quan [12] says to use either
a complementary filter or Kalman filter for attitude estimation and gives very terse, hard to understand
math. Johansen and Kristiansen [8] describes the MEKF model used in a quadrotor with adaptive fading.
Crassidis et al. [3] gives a survey of nonlinear attitude estimation including the MEKF. Nowicki et al. [11]
compares the complementary filter with an EKF for attitude estimation on mobile phones. They find that the
complementary filter is simpler to implement, but the EKF is able to achieve in most cases better accuracy.
Both have comparable processor loads.
We assume the state we are tracking is the vehicle’s attitude, that is roll φ and pitch, θ. Then the obser

7


vation, z t consists of the gyro angular velocity and pitch and roll angles as estimated from the accelerometer.





 (38)



z t =









θ

φ

p

q



The accelerometer estimates for θ and φ are in the global frame, but the velocities from the gyro are in
the body frame. Our approach follows the Linear Complementary Filter from Quan [12]. We assume that
θ and φ are small, so that the turn rates measured by the gyro in the body frame approximate the global
turn rates. In other words,


φ˙˙ p

   



φ˙˙

 θ

 ψ˙



 ≈











(39)








p

q

r




The state is the roll angle and pitch angle:


7.1.1 Linear Complementary Filter



θ
x t = � φ



(40)
�



We define a linear complementary filter following Quan [12]. Here τ is a time constant and T s is the filter
sampling period:



ˆ T s
�θ t−1 + T s z t, ˙θ � + τ + T s z t,θ (41)


ˆ T s
�φ t−1 + T s z t, ˙φ � + τ + T s z t,φ (42)



Similarly for roll:



ˆ τ
θ t =
τ + T s


ˆ τ
φ t =
τ + T s



We do not estimate the yaw with a complementary filter because we will use the magnetometer and do
it in the GPS.
The above math assumes that the angular velocity (which is in body frame) can be used directly as
angular rotation.


7.1.2 Nonlinear Complementary Filter


For the nonlinear complementary filter, following Quan [12] Section 9.1.3, we use the state to define a
quaternion, q t, for the euler angles for φ, θ and ψ. Then we can define dq to be the quaternion that consists
of the measurement of the angular rates from the IMU in the body frame, following Equation 84 in Diebel

[5]. Using these two, we can define a predicted quaternion, ¯q t as follows:


q¯ t = dq ∗ q t (43)


Finally we can define θ [¯] t and φ [¯] t as follows:


θ¯ t = Pitch(¯q t ) (44)
φ¯ t = Roll(¯q t ) (45)


8


Using these predicated estimates, we can compute the non-linear complementary filter as above.



¯ T s
�θ t−1 + T s z t, ˙θ � + τ + T s z t,θ (46)


¯ T s
�φ t−1 + T s z t, ˙φ � + τ + T s z t,φ (47)



Similarly for roll:


7.2 Transition Model



ˆ τ
θ t =
τ + T s


ˆ τ
φ t =
τ + T s



We define the transition function in terms of the rotation matrix R bg which rotates from the body frame to
the global frame. As described in Diebel [5], there are 12 different orders one could perform the rotation; we
follow the convention from aerospace of using the 1, 2, 3 order for roll, pitch, and yaw.
This matrix is defined as follows, taken from the transpose (or inverse) of Diebel [5], equation 67.





(48)




R bg =



 coscos θ θ cos sin ψ ψ sinsin φ φ sin sin θ θ cos sin ψ ψ + cos − cos φ φ cos sin ψ ψ coscos φ φ sin sin θ θ cos sin ψ ψ − + sinsin φ φ cos sin ψ ψ

 − sin θ cos θ sin φ cos θ cos φ



Then the transition function is:


g(x t, u t, ∆t) =


Then we take the Jacobian:









x t,x + x t, ˙x ∆t
x t,y + x t, ˙y ∆t
x t,z + x t, ˙z ∆t

x t, ˙x

x t, ˙y
x t, ˙z − g∆t

x t,ψ















+









0 0 0 0

0 0 0 0

0 0 0 0
R bg [0 :] 0
R bg [1 :] 0
R bg [2 :] 0
0 0 0 1



u t ∆t (49)









g [′] (x t, u t, ∆t) =


=













1 0 0 ∆t 0 0 0

0 1 0 0 ∆t 0 0

0 0 1 0 0 ∆t 0
0 0 0 1 0 0 ∂
∂x t,ψ [(][x] [t,][ ˙][x] [ +][ R] [bg] [[0 :]][u] [t] [[0 : 3]∆][t][)]
0 0 0 0 1 0 ∂
∂x t,ψ [(][x] [t,][ ˙][y] [ +][ R] [bg] [[1 :]][u] [t] [[0 : 3]∆][t][)]
0 0 0 0 0 1 ∂
∂x t,ψ [(][x] [t,][ ˙][z] [ +][ R] [bg] [[2 :]][u] [t] [[0 : 3]∆][t][)]
0 0 0 0 0 0 1



1 0 0 ∆t 0 0 0

0 1 0 0 ∆t 0 0

0 0 1 0 0 ∆t 0

0 0 0 1 0 0 R [′]
bg [[0 :]][u] [t] [[0 : 3]∆][t]
0 0 0 0 1 0 R [′]
bg [[1 :]][u] [t] [[0 : 3]∆][t]
0 0 0 0 0 1 R [′]
bg [[2 :]][u] [t] [[0 : 3]∆][t]
0 0 0 0 0 0 1









We define R bg [′] [as] ∂x∂ t,ψ [, defined as Equation 71 from Diebel [5]:]



(50)


(51)




(52)




R bg [′] [=]



 −coscos θ cosθ sin ψ ψ −sinsin φ φ sin sin θ θ cos sin ψ ψ − −coscos φ φ sin cos ψ ψ −coscosφ φ sin sin θ θ cos sin ψ ψ + sin + sin φ φ sin cos ψ ψ

0 0 0



9


7.3 Measurement Model


We provide measurement models for the GPS and Magnetometer. We use the IMU as a control input so do
not provide a measurement model for it here.


7.3.1 GPS


We assume we get position and velocity from the GPS. We considered using heading from the GPS, but this
does not take into account the drone’s orientation, only the direction of travel. Hence we are removing it
from the observation.




















x

y

z

x˙

y˙
z˙



x t,x

x t,y

x t,z

x t, ˙x

x t, ˙y

x t, ˙z



Then the measurement model is:



z t =


h(x t ) =









Then the partial derivative is the identity matrix, augmented with a vector of zeros for ∂x∂ t,φ [h][(][x] [t] [):]



(53)


(54)


(55)









h [′] (x t ) =









1 0 0 0 0 0 0

0 1 0 0 0 0 0

0 0 1 0 0 0 0

0 0 0 1 0 0 0

0 0 0 0 1 0 0

0 0 0 0 0 1 0



7.3.2 Magnetometer


We assume we get a reading from the magnetometer reporting yaw in the global frame. (This measurement
may need to be computed using roll and pitch from the attitude filter and the mag vector.)


z t = � ψ � (56)


h(x t ) = � x t,ψ � (57)


Again since this is linear, the derivative is a matrix of zeros and ones.


h [′] (x t ) = � 0 0 0 0 0 0 1 � (58)


10


7.4 Further Information


Based on not tracking acceleration as part of the state, we will use the accelerometer and gyro inputs as
control inputs. Note that Erdem and Ercan [6] describe using the accelerometer inputs in the measurement
or control input phases for an EKF over camera and IMU, for all eight combinations. They show that it is
always better to fuse both sensors in the measurement stage. We would also then use a multirate Kalman
Filter [4, 12] to perform updating, with separate updates for the GPS, magnetometer, and IMU.
However this means that both acceleration and angular velocity appear in the state vector, which we
decided not to do in order to simplify the math and implementation. As one example, Ardupilot does it in
this way [1]. The advantage is that it does not need to connect as deeply to the control system. Additionally,
one could even use different state transition models in the control compared to the EKF. One might make
this decision for computational reasons, for example.
Erdem and Ercan [6] give the math for an EKF with an IMU and camera (on a mobile device), showing
the IMU and gyro measurements treated as a prediction and measurement input (all eight combinations).
They show that the best perforance is obtained using the IMU as a measurement input. However the
difference in pose accuracy estimation is around 1 cm. Note that the Ardupilot open source code base [1]
makes a different decision, using the IMU as the control/prediction update, and that Bry et al. [2] describes
this decision as a “commonly-used technique.”
We will use the North/East/Down frame where the positive x gives the distance along the surface of the
earth in the direction of north; the y coordinate gives the distance in the direction of east, and z is altitude,
which is negative for distances above the surface of the earth.
We define an intermediate frame, p, which is a quaternion initialized from the roll, θ, and the pitch, φ,
from the complementary filter, and no yaw correction. We define the global frame, q, as a quaternion filled
out with the roll, θ, the pitch, φ, and the yaw, ψ, from the EKF.

### 8 Conclusion


We have provided equations for filtering for a quadrotor helicopter, combining information and notation
from Thrun et al. [13], Quan [12] and other sources.

### References


[1] Extended kalman filter navigation overview and tuning, Accessed March 5, 2018.
http://ardupilot.org/dev/docs/extended-kalman-filter.html#extended-kalman-filter.


[2] Adam Bry, Abraham Bachrach, and Nicholas Roy. State estimation for aggressive flight in gps-denied
environments using onboard sensing. In Robotics and Automation (ICRA), 2012 IEEE International
Conference on, pages 1–8. IEEE, 2012.


[3] John L Crassidis, F Landis Markley, and Yang Cheng. Survey of nonlinear attitude estimation methods.
Journal of guidance, control, and dynamics, 30(1):12–28, 2007.


[4] Roberto Cristi and Murali Tummala. Multirate, multiresolution, recursive kalman filter. Signal Processing, 80(9):1945–1958, 2000.


[5] James Diebel. Representing attitude: Euler angles, unit quaternions, and rotation vectors. Matrix, 58
(15-16):1–35, 2006.


[6] Arif Tanju Erdem and Ali Ozer Ercan. Fusing inertial sensor data in an extended kalman filter for 3d [¨]
camera tracking. IEEE Transactions on Image Processing, 24(2):538–548, 2015.


[7] Walter T Higgins. A comparison of complementary and kalman filtering. IEEE Transactions on
Aerospace and Electronic Systems, (3):321–325, 1975.


[8] Tor-Aleksander Johansen and Raymond Kristiansen. Quadrotor attitude estimation using adaptive
fading multiplicative ekf. In American Control Conference (ACC), 2017, pages 1227–1232. IEEE, 2017.


11


[9] Rambabu Kandepu, Bjarne Foss, and Lars Imsland. Applying the unscented kalman filter for nonlinear
state estimation. Journal of process control, 18(7-8):753–768, 2008.


[10] F Landis Markley. Attitude error representations for kalman filtering. Journal of guidance, control, and
dynamics, 26(2):311–317, 2003.


[11] Micha�l Nowicki, Jan Wietrzykowski, and Piotr Skrzypczy´nski. Simplicity or flexibility? complementary
filter vs. ekf for orientation estimation on mobile devices. In Cybernetics (CYBCONF), 2015 IEEE 2nd
International Conference on, pages 166–171. IEEE, 2015.


[12] Quan Quan. Introduction to multicopter design and control. Springer, 2017.


[13] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005.


[14] Eric A Wan and Rudolph Van Der Merwe. The unscented kalman filter for nonlinear estimation. In
Adaptive Systems for Signal Processing, Communications, and Control Symposium 2000. AS-SPCC.
The IEEE 2000, pages 153–158. Ieee, 2000.


12


