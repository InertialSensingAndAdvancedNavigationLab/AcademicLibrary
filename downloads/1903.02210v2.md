# RINS-W: Robust Inertial Navigation System on Wheels

Martin B ROSSARD _[∗]_, Axel B ARRAU _[†]_ and Silv`ere B ONNABEL _[∗]_

_∗_ MINES ParisTech, PSL Research University, Centre for Robotics, 60 Boulevard Saint-Michel, 75006 Paris, France

_†_ Safran Tech, Groupe Safran, Rue des Jeunes Bois-Chˆateaufort, 78772, Magny Les Hameaux Cedex, France



_**Abstract**_ **—This paper proposes a real-time approach for long-**
**term inertial navigation based** _**only**_ **on an Inertial Measurement**
**Unit (IMU) for self-localizing wheeled robots. The approach**
**builds upon two components: 1) a robust detector that uses**
**recurrent deep neural networks to dynamically detect a variety**
**of situations of interest, such as zero velocity or no lateral**
**slip; and 2) a state-of-the-art Kalman filter which incorporates**
**this knowledge as pseudo-measurements for localization. Eval-**
**uations on a publicly available car dataset demonstrates that**
**the proposed scheme may achieve a final precision of 20 m for**
**a 21 km long trajectory of a vehicle driving for over an hour,**
**equipped with an IMU of moderate precision (the gyro drift**
**rate is 10 deg/h). To our knowledge, this is the first paper which**
**combines sophisticated deep learning techniques with state-of-**
**the-art filtering methods for pure inertial navigation on wheeled**
**vehicles and as such opens up for novel data-driven inertial**
**navigation techniques. Moreover, albeit taylored for IMU-only**
**based localization, our method may be used as a component**
**for self-localization of wheeled robots equipped with a more**
**complete sensor suite.**
_**Index Terms**_ **—inertial navigation, deep learning, invariant**
**extended Kalman filter, autonomous vehicle, inertial odometry**


I. I NTRODUCTION


Inertial navigation, or “inertial odometry”, uses an Inertial
Measurement Unit (IMU) consisting of accelerometers and
gyrometers to calculate in real time by dead reckoning the
position, the orientation, and the 3D velocity vector of a
moving object without the need for external references or
extra sensors. Albeit a mature field, the development of lowcost and small size inertial sensors over the past two decades
has attracted much interest for robotics and autonomous

systems applications, see e.g. [1]–[3].
High precision Inertial Navigation Systems (INS) achieve
very small localization errors but are costly and rely on timeconsuming initialization procedures [4]. In contrast, MEMSbased IMU suffer from large errors such as scale factor, axis
misalignment, thermo-mechanical white noise and random
walk noise, resulting in rapid localization drift. This prompts
the need for fusion with complementary sensors such as GPS,
cameras, or LiDAR, see e.g., [5]–[7].
In this paper, we provide a method for long-term localization for wheeled robots only using an IMU. Our contributions, and the paper’s organization, are as follows:


_•_
we introduce specific motion profiles frequently encountered by a wheeled vehicle which bring information
about the motion when correctly identified, along with
their mathematical description in Section III;




_•_ we design an algorithm which automatically detects
in real time if any of those assumptions about the
motion holds in Section IV-A, based only on the IMU
measurements. The detector builds upon recurrent deep
neural networks [8] and is trained using IMU and ground
truth data;

_•_ we implement a state-of-the-art invariant extended
Kalman filter [9,10] that exploits the detector’s outputs
as pseudo-measurements to combine them with the IMU
outputs in a statistical way, in Section IV-B. It yields
accurate estimates of pose, velocity and sensor biases,
along with associated uncertainty (covariance);

_•_ we demonstrate the performances of the approach on
a publicly available car dataset [11] in Section V. Our
approach solely based on the IMU produces accurate
estimates with a final distance w.r.t. ground truth of 20 m
on the 73 minutes test sequence urban16, see Figure
1. In particular, our approach outperforms differential
wheel odometry, as well as wheel odometry aided by
an expensive fiber optics gyro whose drift is 200 times
smaller than the one of the IMU we use;


_•_
this evidences that accurately detecting some specific
patterns in the IMU data and incorporating this information in a filter may prove very fruitful for localization;

_•_ the method is not restricted to IMU only based navigation. It is possible to feed the Kalman filter with other
sensors’ measurements. Besides, once trained the detector may be used as a building block in any localization
algorithm.


_A. Related Works_


Odometry and localization based on inertial sensors, cameras, and/or LIDARs have made tremendous progresses over
the last decade, enabling robust real-time localization systems, see e.g., [5]–[7]. As concerns back-end techniques, although optimization-based methods tend to presently prevail,
filtering based methods building upon the Invariant Extended
Kalman Filter (IEKF) [9,10] are currently gaining interest,
owing to its theoretical guarantees in terms of convergence
and consistency. The IEKF [9] has already given rise to a
commercial product in the field of high precision navigation,
see [10,12]. It has also been successfully applied to visual
inertial odometry, in [13]–[15] and bipedal robotics [16].
Taking into account vehicle constraints and odometer measurements are known to increase the robustness of visual

1


0 _._ 8


0 _._ 6


0 _._ 4


0 _._ 2


0


_−_ 0 _._ 2


_−_ 0 _._ 4


_−_ 0 _._ 6


_−_ 0 _._ 8


_−_ 1















_−_ 1 _._ 2


_−_ 0 _._ 4 _−_ 0 _._ 2 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 1 _._ 2 1 _._ 4 1 _._ 6 1 _._ 8 2 2 _._ 2 2 _._ 4 2 _._ 6


_x_ (km)


Fig. 1. Trajectory ground truth and estimates obtained by various methods: integration of IMU signals; odometry based on a differential wheel encoder
system; odometry combined with an highly accurate and expensive Fiber optics Gyro (FoG) that provides orientation estimates; and the proposed RINS-W
approach which considers only the IMU sensor embarked in the vehicle, and which outperforms the other schemes. The final distance error for this
long-term sequence urban16 (73 minutes) of the car dataset [11] is 20 m for the RINS-W solution. The deep learning based detector (see Section IV-A)
has of course _not_ been trained or cross-validated on this sequence.



inertial systems [17,18]. Although quite successful, systems
using vision continuously process a large amount of data
which is computationally demanding and energy consuming.
Moreover, an autonomous vehicle should run in parallel its
own robust IMU-based localization algorithm to perform
maneuvers such as emergency stops if LiDAR and camera
are unavailable due to lack of texture or information, and
more generally failures [1].
Inertial navigation systems have long leveraged pseudomeasurements from IMU signals, e.g. the widespread Zero
velocity UPdaTe (ZUPT) [19,20]. Upon detection of a zerovelocity event, such pseudo-measurement can be fused with
the dead reckoning motion model in an extended Kalman
filter [21,22] or in a factor graph, see [23].
Very recently, great efforts have been devoted to the
use of deep learning and more generally machine learning
frameworks for pedestrian inertial navigation [24]–[28]. In

[24] velocity is estimated using support vector regression
whereas [25]–[27] use recurrent neural networks respectively
for ZUPT detection, speed estimation, and end-to-end inertial navigation. Those methods are promising but difficult
to transfer to a wheeled robots since they generally only
consider horizontal planar motion, and must infer velocity directly from a small sequence of IMU measurements, whereas
we can afford to use larger sequences. Finally, in [29], we
used Gaussian Processes to learn and correct wheel encoders

errors to improve wheel encoder based dead reckoning in 2D.



II. I NERTIAL N AVIGATION S YSTEM & S ENSOR M ODEL


Denoting the IMU orientation by **R** _n_ _∈_ _SO_ (3), i.e. the
rotation matrix that maps the body frame to the world frame
W, its velocity in W by **v** _n_ [W] _[∈]_ [R] [3] [ and its position in] [ W] [ by]
**p** _n_ [W] _[∈]_ [R] [3] [, the dynamics are as follows]


**R** _n_ +1 = **R** _n_ exp _SO_ (3) ( _**ω**_ _n_ _dt_ ) _,_ (1)

**v** _n_ [W] +1 [=] **[ v]** _n_ [W] [+ (] **[R]** _[n]_ **[a]** _[n]_ [+] **[ g]** [)] _[ dt,]_ (2)

**p** _n_ [W] +1 [=] **[ p]** _n_ [W] [+] **[ v]** _n_ [W] _[dt,]_ (3)


between two discrete times, with sampling time _dt_, and where
( **R** 0 _,_ **v** 0 [W] _[,]_ **[ p]** 0 [W] [)][ is the initial configuration. The true angular]
velocity _**ω**_ _n_ _∈_ R [3] and the true specific acceleration **a** _n_ _∈_ R [3]

are the inputs of the system (1)-(3). In our application scenarios, the effects of earth rotation and Coriolis acceleration
are ignored, and earth is considered flat.


_A. Inertial Measurement Unit (IMU) Model_


The IMU provides noisy and biased measurements of _**ω**_ _n_ _∈_
R [3] and **a** _n_ _∈_ R [3] as follows


_**ω**_ _n_ [IMU] = _**ω**_ _n_ + **b** _**[ω]**_ _n_ [+] **[ w]** _n_ _**[ω]**_ _[,]_ (4)

**a** _n_ [IMU] = **a** _n_ + **b** **[a]** _n_ [+] **[ w]** _n_ **[a]** _[,]_ (5)


where **b** _**[ω]**_ _n_ [,] **[ b]** **[a]** _n_ [are quasi-constant biases and] **[ w]** _n_ _**[ω]**_ [,] **[ w]** _n_ **[a]** [are]
zero-mean Gaussian noises. The biases follow a random-walk


**b** _**[ω]**_ _n_ +1 [=] **[ b]** _**[ω]**_ _n_ [+] **[ w]** _n_ **[b]** _[ω]_ _[,]_ (6)


**b** **[a]** _n_ +1 [=] **[ b]** **[a]** _n_ [+] **[ w]** _n_ **[b]** **[a]** _[,]_ (7)


where **w** _n_ **[b]** _**[ω]**_ [,] **[ w]** _n_ **[b]** **[a]** are zero-mean Gaussian noises.
All sources of error - particularly biases - are yet undesirable since a simple implementation of (1)-(3) leads to a
triple integration of raw data, which is much more harmful
that the unique integration of differential wheel speeds. Even
a small error may thus cause the position estimate to be way
off the true position, within seconds.


III. S PECIFIC M OTION P ROFILES FOR W HEELED

S YSTEMS


We describe in this section characteristic motions fre
quently encountered by a wheeled robot, that provide useful complementary information to the IMU when correctly
detected.


_A. Considered Motion Profiles_

We consider 4 distinct specific motion profiles, whose
validity is encoded in the following binary vector:


**z** _n_ = ( _z_ _n_ [VEL] _[, z]_ _n_ [ANG] _, z_ _n_ [LAT] _[, z]_ _n_ [UP] [)] _[ ∈{]_ [0] _[,]_ [ 1] _[}]_ [4] _[.]_ (8)


_•_ **Zero velocity** : the velocity of the platform is null. As a
consequence so is the linear acceleration, yielding:

_z_ _n_ [VEL] = 1 _⇒_ � **R** _n_ **av** _nn_ + = **g 0** = **0** _[.]_ (9)

Such profiles frequently occur for vehicles moving in
urban environments, and are leveraged in the well known
Zero velocity UPdaTe (ZUPT), see e.g. [19,20].

_•_ **Zero angular velocity** : the heading is constant:


_z_ _n_ [ANG] = 1 _⇒_ _**ω**_ _n_ = **0** _._ (10)


_•_ **Zero lateral velocity** [1] : the lateral velocity is considered
roughly null


_z_ _n_ [LAT] = 1 _⇒_ _v_ _n_ [LAT] _≃_ 0 _,_ (11)


where we obtain the lateral velocity _v_ _n_ [LAT] after expressing
the velocity in the body frame B as




_·_ 10 _[−]_ [4]



8





 _._ (12)





6


4


2


0


0 1 2 3 4 5 6 7 8


_t_ (s)


Fig. 2. Real IMU data of a car stopping from sequence urban06 of [11].
We see (9) holds and thus _z_ _n_ [VEL] = 1 at _t_ = 5 _._ 8 _s_ while (10) does not hold
yet.


forward and then backward. As a result, null velocity (9) does
not imply null angular velocity (10). For the level of precision
we pursue in the present paper, distinguishing between (9)
and (10) is pivotal since it allows us to: _i_ ) properly label
motion profiles before training (see Section V-B, where we
have different thresholds on position and on angular velocity);
and: _ii_ ) improve detection accuracy since only one motion
pattern can be identified as valid.
(11) and (13) generally hold for robots moving indoors or
cars on roads. Note that (13) is expressed in the body frame,
and thus generally holds for a car moving on a road even if
the road is not level. As such (13) is more refined that just
assuming the car is moving in a 2D horizontal plane. And
it is actually quite a challenge for an IMU-based detector to
identify when (11) and (13) are _not_ valid.


_C. Expected Impacts on Robot Localization_

The motion profiles fall into two categories in terms of the
information they bring:

1) **Zero velocity constraints** : the profiles (9)-(10), when
correctly detected may allow to correct the IMU biases,
the pitch and the roll.
2) **Vehicle motion constraints** : the profiles (11) and (13)
are useful constraints for the estimates accuracy over the
long term. In particular, Section V-E will experimentally
demonstrate the benefits of accounting for (11) and (13).


IV. P ROPOSED RINS-W A LGORITHM


This section describes our system for recovering trajectory
and sensor bias estimates from an IMU. Figure 3 illustrates
the approach which consists of two main blocks summarized
as follow:


_•_ the detector estimates the binary vector **z** _n_ from raw
IMU signals, and consists of recurrent neural networks;


_•_
the filter integrates the IMU measurements in its dynamic model and exploits the detected motion profiles
as pseudo-measurements to refine its estimates, as customary in inertial navigation, see e.g. [19].
The detector does not use the filter’s output and is based
only on IMU measurements. Thus the detector operates
autonomously and is trained independently, see Section V-B.



**v** _n_ [B] [=] **[ R]** _[T]_ _n_ **[v]** _n_ [W] [=]



_v_ _n_ [FOR]

 _v_ _n_ [LAT]

 _v_ _n_ [UP]




_•_ **Zero vertical velocity** : the vertical velocity is considered roughly null


_z_ _n_ [UP] [= 1] _[ ⇒]_ _[v]_ _n_ [UP] _[≃]_ [0] _[.]_ (13)


The two latter are common assumptions for wheeled robots
or cars moving forward on human made roads.


_B. Discussion on the Choice of Profiles_

The motion profiles were carefully chosen in order to
match the specificity of wheeled robots equipped with shock
absorbers. Indeed, as illustrated on Figure 2, when the wheels
of a car actually stop, the car undergoes a rotational motion


1 Without loss of generality, we assume that the body frame is aligned
with the IMU frame.


**z** ˆ _n_ +1


**a** _n_ [IMU]











**x** ˆ _n_ +1



|Propagation Update<br>Invariant Extended Kalman Filter|Col2|
|---|---|
|||
|||


Fig. 5. Structure of the IEKF. The filter leverages motion profile information **z** ˆ _n_ both for the propagation and the update of the state ˆ **x** _n_ +1 .


recently given raise to a commercial product, see [10,12], and
various successes in robotics [13]–[16]. We thus opt for an
IEKF to perform the fusion between the IMU measurements
and the detected specific motion profiles. The IEKF outputs
the state ˆ **x** _n_ that consists of pose and velocity of the IMU, the
IMU biases, along with their covariance. We now describe
the filter more in detail, whose architecture is displayed on
Figure 5.
_1) IMU state:_ we define the IMU state as


**x** _n_ = ( **R** _n_ _,_ **v** _n_ [W] _[,]_ **[ p]** _n_ [W] _[,]_ **[ b]** _n_ _**[ω]**_ _[,]_ **[ b]** **[a]** _n_ [)] _[,]_ (16)


which contains robot pose, velocity, and the IMU biases. The
state evolution is given by the dynamics (1)-(7), see Section
II. As (9)-(13) are all measurements expressed in the robot’s
frame, they lend themselves to the Right IEKF methodology,
see [9]. Applying it, we define the linearized state error as





Fig. 3. Structure of the proposed RINS-W system for inertial navigation.
The detector identifies specific motion profiles (8) from raw IMU signals
only.





**a** _n_ [IMU]





Fig. 4. Structure of the detector, which consists for each motion pattern
of a recurrent neural network (LSTM) followed by a threshold to obtain an
output vector ˆ **z** _n_ that consists of binary components. The hidden state **h** _n_
of the neural network allows the full structure to be recursive.


Note that our approach is different from more tightly coupled
approaches such as [30]. We now describe each block.


_A. The Specific Motion Profile Detector_


The detector determines at each instant _n_ which ones of the
specific motion profiles (8) are valid, see Figure 4. The base
core of the detector is a recurrent neural network, namely a
Long-Short Term Memory (LSTM) [8]. The LSTMs take as
input the IMU measurements and compute:


**u** ˆ _n_ +1 _,_ **h** _n_ +1 = LSTM � _{_ _**ω**_ _i_ [IMU] _,_ **a** _i_ [IMU] _}_ _[n]_ _i_ =0 � (14)

= LSTM ( _**ω**_ _n_ [IMU] _[,]_ **[ a]** _n_ [IMU] _[,]_ **[ h]** _[n]_ [)] _[,]_ (15)


where ˆ **u** _n_ +1 _∈_ R [4] contains probability scores for each motion
profiles and **h** _n_ is the hidden state of the neural network.
Probability scores are then converted to a binary vector ˆ **z** _n_ =
Threshold (ˆ **u** _n_ +1 ) with a threshold for each motion profile.
The thresholds must be set with care, and the procedure
will be described in Section V-A. Indeed, false alarms lead
to degraded performance, since a zero velocity assumption
is incompatible with an actual motion. On the other hand, a
missed profile is not harmful since it results in standard IMU
based dead reckoning using (1)-(3).


_B. The Invariant Extended Kalman Filter_

The extended Kalman filter is the most widespread technique in the inertial navigation industry, since it was first
successfully used in the Apollo program half a century
ago. However, recent work advocates the use of a modified
version, the Invariant Extended Kalman Filter (IEKF) [9] that
has proved to bring drastic improvement over EKF, and has



where uncertainty is based on the use of the Lie exponential
as advocated in [31] in a wheel odometry context, and
mapped to the state as


_**χ**_ _n_ = exp _SE_ 2 (3) ( _**ξ**_ _n_ ) ˆ _**χ**_ _n_ _,_ (18)

**b** _n_ = **b** [ˆ] _n_ + **e** **[b]** _n_ _[,]_ (19)


where _**[χ]**_ _n_ _∈_ _SE_ 2 (3) is a matrix that lives is the Lie group
_SE_ 2 (3) and represents the vehicle state **R** _n_, **v** _n_ [W] [,] **[ p]** _n_ [W] [(see]
Appendix A for the definition of _SE_ 2 (3) and its exponential
map), **P** _n_ _∈_ R [15] _[×]_ [15] is the error state covariance matrix,

ˆ
**b** _n_ = ( **b** _**[ω]**_ _n_ _[,]_ **[ b]** **[a]** _n_ [)] _[ ∈]_ [R] [6] [,][ ˆ] **[b]** _[n]_ [=] **b** _**[ω]**_ _n_ _[,]_ [ ˆ] **[b]** **[a]** _n_ _∈_ R [6], and ( [ˆ] _·_ )
� �
denote estimated variables.
_2) Propagation step:_ if no specific motion is detected, i.e.
_z_ ˆ _n_ [VEL] +1 [= 0][,][ ˆ] _[z]_ _n_ [ANG] +1 [= 0][, we apply (1)-(7) to propagate the]
state and obtain ˆ **x** _n_ +1 and associated covariance through the
Riccati equation


**P** _n_ +1 = **F** _n_ **P** _n_ **F** _[T]_ _n_ [+] **[ G]** _[n]_ **[Q]** _[n]_ **[G]** _[T]_ _n_ _[,]_ (20)


where the Jacobians **F** _n_, **G** _n_ are given in Appendix B,
and where **Q** _n_ denotes the covariance matrix of the noise
**w** _n_ = � **w** _n_ _**[ω]**_ _[,]_ **[ w]** _n_ **[a]** _[,]_ **[ w]** _n_ **[b]** _**[ω]**_ _[,]_ **[ w]** _n_ **[b]** **[a]** � _∼N_ ( **0** _,_ **Q** _n_ ). By contrast, if a
specific motion profile is detected, we modify model (1)-(7)
as follows:

_z_ ˆ _n_ [VEL] +1 [= 1] _[ ⇒]_ **v** _n_ [W] +1 [=] **[ v]** _n_ [W] _,_ (21)
� **p** _n_ [W] +1 [=] **[ p]** _n_ [W]



_**ξ**_ _n_
**e** _n_ = � **e** **[b]** _n_



_∼N_ ( **0** _,_ **P** _n_ ) _,_ (17)
�


_z_ ˆ _n_ [ANG] +1 [= 1] _[ ⇒]_ **[R]** _[n]_ [+1] [=] **[ R]** _[n]_ _[,]_ (22)


and the estimated state ˆ **x** _n_ +1 and covariance **P** _n_ +1 are
modified accordingly.
_3) Update:_ each motion profile yields one of the following
pseudo-measurements:



**R** _[T]_ _n_ +1 **[v]** _n_ [W] +1
**y** _n_ [VEL] +1 [=] � **b** **[a]** _n_ +1 _[−]_ **[R]** _[T]_ _n_ +1 **[g]**



**0**

=
� � **a** _n_ [IMU]



_,_ (23)
�



Fig. 6. The considered dataset [11] contains data logs of a Xsens MTi300 [2] (right) recorded at 100 Hz along with the ground truth pose.


_A. Implementation Details_

We provide in this section the detector and filter setting of
the RINS-W system. The detector disposes of four LSTMs,
one for each motion profile. Each LSTM consists of 2 layers
of 250 hidden units and its hidden state is mapped to a
score probability by a 2 layers multi-perceptron network with
a ReLU activation function and is followed by a sigmoid
function [8] that outputs a scalar value in the range [0 _,_ 1].
We implement the detector on PyTorch [3] and set the threshold
values to 0.95 for ( _z_ _n_ [VEL] [,] _[ z]_ _n_ [ANG] ), and 0.5 for ( _z_ _n_ [LAT] [,] _[ z]_ _n_ [UP] [). The]
filter operates at the 100 Hz IMU rate ( _dt_ = 10 _[−]_ [2] s) and its
noise covariance matrices are parameterized as


**Q** _n_ = diag � _σ_ _**ω**_ [2] **[I]** _[, σ]_ **a** [2] **[I]** _[, σ]_ **b** [2] _**ω**_ **[I]** _[, σ]_ **b** [2] **a** **[I]** � _,_ (31)

**N** _n_ = diag � _σ_ VEL [2] _,_ **v** **[I]** _[, σ]_ VEL [2] _,_ **a** **[I]** _[, σ]_ ANG [2] **[I]** _[, σ]_ LAT [2] _[, σ]_ UP [2] � _,_ (32)


where we set _σ_ _**ω**_ = 0 _._ 01 rad _/_ s, _σ_ **a** = 0 _._ 2 m _/_ s [2], _σ_ **b** _**ω**_ =
0 _._ 001 rad _/_ s, _σ_ **b** **a** = 0 _._ 02 m _/_ s [2] for the noise propagation covariance matrix **Q** _n_, and _σ_ VEL _,_ **v** = 1 m _/_ s, _σ_ VEL _,_ **a** = 0 _._ 4 m _/_ s [2],
_σ_ ANG = 0 _._ 04 rad _/_ s, _σ_ LAT = 3 m _/_ s, and _σ_ UP = 3 m _/_ s for the
noise measurement covariance matrix **N** _n_ .


_B. Detector Training_


The detector is trained with the sequences urban06 to
urban14, that represents 100 km of training data (sequences
urban00 to urban05 does not have acceleration data). For
each sequence, we compute ground truth position velocity
**v** _n_ [W] [and angular velocity] _**[ ω]**_ _[n]_ [after differentiating the ground]
pose and applying smoothing. We then compute the groundtruth **z** _n_ by applying a small threshold on the ground truth
velocities, e.g. we consider _z_ _n_ [VEL] = 1 if _∥_ **v** _n_ [W] _[∥]_ _[<]_ [ 0] _[.]_ [01 m] _[/]_ [s][.]
We set similarly the other motion profiles and use a threshold
of 0 _._ 005 rad _/_ s for the angular velocity, and a threshold of
0 _._ 1 m _/_ s for the lateral and upward velocities.
The detector is trained during 500 epochs with the ADAM
optimizer [32], whose learning rate is initializing at 10 _[−]_ [3]

and managed by a learning rate scheduler. Regularization is
enforced with dropout layer, where _p_ = 0 _._ 4 is the probability
of any element to be zero. We use the binary cross entropy
loss since we have four binary classification problems. For
each epoch, we organize data as a batch of 2 min sequences,
where we randomly set the start instant of each sequence, and
constraints each starting sequence to be a stop of at minimum
1 s. Training the full detector takes less than one day with a
GTX 1080 GPU.


3 [https://pytorch.org/](https://pytorch.org/)



**y** _n_ [ANG] +1 [=] **[ b]** _**[ω]**_ _n_ +1 [=] _**[ ω]**_ _n_ [IMU] _[,]_ (24)

**y** _n_ [LAT] +1 [=] _[ v]_ _n_ [LAT] +1 [= 0] _[,]_ (25)

**y** _n_ [UP] +1 [=] _[ v]_ _n_ [UP] +1 [= 0] _[.]_ (26)


A vector **y** _n_ +1 is computed by stacking the pseudomeasurements of the detected motion profiles. Note that, if
_z_ ˆ _n_ [VEL] +1 [= 1][ we do not consider (25)-(26) since (23) implies]
(25)-(26). If no specific motion is detected, the update step
is skipped, otherwise we follow the IEKF methodology [9]
and compute


**K** = **P** _n_ +1 **H** _[T]_ _n_ +1 _[/]_ � **H** _n_ +1 **P** _n_ +1 **H** _[T]_ _n_ +1 [+] **[ N]** _[n]_ [+1] � _,_ (27)



+
**e** [+] = **K** ( **y** _n_ +1 _−_ **y** ˆ _n_ +1 ) = _**ξ**_

**e** **[b]** [+]

�



_,_ (28)
�



_**χ**_ ˆ [+] _n_ +1 [= exp] � _**ξ**_ [+] [�] _**χ**_ [ˆ] _n_ +1 _,_ **b** [+] _n_ +1 [=] **[ b]** _[n]_ [+1] [ +] **[ e]** **[b]** [+] _[,]_ (29)

**P** [+] _n_ +1 [= (] **[I]** _[ −]_ **[KH]** _[n]_ [+1] [)] **[ P]** _[n]_ [+1] _[,]_ (30)


summarized as Kalman gain (27), state innovation (28), state
update (29) and covariance update (30), where **H** _n_ +1 is
the measurement Jacobian matrix given in Appendix B and
**N** _n_ +1 the noise measurement covariance.

_4) Initialization:_ launching the system when the platform
is moving without estimating biases and orientation can
induce drift at the beginning which then is impossible to
compensate. As the filter is able to correctly self-initialize
the biases, pitch, roll and its covariance matrix when the
trajectory is first stationary, we enforce each sequence in
Section V to start by a minimum of 1 s stop. Admittedly
restrictive, the required stop is of extremely short duration,
especially as compared to standard calibration techniques [4].


V. R ESULTS ON C AR D ATASET


The following results are obtained on the _complex urban_
_LiDAR dataset_ [11], that consists of data recorded on a
consumer car moving in complex urban environments, e.g.
metropolitan areas, large building complexes and underground parking lots, see Figure 6. Our goal is to show that
using an IMU of moderate cost, one manages to obtain
surprisingly accurate dead reckoning by using state-of-the-art
machine learning techniques to detect relevant assumptions
that can be fed into a state-of-the-art Kalman filter. The
detector is trained on a series of sequences and tested on
another sequences, but all sequences involve the same car and
inertial sensors. Generalization to different robot and IMU is

not considered herein and is left for future work.


2 [https://www.xsens.com/](https://www.xsens.com/)


test seq. wheels odo. odo. + FoG **RINS-W**


15: 16 min 19 / 5 / 36 **7** / **2** / **7** **7** / 5 / 12

16: 73 min 140 / 127 / 1166 34 / 20 / 164 **27** / **11** / **20**

17: 19 min 96 / 64 / 427 58 / 51 / 166 **13** / **11** / **13**


15-17:
114 / 98 / 677 34 / 30 / 152 **22** / **10** / **18**
108 min


Table 1. Results obtained by the 3 methods on urban test sequences 15,
16, 17 in terms of: _m_ -ATE /aligned _m_ -ATE / final distance error to ground
truth, in m. Last line is the concatenation of the three sequences. Direct IMU
integration always diverges. The proposed RINS-W outperforms differential
wheel speeds based odometry and outperforms on average (see last line) the
expensive combination of odometry + FoG. Indeed, RINS-W uses an IMU
with gyro stability of 10 deg _/_ h, whereas FoG stability is 0 _._ 05 deg _/_ h.


_C. Evaluation Metrics_


To assess performances we consider three error metrics:
_1) Mean Absolute Trajectory Error (m-ATE):_ which averages the planar translation error of estimated poses with
respect to a ground truth trajectory and is less sensitive to
single poor estimates than root mean square error;
_2) Mean Absolute Aligned Trajectory Error (aligned m-_
_ATE):_ that first aligns the estimated trajectory with the
ground truth and then computes the mean absolute trajectory
error. This metric evaluates the consistency of the trajectory
estimates;
_3) Final distance error:_ which is the final distance between the un-aligned estimates and the ground truth.


_D. Trajectory Results_


After training the detector on sequences urban06 to
urban14, we evaluate the approach on test sequences
urban15 to urban17, that represent 40 km of evaluation
data. We compare 4 methods:


_•_ **IMU** : the direct integration of the IMU measurements
based on (1)-(7), that is, pure inertial navigation;

_•_ **Odometry** : the integration of a differential wheel encoder which computes linear and angular velocities;

_•_ **RINS-W (ours)** : the proposed approach, that uses only
the IMU signals and involves no other sensor.

_•_ **Odometry + FoG** : the integration of a differential
wheel encoder which computes only linear velocity.
The angular velocity is obtained after integrating the
increments of an highly accurate and costly KVH DSP1760 [4] Fiber optics Gyro (FoG). The FoG gyro bias
stability (0 _._ 05 deg _/_ h) is 200 times smaller than the gyro
stability of the IMU used by RINS-W;

We delay each sequence such that the trajectory starts
with a 1 s stop to initialize the orientation and the biases,
see Section IV-B. Bias initialization is _also_ performed for
IMU pure integration and the FoG. Optimized parameters for
wheel speeds sensors calibration are provided by the dataset.
Experimental results in terms of error with respect to
ground truth are displayed in Table 1, and illustrated on
Figures 1, 7, and 8. Results demonstrate that:


4 [https://www.kvh.com](https://www.kvh.com)



_−_ 0 _._ 2


_−_ 0 _._ 6 _−_ 0 _._ 4 _−_ 0 _._ 2 0


_x_ (km)


Fig. 7. Ground truth and trajectory estimates for the test sequence urban15
of the car dataset [11]. RINS-W obtains results comparable with FoG-based
odometry.


_•_
directly integrating IMU leads to divergence at the first
turn, even after a careful calibration procedure;

_•_ wheel-based differential odometry accurately estimates
the linear velocity but has troubles estimating the yaw
during sharp bends, even if the dataset has been obtained
in an urban environment and the odometry parameters
are calibrated. This drawback may be remedied at the
expense of using an additional high-cost gyroscope;

_•_ the proposed scheme completely outperforms wheel encoders, albeit in urban environment. More surprisingly,
our approach competes with the combination of wheel
speed sensors + (200 hundred times more accurate)
Fyber optics Gyro, and even outperforms it on average.

Furthermore, although comparisons were performed in 2D
environments our method yields the full 3D pose of the robot,
and as such is compatible with non planar environments.


_E. Discussion_


The performances of RINS-W can be explained by: _i_ )
a false-alarm free detector; _ii_ ) the fact incorporating side
information into IMU integration obviously yields better
results; and _iii_ ) the use of a recent IEKF that has been proved
to be well suited for localization tasks.

We also emphasize the importance of (11) and (13) in
the procedure, i.e. applying (25)-(26). For illustration, we
consider sequence urban07 of [11], where the vehicle
moves during 7 minutes without stop so that ZUPT may not
be used. We implement the detector trained on the first 6
sequences, and compare the proposed RINS-W to a RINSW which does _not_ use updates (25)-(26) when detected,
see Figure 9. Clearly, the reduced RINS-W diverges at the
first turn whereas the full RINS-W is accurate along all the



0 _._ 6


0 _._ 4


0 _._ 2


0






2


1 _._ 5


1


0 _._ 5







0 _._ 35





0


0 0 _._ 5 1 1 _._ 5 2


_x_ (km)


Fig. 8. Ground truth and trajectory estimates for the test sequence urban17
of the car dataset [11]. RINS-W clearly outperforms the odometry and even
the odometry + FoG solution. We note that RINS-W accurately follows the
interchange road located at ( _x_ = 2 _, y_ = 1).


trajectory and obtains a final distance w.r.t. ground truth of
5 m. In contrast, odometry + FoG achieves 16 m.


_F. Detector Evaluation_


In Section V-E we mentioned three possible reasons explaining the performances of RINS-W, but could not assess
what is owed to each. To assess the detector’s performance,
and to demonstrate the interest of our powerful deep neural
network based approach (see Section IV-A) we focus on the
zero velocity detection (9), and compare the detector with the
Acceleration-Moving Variance Detector (AMVD) [20] on the
test sequences urban15-17, which represent 64 _._ 10 [4] measurements. The AMVD computes the accelerometer variance
over a sample window _W_ = 10 [2] and assumes the vehicle is
stationary if the variance falls below a threshold _γ_ = 10 _[−]_ [3] .
To make sure AMVD performs at its best, the parameters _W_
and _γ_ are optimized by grid search _on the test sequences_ .
Results are shown in Table 2 and demonstrate the detector is

more accurate than this “ideal” AMVD.


VI. C ONCLUSION


This paper proposes a novel approach for robust inertial
navigation for wheeled robots (RINS-W). Even if an autonomous vehicle is equipped with wheel encoders, LiDAR,
and vision besides the IMU, the algorithm may be run in
parallel for safety, in case of sensor failure, or more simply
for validation such as slip angle monitoring [1]. Our approach
exploits deep neural networks to identify specific patterns in
wheeled vehicle motions and incorporates this knowledge in
IMU integration for localization. The entire algorithm is fed
with IMU signals only, and requires no other sensor. The



0 _._ 3


0 _._ 25


0 _._ 2


0 _._ 15


0 _._ 1


0 _._ 05


0

|start|Col2|Col3|
|---|---|---|
|start|true trajectory<br>usual estimation<br>**proposed estimation**|true trajectory<br>usual estimation<br>**proposed estimation**|
|start|true trajectory<br>usual estimation<br>**proposed estimation**||
|start|||



_−_ 0 _._ 05 0 0 _._ 05 0 _._ 1 0 _._ 15 0 _._ 2 0 _._ 25


_x_ (km)


Fig. 9. Comparison on the sequence urban07 between proposed RINSW, and a similar algorithm that does not use zero lateral and vertical
velocities assumptions brought by the detector, see (25)-(26). The final
distance between ground truth and RINS-W estimates is as small as 5 m,
whereas ignoring (25)-(26) yields divergence.


_z_ _n_ [vel] detection ideal AMVD our detector


true positive / false pos. 47 _._ 10 [4] / 4 _._ 10 [3] **48** _._ **10** **[4]** / **7** _._ **10** **[2]**

true negative / false neg. **16** _._ **10** **[4]** / 1 _._ 10 [4] **16** _._ **10** **[4]** / **9** _._ **10** **[3]**

precision / recall 0 _._ 974 / **0** _._ **940** **0** _._ **996** / **0** _._ **940**


Table 2. Results on zero velocity (9) detection obtained by an _ideal_
AMVD [20] and the proposed detector on test sequences urban15-17. The
proposed detector systematically obtains better results and precision. This is
remarkable because the detector is not trained on those sequences, whereas
AMVD parameters were optimized on the considered test sequences.


method leads to surprisingly accurate results, and opens new
perspectives for combination of data-driven methods with
well established methods for autonomous systems. Moreover,
the pseudo measurements output by the detector are not
reserved for dead reckoning and may prove useful in any
fusion scheme. In future work, we would like to address
the learning of the Kalman covariance matrices, and also the
issue of generalization from one vehicle to another.


A PPENDIX A


The Lie group _SE_ 2 (3) is an extension of the Lie group
_SE_ (3) and is described as follow, see [9] for more details.
A 5 _×_ 5 matrix _**χ**_ _n_ _∈_ _SE_ 2 (3) is defined as



The uncertainties _**ξ**_ _n_ _∈_ R [9] are mapped to the Lie algebra
se 2 (3) through the transformation _**ξ**_ _n_ _�→_ _**ξ**_ _n_ _[∧]_ [defined as]


_**ξ**_ _n_ = � _**ξ**_ _n_ **[R]** _[,]_ _**[ ξ]**_ _n_ **[v]** _[,]_ _**[ ξ]**_ _n_ **[p]** � _,_ (34)



_**χ**_ _n_ = **R0** _n_ **v** _n_ [W] **I** **p** _n_ [W]
�



_∈_ _SE_ 2 (3) _._ (33)
�


_**ξ**_ _n_ **[R]** �
_**ξ**_ _n_ _[∧]_ [=]
��



_×_ _**ξ**_ _n_ **[v]** _**ξ**_ _n_ **[p]**
**0**



_∈_ se 2 (3) _,_ (35)
�



where ( _·_ ) _×_ transforms a vector to a skew-symmetric matrix,
_**ξ**_ _n_ **[R]** _[∈]_ [R] [3] [,] _**[ ξ]**_ _n_ **[v]** _[∈]_ [R] [3] [ and] _**[ ξ]**_ _n_ **[p]** _[∈]_ [R] [3] [. The closed-form expression]
for the exponential map is given as


exp _SE_ 2 (3) ( _**ξ**_ _n_ ) = **I** + _**ξ**_ _n_ _[∧]_ [+] _[ a]_ [(] _**[ξ]**_ _n_ _[∧]_ [)] [2] [ +] _[ b]_ [(] _**[ξ]**_ _n_ _[∧]_ [)] [3] _[,]_ (36)


1 _−_ cos ( _∥_ _**ξ**_ _n_ **[R]** _[∥]_ [)] _∥_ _**ξ**_ _n_ **[R]** _[∥−]_ [sin] [(] _[∥]_ _**[ξ]**_ _n_ **[R]** _[∥]_ [)]
where _a_ = and _b_ = .
_∥_ _**ξ**_ _n_ **[R]** _[∥]_ _∥_ _**ξ**_ _n_ **[R]** _[∥]_ [3]


A PPENDIX B


Following the Right IEKF of [9], the Jacobians required
for the computation of the filter propagation (20) are given

as






_dt,_ (37)





[7] J.-E. Deschaud, “IMLS-SLAM: Scan-to-Model Matching Based on 3D
Data,” in _IEEE ICRA_, 2018.

[8] I. Goodfellow, Y. Bengio, and A. Courville, _Deep Learning_ . The MIT
press, 2016.

[9] A. Barrau and S. Bonnabel, “The Invariant Extended Kalman Filter as
a Stable Observer,” _IEEE Trans. on Automatic Control_, vol. 62, no. 4,
pp. 1797–1812, 2017.

[10] ——, “Invariant Kalman Filtering,” _Annual Review of Control,_
_Robotics, and Autonomous Systems_, vol. 1, no. 1, pp. 237–257, 2018.

[[Online]. Available: https://doi.org/10.1146/annurev-control-060117-](https://doi.org/10.1146/annurev-control-060117-105010)
[105010](https://doi.org/10.1146/annurev-control-060117-105010)

[11] J. Jeong, Y. Cho, Y.-S. Shin, H. Roh, and A. Kim, “Complex Urban
LiDAR Data Set,” in _IEEE ICRA_, 2018.

[12] A. Barrau and S. Bonnabel, “Aligment Method for an Inertial Unit,”
patent 15/037,653, 2016.

[13] K. Wu, T. Zhang, D. Su, S. Huang, and G. Dissanayake, “An invariantEKF VINS algorithm for improving consistency,” in _IEEE IROS_, 2017,
pp. 1578–1585.

[14] S. Heo and C. G. Park, “Consistent EKF-Based Visual-Inertial Odometry on Matrix Lie Group,” _IEEE Sensors Journal_, vol. 18, no. 9, pp.
3780–3788, 2018.

[15] M. Brossard, S. Bonnabel, and A. Barrau, “Unscented Kalman Filter
on Lie Groups for Visual Inertial Odometry,” in _IEEE IROS_, 2018.

[16] R. Hartley, M. G. Jadidi, J. W. Grizzle, and R. M. Eustice, “ContactAided Invariant Extended Kalman Filtering for Legged Robot State
Estimation,” in _Robotics Science and Systems_, 2018.

[17] K. Wu, C. Guo, G. Georgiou, and S. Roumeliotis, “VINS on Wheels,”
in _IEEE ICRA_, 2017, pp. 5155–5162.

[18] F. Zheng, H. Tang, and Y.-H. Liu, “Odometry Vision Based Ground
Vehicle Motion Estimation With SE(2)-Constrained SE(3) Poses,”
_IEEE Trans. on Cybernetics_, pp. 1–12, 2018.

[19] A. Ramanandan, A. Chen, and J. Farrell, “Inertial Navigation Aiding
by Stationary Upyears,” _IEEE Trans. on Intelligent Transportation_
_Systems_, vol. 13, no. 1, pp. 235–248, 2012.

[20] I. Skog, P. Handel, J. O. Nilsson, and J. Rantakokko, “Zero-Velocity
Detection—An Algorithm Evaluation,” _IEEE Trans. on Biomedical_
_Engineering_, vol. 57, no. 11, pp. 2657–2666, 2010.

[21] G. Dissanayake, S. Sukkarieh, E. Nebot, and H. Durrant-Whyte, “The
Aiding of a Low-cost Strapdown Inertial Measurement Unit Using
Vehicle Model Constraints for Land Vehicle Applications,” _IEEE T-_
_ROA_, vol. 17, no. 5, pp. 731–747, 2001.

[22] A. Solin, S. Cortes, E. Rahtu, and J. Kannala, “Inertial Odometry on
Handheld Smartphones,” in _FUSION_, 2018.

[23] D. Atchuthan, A. Santamaria-Navarro, N. Mansard, O. Stasse, and
J. Sol`a, “Odometry Based on Auto-Calibrating Inertial Measurement
Unit Attached to the Feet,” in _ECC_, 2018.

[24] H. Yan, Q. Shan, and Y. Furukawa, “RIDI: Robust IMU Double
Integration,” in _ECCV_, 2018.

[25] B. Wagstaff and J. Kelly, “LSTM-Based Zero-Velocity Detection for
Robust Inertial Navigation,” in _IPIN_, 2018.

[26] S. Cortes, A. Solin, and J. Kannala, “Deep Learning Based Speed
Estimation for Constraining Strapdown Inertial Navigation on Smartphones,” _IEEE MLSP workshop_, 2018.

[27] C. Chen, X. Lu, A. Markham, and N. Trigoni, “IONet: Learning to
Cure the Curse of Drift in Inertial Odometry,” in _AAAI_, 2018.

[28] C. Chen, P. Zhao, C. X. Lu, W. Wang, A. Markham, and N. Trigoni,
“OxIOD: The Dataset for Deep Inertial Odometry,” 2018.

[29] M. Brossard and S. Bonnabel, “Learning Wheel Odometry and IMU
Errors for Localization,” in _IEEE ICRA_, 2019.

[30] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF:
Learning Discriminative Deterministic State Estimators,” in _NIPS_,
2016.

[31] T. Barfoot and P. Furgale, “Associating Uncertainty With ThreeDimensional Poses for Use in Estimation Problems,” _IEEE T-RO_,
vol. 30, no. 3, pp. 679–693, 2014.

[32] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” in _ICLR_, 2014.



**F** _n_ = **I** +



**0** **0** **0** _−_ **R** _n_ **0**

( **g** ) _×_ **0** **0** _−_ ( **v** _n_ [W] [)] _[×]_ **[R]** _[n]_ _−_ **R** _n_

**0** **I** **0** _−_ ( **p** _n_ [W] [)] _[×]_ **[R]** _[n]_ **0**
**0** **0** **0** **0** **0**

**0** **0** **0** **0** **0**









**G** _n_ =









**R** _n_ **0** **0** **0**
( **v** _n_ [W] [)] _[×]_ **[R]** _[n]_ **R** _n_ **0** **0**
( **p** _n_ [W] [)] _[×]_ **[R]** _[n]_ **0** **0** **0**
**0** **0** **I** **0**

**0** **0** **0** **I**



_dt,_ (38)




when ˆ _z_ _n_ [VEL] = 0 and ˆ _z_ _n_ [ANG] = 0. Otherwise, we set the
appropriate rows to zero in **F** _n_ and **G** _n_, i.e.:

_•_ if ˆ _z_ _n_ [VEL] = 1 we set the 4 to 9 rows of the right part of
**F** _n_ in (37) and of **G** _n_ to zero.

_•_ if ˆ _z_ _n_ [ANG] = 1 we set the 3 first rows of the right part of
**F** _n_ in (37) and of **G** _n_ to zero.

Once again following [9], the measurement Jacobians used
in the filter update (27)-(30) are given as



**H** _n_ [VEL] = � **R** _[T]_ _n_ **0** [(] **[g]** [)] _[×]_ **R0** _[T]_ _n_ **00** **00** _−_ **0I**



_,_ (39)
�



**H** _n_ [ANG] = � **0** **0** **0** _−_ **I** **0** [�] _,_ (40)


and we obtains **H** _n_ [LAT] and **H** _n_ [UP] [as respectively the second and]
third row of **H** _n_ [VEL] [.]


R EFERENCES


[1] OxTS. (2018) Why it is Necessary to Integrate an Inertial
Measurement Unit with Imaging Systems on an Autonomous Vehicle.

[Online]. Available: [https://www.oxts.com/technical-notes/why-use-](https://www.oxts.com/technical-notes/why-use-ins-with-autonomous-vehicle/)
[ins-with-autonomous-vehicle/](https://www.oxts.com/technical-notes/why-use-ins-with-autonomous-vehicle/)

[2] J. Collin, P. Davidson, M. Kirkko-Jaakkola, and H. Lepp¨akoski, “Inertial Sensors and Their Applications,” in _Handbook of Signal Processing_
_Systems_ . Springer, 2018, pp. 69–96.

[3] M. Kok, J. Hol, and T. Sch¨on, “Using Inertial Sensors for Position
and Orientation Estimation,” _Foundations and Trends⃝_ R _in Signal_
_Processing_, vol. 11, no. 1-2, pp. 1–153, 2017.

[4] Safran. (2018) Inertial navigation systems. [Online]. Available:
[https://www.safran-group.com/fr/video/13612](https://www.safran-group.com/fr/video/13612)

[5] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,
I. Reid, and J. Leonard, “Past, Present, and Future of Simultaneous
Localization and Mapping: Toward the Robust-Perception Age,” _IEEE_
_T-RO_, pp. 1309–1332, 2016.

[6] K. Sun, K. Mohta, B. Pfrommer, M. Watterson, S. Liu, Y. Mulgaonkar,
C. J. Taylor, and V. Kumar, “Robust Stereo Visual Inertial Odometry
for Fast Autonomous Flight,” _IEEE RA-L_, vol. 3, no. 2, pp. 965–972,
2018.


