## A Comprehensive Introduction of Visual-Inertial Navigation

Yangyang NING


March 2022

#### **Abstract**


In this article, a tutorial introduction to visual-inertial navigation(VIN) is presented. Visual and
inertial perception are two complementary sensing modalities. Cameras and inertial measurement
units (IMU) are the corresponding sensors for these two modalities. The low cost and light weight
of camera-IMU sensor combinations make them ubiquitous in robotic navigation. Visual-inertial
Navigation is a state estimation problem, that estimates the ego-motion and local environment of
the sensor platform. This paper presents visual-inertial navigation in the classical state estimation
framework, first illustrating the estimation problem in terms of state variables and system models,
including related quantities representations (Parameterizations), IMU dynamic and camera measurement models, and corresponding general probabilistic graphical models (Factor Graph). Secondly,
we investigate the existing model-based estimation methodologies, these involve filter-based and
optimization-based frameworks and related on-manifold operations. We also discuss the calibration
of some relevant parameters, also initialization of state of interest in optimization-based frameworks.
Then the evaluation and improvement of VIN in terms of accuracy, efficiency, and robustness are
discussed. Finally, we briefly mention the recent development of learning-based methods that may
become alternatives to traditional model-based methods.

#### **1 Introduction**


Environment and ego-motion perception is critically important for navigation. Motion perception
in biology offers a more intuitive perspective, human use various sensory modalities to feel selfmotion and surroundings, especially combining motion and balance from the inner ear, joint position,
and visual information from the eyes to obtain a virtual sense of movement, called kinesthesia[1].
Instead, motion perception of robots is provided through the use of navigation and positioning
techniques including dead reckoning methods (using inertial sensors, pedometer, wheel encoder,
magnetometer, and gyrocompass, etc.) and position fixing methods (using global navigation satellite
system (GNSS), ultra-wideband (UWB), acoustic ranging, lidar, and cameras, etc.)[2]. In this work,
we focus on two types of sensory modalities: visual and inertial. Visual-inertial navigation is a typical
state estimation problem, that estimates ego-motion and local map given measurement data from the
camera and IMU. Ego-motion can be quantitatively described as robot states normally involving
translation, rotation, and velocity, evenly higher time derivatives (acceleration, etc.) in certain
reference coordinates over time[3]. In terms of local map representation, VIN usually quantifies the
geometric information of local map in the form of 3D point clouds, meshes, voxels, or signed distance
functions (SDF)[4] with different capabilities and efficiencies.
For robot navigation, VIN provides both robot’s current state and local map, which are essential
for path planning, obstacle avoidance, and real-time control. As a popular navigation technique, VIN
greatly contributes to the general guidance, navigation, and control (GNC) systems shown in Figure
1. In particular, since VIN uses onboard sensors, such sensory modalities are critically essential in
the absence of external positioning systems like GPS, UWB, and visual motion capture systems. As
such, VIN is widely used for indoor localization, underwater exploration, city reconstruction, and
search-and-rescue.

Compared to some GPS-free navigation system that uses only cameras and lidars, visual-inertial
fusion improves accuracy, robustness, and efficiency of estimation. Visual-only systems like visual
simultaneous localization and mapping(V-SLAM) require static scenes, sufficient illumination, Lambertian textured surface (no transparent or reflective), rich texture, and scene overlap. However,
as a proprioceptive sensor, IMU is not limited by the above conditions by its internal mechanism
that reads local linear acceleration and angular velocity. Micro-electro-mechanical-system (MEMS)


1


Figure 1: VIN in general guidance, navigation, and control (GNC) framework


IMU, in particular, has become ubiquitous in many robotics applications such as micro aerial vehicles
(MAVs)[5][6], autonomous underwater vehicles (AUVs)[7], autonomous ground vehicles (AGVs)[8],
quadruped robot[9], spacecraft[10], and mobile devices[11] due to its low cost and lightweight. However, IMU uses the dead reckoning technique for estimation which leads to drift over time caused
by integrating the bias and noisy measurements. Two types of sensory modalities compensate for
each other, IMU won’t affect by visual extreme conditions including motion blur, low texture, and
illumination change, and provides high rate measurements, whereas cameras are more accurate at
slow motion with no drift in static scenes.

#### **2 States and Models in VIN**


As a typical state estimation problem, VIN is to estimate the robot motion and local map
given IMU kinematics model and camera measurement model. The robot’s current ego-motion
can be characterized as the pose at the current moment and its time derivatives (velocity and
acceleration). The metric information of a local map can be represented as 3D points, lines, or
surfaces distributed in 3D space, where semantic information acted as ”category signatures” tagged
on these metric representations. In this work, we focus on metric information and explore works
that use 3D points as local map landmarks. Poses and 3D landmarks are normally of interest to
VIN state estimation. In VIN, some supporting parameters are also needed for estimating poses and
3D landmarks, including some time-varying quantities like velocity, IMU bias, gravity direction in
the local frame, and some time-invariant quantities like camera intrinsic, and camera-IMU extrinsic,
etc. A typical state estimation model is shown in Equation 1a and 1b as dynamic and measurement
models respectively. In many VIN works[12][13][14], systems are modeled as hybrid systems[15] with
continuous-time dynamic and discrete-time measurements.


**Continuous-time Dynamic Model:** **x** ˙ = **f** ( **x** _,_ ˜ **u** _,_ **w** ) (1a)


**Discrete-time Measurement Model:** **z** ˜ _k_ = **h** ( **x** _k_ ) + **v** (1b)


In VIN, poses, landmarks, time-varying quantities(velocity, IMU bias, etc.), and sometimes timeinvariant quantities (online calibration) are contained in state vector **x**, the dynamic function **f** ( _·_ )
is represented by IMU kinematics with linear acceleration and angular velocity measurement as
control input ˜ **u** . The measurement model **h** ( _·_ ) is normally a camera measurement model with 2D
image features as output ˜ **z** _k_ . **w** and **v** are process noise (noises from IMU measurements) and
measurement noise (noises from camera measurements) respectively. In this section, we first define
states and reference frames in VIN problem and discuss the parametrization of quantities involved
in VIN. Lastly, we briefly derive IMU kinematics model and camera measurement model and relate
them as factors in the factor graph for estimation methods clarification in Section 3.


2


**2.1** **Reference Frames and States in VIN**


Geometric quantities such as poses and landmarks are relative quantities that depend on the
frame of reference (coordinate). It is critical to firmly identify different frames. These include a _fixed_
world frame _**F**_ _**W**_ (global frame, map frame, or inertial frame), a _moving_ body frame _**F**_ _**B**_ (robot
frame or vehicle frame), some sensors frames (rigidly installed on the robot) like camera frames _**F**_ _**C**_
and IMU frames _**F**_ _**I**_ . In many VIN cases, the IMU frame is coincident with the body frame as
shown in Figure 2, where _X_, _Y_, and _Z_ axis are in red, green, and blue respectively.


Figure 2: Fixed world frame and moving body frame with rigidly mounted sensor frames


Normally, the final result of VIN including a pose track and a local map is expressed in an arbitrary stationary world frame. The pose of a robot at each time-step is expressed in translation and
rotation ( _W_ _**t**_ _W B_ _,_ _**R**_ _W B_ ) concerning the transformation between body frame and world frame. In
world-centric framework, The 3D landmarks are also expressed in world frame. In many VIN practices, the world frame is set to the first camera frame’s corresponding IMU frame _**F**_ _**I**_ **0** . This allows
zero initial pose uncertainty, which reduces the level of uncertainty and increases the consistency of
the estimate[16][17].













Figure 3: Landmarks and Camera-IMU poses along discrete measurement trajectory


The visual measurements of a moving camera are shown in a sequence of image frames that are
expressed in a 2D image coordinate. This 2D image coordinate is parallel to the y-z plane of the
3D camera frame _**F**_ _**C**_, which is rigidly mounted on the robot, as does the IMU frame _**F**_ _**I**_ . Figure 3 shows sparse landmark features, and discrete visual-inertial measurements along trajectories,
whose sampling rate is indicated by the density of dashed lines. IMU and camera, as a multi

3


sensor system, exist temporal and spatial differences. The temporal difference is mainly reflected
in their different measurement frequency and latency, and the spatial difference is caused by the
fixed installation displacement of these two sensors (camera-IMU extrinsic as shown in Figure 4a).
The measurement sampling rate of the IMU (100-1000Hz) is much faster than that of the camera (10-80Hz). Due to the difference in sensors’ latency, there exists temporal misalignment (time
offset) between IMU and camera measurements. This temporal misalignment can be resolved by
temporal calibration[18][19][20][21] that estimates the time offset or hardware synchronization[22],
Figure 4b shows the periodic time-synchronized IMU and camera measurement timestamps with
possible keyframe selection. Camera-IMU extrinsic can be identified by offline and online spatial
calibration, which we discuss later in this paper. For now, we assume IMU and camera data are
time-synchronized and camera-IMU extrinsic is known.


**IMU**



**Camera**


**Keyframe**





(a) Camera-IMU Extrinsic



(b) Time-synchronized IMU and camera measurement rate with possible keyframes
selection



Figure 4: Spatial and Temporal difference between Camera and IMU


The states of interest in VIN normally consist of poses and landmarks. In structureless visualinertial estimation, landmarks are excluded from the state of interest. For filter-based estimation,
the estimated state usually contains only one pose at the current moment, while in an optimizationbased framework, a fixed or incremental number of poses will be involved in the state. In this work,
we adopt Carlone _et al._ [23] convention, which classifies _target_ variables and _support_ variables in the
state variables, where target variables are normally of interest variables and support variables are
essential for estimation of target variables. In VIN, target variables are the poses (translation and
rotation) of a robot. Support variables contain the velocity and bias of IMU. Landmarks can be
target or support variables depending on the focus of estimation. For example, in visual-inertial
odometry (VIO), the current state of the robot including pose and its time derivatives are the most
concerned, whereas, in visual-inertial simultaneous localization and mapping (VI-SLAM), accurate
estimation of both landmarks and poses throughout the trajectory is required.
Note that the IMU dynamic model and the camera model are based on IMU and camera measurement respectively, in which case the maximum state estimation rate of VIN is bounded by the
sensor measurement rate of the camera and IMU. A new state estimate can be computed whenever
a new IMU or camera measurement arrives. However, the frequency of IMU measurements is high,
which will result in a high computational burden and the prediction of estimation is prone to drift
before a new camera measurement arrives. In this case, the poses’ estimate of the robot is normally considered with the corresponding camera frames instead of all IMU frames, which means the
poses in states are usually poses of camera frames’ corresponding IMU frames (assume camera-imu
extrinsic is known).
For generality, we define active states in time _k_ in VIN as,


_X_ _k_ = _{I_ _k_ _, L_ _k_ _}_


where states in time _k_ consist of a set of IMU frames _I_ _k_ and a set of 3D landmarks _L_ _k_ . We assume
all camera frames have corresponding time-synchronized IMU frames. The selections and number
of frames are different depending on estimation frameworks, we here show some of the typical cases
of frames’ composition.


  - Filter-based framework:


Only contain one most recent camera frame, or the latest selected keyframe, or the last IMU
frame in time _k_ .
_I_ _k_ = _{_ **F** _k_ _}_, or _{_ **K** _k_ _}_, or _{_ **I** _k_ _}_


where **F** _k_ denotes the last camera frame corresponded IMU frame in time _k_, **K** _k_ denotes the
last camera keyframe corresponded IMU frame in time _k_, **I** _k_ denotes the last IMU frame in
time _k_ .


4


  - Fixed-Lag Smoother framework:


Normally contain a fixed sliding window of recent frames and consecutive keyframes in time
_k_ .
_I_ _k_ = _{_ **K** _k−m_ _, · · ·,_ **K** _k_ _,_ **F** _k−n_ _, · · ·,_ **F** _k_ _}_


where _{_ **K** _k−m_ _, · · ·,_ **K** _k_ _}_ denotes _m_ +1 recent consecutive keyframes corresponded IMU frames
in time _k_, _{_ **F** _k−n_ _, · · ·,_ **F** _k_ _}_ denotes _n_ + 1 recent camera frames corresponded IMU frames in
time _k_ . There is no overlap between keyframes and recent frames, a new camera frame will be
selected as a new keyframe if it passes the keyframe selection test, otherwise, it will be a new
recent frame.


  - Full batch optimization-based framework:


Contain the entire keyframes in time _k_ that incremented over time.


_I_ _k_ = _{_ **K** 1 _, · · ·,_ **K** _k_ _}_


  - Covisibility-based framework:


Contain frames that share sufficient common features with the most recent keyframe in time
_k_ (covisibility graph). This may involve past frames selected independently of time.


_I_ _k_ = � **K** _k_ _,_ **K** _[k]_ _c_ 1 _[,][ · · ·][,]_ **[ K]** _c_ _[k]_ _o_ �


where **K** _[k]_ _c_ 1 _[,][ · · ·][,]_ **[ K]** _c_ _[k]_ _o_ [denote a set of camera frames corresponded IMU frames in the covisibility]
graph of the last keyframe **K** _k_ .


Note that the _keyframes_ mentioned above are carefully selected from camera frames that exhibit
good feature tracking quality and significant parallax with previous keyframes. The purpose of
selecting keyframes from camera frames is to reduce the uncertainty of initialization and to improve
the efficiency of graph-based optimization by preserving the sparsity that only a subset of camera
frames is maintained.

Recent IMU frames **I**, camera frames’ corresponding IMU frames **F**, and camera keyframes’
corresponding IMU frames **K** contain pose information of the IMU frame and its direct support
variables. In VIN, these IMU frames can be represented in IMU state **x** _I_, where camera-IMU
extrinsic ( _C_ **t** _CI_ _,_ **q** _CI_ ) is assumed given and camera-IMU measurements are time-synchronized. For
visual-inertial state estimation with a monocular camera, an IMU state is generally defined as
followed,


⊺
**x** _I_ = � _W_ **[t]** [⊺] _W I_ _[,]_ **[ q]** _W I_ [⊺] _[,]_ _[ W]_ **[ v]** _W I_ [⊺] _[,]_ **[ b]** [⊺] _g_ _[,]_ **[ b]** _a_ [⊺] �


where


_W_ **[t]** _W I_ _[∈]_ [R] [3] [ : Position of IMU frame origin wrt. world frame expressed in world frame]
**q** _W I_ _∈_ S [3] : Rotation from IMU frame to world frame in Hamiltonian unit quaternion

_W_ **[v]** _W I_ _[∈]_ [R] [3] [ : Velocity of IMU frame wrt. world frame expressed in world frame]
**b** _g_ _∈_ R [3] : Gyro bias of IMU frame wrt. world frame expressed in IMU frame
**b** _a_ _∈_ R [3] : Accelerometer bias of IMU frame wrt. world frame expressed in IMU frame


The set of 3D landmarks _L_ _k_ in time _k_ contains all of the landmarks observed in the camera
frames corresponding to the IMU frames _I_ _k_ . For generality, we define


_L_ _k_ = � **L** _[k]_ 1 _[,][ · · ·][,]_ **[ L]** _[k]_ _l_ _k_ �


where **L** _[k]_ _j_ _[j]_ [ = 1] _[,][ · · ·][, l]_ _[k]_ [ denotes a single 3D landmark,] _[ l]_ _[k]_ [ denotes the number of active 3D land-]
marks in _I_ _k_ . Each 3D landmark normally contains the position and indexing information. Position
information can be straightforwardly presented in 3D Euclidean space, more parameterizations are
presented in Section 2.2. The indexing information for a 3D landmark contains a ”serial number”
that corresponds to the 2D feature in the set of camera frames that observed the landmark, and the
indices of this set of camera frames. This ”serial number” is a feature descriptor like ORB[24] or
BRIEF[25], etc. in descriptor-based data association.


5


**2.2** **Related Quantities in VIN and thier Parametrizations**


In this section, we discuss all relevant quantities involved in VIN estimation and the properties of
their typical representations (parametrization). Pose and landmark is the most concerned quantities
in VIN. The pose of frames consists of translation and rotation. We adopt Furgale’s conversion[26],
where coordinates of 3-vectors in VIN like translation, velocity, and acceleration need three elements
of decoration to clearly specify, which contain a physical quantity from one frame to another in
addition to being represented in a specific reference frame. For rotation, two elements are required
to represent the origin and destination frame. Assume for the moment that poses and landmarks
are all expressed in the world frame.
Parametrizations define a set of representations of the same quantity, like a 2D coordinate can
be represented in both Polar and Cartesian Coordinates. However, some limitations come with certain parametrization, like singularities and constraints. The translation lives in vector space with
no singularity and constraints. However, the rotation has several forms, including rotation matrix,
Euler angle, axis-angle, rotation vector, and unit quaternion shown in the below notation tree. The
summary of their properties is shown in Table 1. Unit quaternion compared to other parametrizations, contains fewer constraints and no singularity becoming a ”standard” way to represent rotation.
Since rotation is not in vector space but in a smooth manifold (Lie group), conventional additive
and subtractive operations in vector space cannot be used. In this case, in order to use canonical
filtering and optimization methods built on vector space operations, it is necessary to transform
the quantities in Lie groups into operable vector spaces (Lie algebras). We will discuss this later in
Section 3.3.


```
Pose

  Translation

```

`3D Euclidean Vector` _W_ **t** _W I_ _∈_ R [3]

```
  Rotation

```

`Rotation Matrix` **R** _W I_ _∈_ SO(3)


`Euler Angle` ( _p, q, r_ ) _W I_ _∈_ R [3]


`Axis-angle` ( **n** _, θ_ ) _W I_ _∈_ S [2] _×_ R


`Rotation Vector` _**θ**_ _W I_ _∈_ R [3]


`Unit Quaternion` **q** _W I_ _∈_ S [3]


```
Landmark

  3D Landmark

```

`Euclidean Point` ( _x, y, z_ ) _∈_ R [3]


`Homogeneous Point` ( _m_ _x_ _, m_ _y_ _, m_ _z_ _, ρ_ ) _∈_ P [3]

```
   Inverse Depth Parametrization
```

( _x_ 0 _, y_ 0 _, z_ 0 _, ε, α, ρ_ ) _∈_ R [6]

```
  2D Feature

```

`Pixel Location` ( _u, v_ ) _∈_ R [2]


`Elevation and Azimuth Angle` ( _ε, α_ ) _∈_ R [2]


`3D Unit Vector` ( _n_ _x_ _, n_ _y_ _, n_ _z_ ) _∈_ S [2]



Dimension Constraints Direct Composition Singularity
Rotation matrix 9 6 Matrix Multiplication None
Euler angle 3 0 None Gimbal lock
Axis-angle 4 1 Compositable over Single Axis _θ_ = 0 and _π_
Rotation Vector 3 0 Compositable over Single Axis None
Unit Quaternion 4 1 Quaternion Multiplication None


Table 1: Rotation Parametrizations


Point landmarks in VIN exist in 3D space and are observed in their 2D projection. Geometrically, the camera is a bearing sensor, and distances to landmarks cannot be measured with a
single frame. 3D landmarks in Euclidean space cannot be initialized at the first observation. In
this case, the concept of undelayed landmark initialization (ULI) is introduced by Sol`a[27], which
represents the unmeasured DOF by a Gaussian prior. This involves two important ULI landmark
parametrizations: Homogeneous points (HP) and inverse depth parametrization (IDP). HP lives
in 4-vector projective space, where ( _m_ _x_ _, m_ _y_ _, m_ _z_ ) denotes the 3D vector toward the landmark and
the scalar _ρ_ is proportional to inverse distance. IDP is introduced by Montiel _et al._ in [28] and
extended by Civera in [29]. IDP maintains relatively low non-linearity in measurement equation at
low and high feature parallax compared to Euclidean parametrization. Compared to HP, IDP adds
a predicted first-seen frame as an anchor, which allows to represent landmark uncertainty locally,
thus reducing the accumulated linearization error[30]. In [31], Sol`a _et al._ show that IDP establishes


6


better consistency in monocular EKF-SLAM compared to HP. 2D features are originally measured
in the image coordinate with the pixel unit. However, in pixel coordinate parameterization, the same
pixel coordinates represent different bearing directions in different cameras (perspective, refraction,
or catadioptric cameras). For generality, the 3D bearing unit vector is used for representing the 2D
features in the camera frame, it has no singular configurations compared to elevation and azimuth
angle (north and south poles).
Other time-varying quantities like velocity and acceleration support the estimation of poses,
whereas IMU bias, gravity, and earth rotation effect affect the estimate of velocity and acceleration.
In addition, in the case of the monocular camera, scale ambiguity also needs to be considered. Local
gravity and earth angular velocity expressed in IMU frame are fixed-length vectors _I_ **g** = _g_ _I_ ˆ **g** and

_I_ _**[ω]**_ _E_ [=] _[ ω]_ _ϵI_ _**[ω]**_ [ˆ] _E_ [, where] _[ g][ ≈]_ [9] _[.]_ [81m] _[/]_ [s] [2] [ and] _[ ω]_ _ϵ_ _[≈]_ [7] _[.]_ [29] _[×]_ [10] _[−]_ [5] [rad] _[/]_ [s with 3D unit direction vectors] _I_ **[g]** [ˆ][ and]

_I_ _**[ω]**_ [ˆ] _E_ [, symbol ˆ] _{·}_ indicates unit vector. All other quantities live in 3D vector space. Note angular
velocity and linear acceleration are measured by gyroscope and accelerometer respectively. Earth
rotation effect is measurable when using high-end IMUs, but is negligible in many VINs using lowcost IMUs. For simplicity, we neglect it in this tutorial, for considering this effect, we refer readers
to [12][3].


```
Velocity and Acceleration

```

`Linear Velocity` _W_ **v** _W I_ _∈_ R [3]


`Angular Velocity` _I_ _**ω**_ _W I_ _∈_ R [3]


`Linear Acceleration` _I_ _**a**_ _W I_ _∈_ R [3]


```
IMU Bias, Gravity, Scale and Earth Rate

```

`Gyroscope Bias` **b** _g_ _∈_ R [3]


`Accelerometer Bias` **b** _a_ _∈_ R [3]


`Local Gravity Direction` _I_ ˆ **g** _∈_ S [2]


`Monocular Scale` _s ∈_ R


`Local Earth Rate Direction` _I_ ˆ _**ω**_ _E_ _∈_ S [2]



Besides time-varying quantities that must be estimated online, there are time-invariant quantities
that can be estimated offline or online (out-of-box operation). These quantities are associated with
the sensors properties of the camera and IMU, which can be classified into the camera, IMU, and
camera-IMU related quantities shown in the below notation tree. For camera-related quantities,
camera intrinsic should be considered since images are rendered in image coordinates in pixel units
with lens distortion. Some VIN systems[32][33][34] rely on the photometric intensity of feature pixels
(direct methods), in which case time-varying auto exposure time, camera response function and the
attenuation factors due to vignetting are also needed to be calibrated[35][36] for better direct feature
tracking. For IMU-related quantities, the effects of axis misalignment, scale factor errors, and linear
acceleration on the gyroscope should be considered, especially for low-cost, consumer-grade MEMS
IMUs. For camera-IMU-related quantities, the temporal and spatial differences between these two
sensors should be considered including camera-IMU extrinsic and their time offset. In a multi-camera
multi-IMU setup, these differences in camera-camera and IMU-IMU should also be calibrated.
```
Relevant Time Invariant Quantities

  Camera Intrinsic

```

`Principal Point` ( _c_ _x_ _, c_ _y_ ) _∈_ R [2]


`Focal length` ( _f_ _x_ _, f_ _y_ ) _∈_ R [2]


`Radial and Tangential Distortion Parameters` ( **k** _,_ **p** ) _∈_ R [6] _×_ R [2]

```
  IMU Axis misalignment, Scale factor, and Linear acceleration effect on gyro

```

`Gyroscope and Accelerometer Axis misalignment` **M** _g_ _,_ **M** _a_ _∈_ _**L**_ [3] _[×]_ [3]


`Gyroscope and Accelerometer Scale factor` **S** _g_ _,_ **S** _a_ _∈_ **Λ** [3] _[×]_ [3]


`Linear acceleration effect on gyro` **B** _g_ _∈_ R [3] _[×]_ [3]

```
  Camera-IMU Extrinsic and Time Offset

```

`Camera-IMU Displacement` _C_ **t** _CI_ _∈_ R [3]


`Camera-IMU Orientation` **q** _CI_ _∈_ S [3]


`Camera-IMU Time Offset` _t_ _d_ _∈_ R


7


The active states in VIN including poses-related variables and landmarks are relative quantities
expressed in a relative reference frame. Based on the reference frame of the active state, VIN
can be divided into world-centric and robot-centric frameworks. Landmarks are measured locally,
presenting them on a local frame limits the level of uncertainty and thus reduces the linearization
errors and increases consistency in EKF framework[16][17][37].

```
Reference Frame of Active States

```

`World-Centirc` _W_ _{·}_


`Robot-Centirc` _I_ _{·}_


**2.3** **IMU and its Kinematic Model**


IMU, as a proprioceptive sensor, measures angular velocity _I_ ˜ _**ω**_ _W I_ and linear acceleration _I_ ˜ _**a**_ _W I_
by gyroscope and accelerometer respectively. Generally, IMUs are often grouped into five categories
in terms of performance: consumer-grade, tactical-grade, intermediate-grade, aviation-grade, and
marine-grade with decreasing bias and random walk but dramatically increasing price[2]. There are
also four major types of IMUs based on their mechanism: mechanical gyroscopes, RLG (ring laser
gyroscopes), FOG (fiber optic gyroscopes), and MEMS (micro-electro-mechanical systems)[38][39].
Although MEMS IMUs normally perform in consumer-grade and tactical-grade, they are extremely
low-cost, lightweight, and have low power consumption.
A general IMU measurement model is shown in Equation 2a and 2b. However, consumer-grade
IMUs often exist axis misalignment, scale factors, and linear acceleration effects on gyroscope. They
can be estimated by IMU calibration[40] and be used in a extended IMU measurement model[41].


**Gyroscope Measurement:** _I_ _**[ω]**_ [˜] _W I_ [=] _I_ _**[ω]**_ _W I_ [+] **[ b]** _g_ [+] **[ n]** _g_ (2a)

**Accelerometer Measurement:** _I_ _**[a]**_ [˜] _W I_ [=] **[ R]** _IW W_ _**[a]**_ _W I_ [+] _I_ **[g]** [ +] **[ b]** _a_ [+] **[ n]** _a_ (2b)


where


_I_ _**[ω]**_ [˜] _W I_ _[∈]_ [R] [3] : Measured angular velocity of IMU frame wrt. world frame expressed in IMU
frame

_I_ _**[a]**_ [˜] _W I_ _[∈]_ [R] [3] : Measured acceleration of IMU frame wrt. world frame expressed in IMU frame

_I_ _**[ω]**_ _W I_ _[∈]_ [R] [3] : True angular velocity of IMU frame wrt. world frame expressed in IMU frame

_W_ _**[a]**_ _W I_ _[∈]_ [R] [3] : True acceleration of IMU frame wrt. world frame expressed in IMU frame
**n** _g_ _∈_ R [3] : Gyroscope measurement noise (angular random walk)
**n** _a_ _∈_ R [3] : Accelerometer measurement noise (velocity random walk)
**R** _IW_ _∈_ SO(3) : Rotation matrix from world frame to IMU frame

_I_ **[g]** _[ ∈]_ _[g][ ·]_ [ S] [2] : Gravitational acceleration in IMU frame


Define Hamiltonian unit quaternion to rotation matrix transformation,





3
_∈_ S _,_ **R** =




**q** =



_q_ _w_



_q_ _x_

_q_ _y_

 _q_ _z_



_q_ _w_ [2] [+] _[ q]_ _x_ [2] _[−]_ _[q]_ _y_ [2] _[−]_ _[q]_ _z_ [2] 2 ( _q_ _x_ _q_ _y_ _−_ _q_ _w_ _q_ _z_ ) 2 ( _q_ _x_ _q_ _z_ + _q_ _w_ _q_ _y_ )



2 ( _q_ _x_ _q_ _y_ + _q_ _w_ _q_ _z_ ) _q_ _w_ [2] _[−]_ _[q]_ _x_ [2] [+] _[ q]_ _y_ [2] _[−]_ _[q]_ _z_ [2] 2 ( _q_ _y_ _q_ _z_ _−_ _q_ _w_ _q_ _x_ )
 2 ( _q_ _x_ _q_ _z_ _−_ _q_ _w_ _q_ _y_ ) 2 ( _q_ _y_ _q_ _z_ + _q_ _w_ _q_ _x_ ) _q_ _w_ [2] _[−]_ _[q]_ _x_ [2] _[−]_ _[q]_ _y_ [2] [+] _[ q]_ _z_ [2]



 _∈_ SO(3)





In VIN’s IMU dynamic models, measurements of angular velocity _I_ ˜ _**ω**_ _W I_ and acceleration _I_ ˜ _**a**_ _W I_
are normally treated as input. The IMU biases are modeled as random walk processes, driven by
the white Gaussian noise. The continuous-time IMU dynamic is expressed in the nonlinear state
space equation as shown,


**Translation:** _W_ **[˙t]** _W I_ [=] _W_ **[v]** _W I_ (3a)


**Velocity:** _W_ **[˙v]** _W I_ [=] _W_ _**[a]**_ _W I_ (3b)




[1] 0

2 **[q]** _[W I]_ _[ ⊗]_ � _I_ _**[ω]**_



**Rotation:** **˙q** _W I_ = [1]



_I_ _**[ω]**_ _W I_



(3c)
�



**Gyroscope bias:** **˙b** _g_ = **n** **b** _g_ (3d)

**Accelerometer bias:** **˙b** _a_ = **n** **b** _a_ (3e)


8


where


**n** **b** _g_ _∈_ R [3] : Gyroscope random walk (rate random walk)
**n** **b** _a_ _∈_ R [3] : Accelerometer random walk (acceleration random walk)


Since the angular velocity is expressed locally in IMU frame, Equation 3c can be obtained by
quaternion multiplication, where the local angular velocity is the second term of the product in pure
quaternion representation. The quaternion multiplication is shown in Equation 4,




[1] 0

2 **[q]** _[W I]_ _[ ⊗]_ � _I_ _**[ω]**_



**q** _W I_ (4)
��



**˙q** _W I_ = [1]



_I_ _**[ω]**_ _W I_



= [1]
� 2




[1] 0

2 **[Ω]** �� _I_ _**[ω]**_



_I_ _**[ω]**_ _W I_



where,



 0

_∈_ H _p_ _,_ **Ω** ( _**ω**_
�






_∈_ R 4 _×_ 4




0

_**ω**_
�



=
�



 _ω_ 0 1


_ω_ 2

 _ω_ 3



) =
�



0 _−ω_ 1 _−ω_ 2 _−ω_ 3

 _ω_ 1 0 _−ω_ 3 _ω_ 2

_ω_ 2 _ω_ 3 0 _−ω_ 1

 _ω_ 3 _−ω_ 2 _ω_ 1 0



Note that gyroscope and accelerometer random walks rarely appear in the datasheet, but can be
obtained by using Allan standard deviation[42][43][44]. In this work, these four noises **n** _g_ _,_ **n** _a_ _,_ **n** **b** _g_ _,_ **n** **b** _a_
are all assumed to be zero mean and uncorrelated white Gaussian processes. These noise parameters (noise covariance matrix **Q** ) should be determined offline or adaptively by sensor calibration.
In addition, for high-end IMUs (navigation grade or aviation grade), the earth rotation effect is
measurable, and the IMU dynamics model considering this effect can be found in [12][3]. By substituting Equation 2a and 2b into Equation 3c and 3b respectively, the non-linear continuous-time
state space Equation of IMU dynamic can be obtained in Equation 5.


**x** ˙ _I_ = **f** ( **x** _I_ _,_ ˜ **u** _,_ **w** ) (5)


where


**x** _I_ _∈_ R [3] _×_ S [3] _×_ R [3] _×_ R [3] _×_ R [3] : IMU states, ( _W_ **t** _W I_ _,_ **q** _W I_ _,_ _W_ **v** _W I_ _,_ **b** _g_ _,_ **b** _a_ )
**u** ˜ _∈_ R [3] _×_ R [3] : IMU measurements, ( _I_ ˜ _**ω**_ _W I_ _,_ _I_ ˜ _**a**_ _W I_ )
**w** _∈_ R [3] _×_ R [3] _×_ R [3] _×_ R [3] : IMU noises, ( **n** _g_ _,_ **n** _a_ _,_ **n** **b** _g_ _,_ **n** **b** _a_ )


In state estimation, we can propagate the state estimate and its uncertainty through a dynamic model. To propagate state estimates in actual implementations, discretization of nonlinear
continuous-time dynamic models is required. The discretization can be obtained by exact or numerical integration during period [ _t_ _k_ _, t_ _k_ +1 ]. For simplifying the notations, we denote IMU states at
⊺
time step _k_ (time _t_ _k_ ) as **x** _I_ _k_ = � _W_ **[t]** [⊺] _k_ _[,]_ **[ q]** _W I_ [⊺] _k_ _[,]_ _[ W]_ **[ v]** _k_ [⊺] _[,]_ **[ b]** [⊺] _g_ _k_ _[,]_ **[ b]** _a_ [⊺] _k_ � and IMU measurements at time step _k_
as ˜ **u** _k_ = [ _I_ _k_ ˜ _**ω**_ [⊺] _,_ _I_ _k_ ˜ _**a**_ [⊺] ] [⊺], and define the periodic interval between time steps ∆= _t_ _k_ +1 _−_ _t_ _k_ .
For generality, we adopt the constant linear velocity model from [45] (see the second term in
RHS of Equation 6a). In addition, we assume that the direction of the angular velocity does not
change in the interval[46], as shown in Equation 6c. For the time-varying rotation axis of angular
velocity, we refer readers to [46] and [47]. As such, the discretized model with exact integration can
be shown in the Equations 6a-6e



**Translation:** _W_ **[t]** _k_ +1 [=] _W_ **[t]** _k_ [+] _W_ **[v]** _k_ [∆] _[−]_ [1]

2 _[W]_ **[ g]** [∆] [2] [ +] **[ R]** _[W I]_ _[k]_



_t_ _k_ +1 _τ_
� �



**R** _I_ _k_ _I_ _t_ ( _I_ _t_ ˜ _**a**_ _−_ **b** _a_ _t_ _−_ **n** _a_ ) d _t_ d _τ_ (6a)



_t_ _k_



_t_ _k_



**Velocity:** _W_ **[v]** _k_ +1 [=] _W_ **[v]** _k_ _[−]_ _W_ **[g]** [∆+] **[ R]** _W I_ _k_



_t_ _k_ +1

**R** _I_ _k_ _I_ _t_ ( _I_ _t_ ˜ _**a**_ _−_ **b** _a_ _t_ _−_ **n** _a_ ) d _t_ (6b)

� _t_ _k_



0
**Rotation:** **q** _W I_ _k_ +1 = **q** _W I_ _k_ _⊗_ exp �� 21 � _tt_ _kk_ +1 ( _I_ _t_ ˜ _**ω**_ _−_ **b** _g_ _t_ _−_ **n** _g_ ) d _t_



(6c)
��



_t_ _k_ +1
**Gyro. bias:** **b** _g_ _k_ +1 = **b** _g_ _k_ + **n** **b** _g_ d _t_ (6d)
� _t_ _k_


_t_ _k_ +1
**Acc. bias:** **b** _a_ _k_ +1 = **b** _a_ _t_ + **n** **b** _a_ d _t_ (6e)
� _t_ _k_


9


_·_
where exp( ) is the quaternion exponential that maps the angular velocity in pure quaternion
(Lie Algebra) to quaternion (Lie Group).



0
_∈_ H _p_ _,_ exp _**ω**_
�� 2




0

_**ω**_
� 2



= [1]
� 2



 _ω_ 0 1


_ω_ 2

 _ω_ 3







_∥_ _**ω**_ _∥_
cos

2

�
���1 _,_ [1] 2 _**[ω]**_ [⊺] [�] [⊺]



cos � _∥_ _**ω**_ 2 _∥_ � _,_ _∥_ _**[ω]**_ _**ω**_ [⊺] _∥_ [sin] � _∥_ _**ω**_ 2 _∥_ �� ⊺ _∈_ S [3], otherwise

�1 _,_ [1] 2 _**[ω]**_ [⊺] [�] [⊺] _∈_ H, if _∥_ _**ω**_ _∥→_




[1] 2 _**[ω]**_ [⊺] [�] [⊺] _∈_ H, if _∥_ _**ω**_ _∥→_ 0



_**ω**_ 2 _∥_ � _,_ _∥_ _**[ω]**_ _**ω**_ [⊺]




_**[ω]**_ [⊺] _∥_ _**ω**_ _∥_

_∥_ _**ω**_ _∥_ [sin] � 2



2



=
��



However, the exact discretized model contains integral terms that may or may not have the
corresponding closed-form solution in analytical form. Also, discrete IMU measurements are sampled
at periodic time steps, so certain assumptions (piecewise constant or piecewise linear) should be
made during interval integration. In this case, closed-form or numerical integration with certain
assumptions is used to eliminate the integral terms. Note that in Equation 6d and 6e the integration
of noises does not shift mean but increases the uncertainty of bias. Isolating the integral terms, we
obtain the ”preintegrated” IMU measurements as follows in Equations 7a-7c,



_t_ _k_ +1

_**α**_ _I_ _k_ _I_ _k_ +1 =
� _t_ _k_



_τ_

**R** _I_ _k_ _I_ _t_ ( _I_ _t_ ˜ _**a**_ _−_ **b** _a_ _t_ _−_ **n** _a_ ) d _t_ d _τ_ (7a)

� _t_ _k_



_t_ _k_ +1
_**β**_ _I_ _k_ _I_ _k_ +1 = **R** _I_ _k_ _I_ _t_ ( _I_ _t_ ˜ _**a**_ _−_ **b** _a_ _t_ _−_ **n** _a_ ) d _t_ (7b)
� _t_ _k_



0
_**γ**_ _I_ _k_ _I_ _k_ +1 = exp �� 21 � _tt_ _kk_ +1 ( _I_ _t_ ˜ _**ω**_ _−_ **b** _g_ _t_ _−_ **n** _g_ ) d _t_



(7c)
��



These three integral terms ( _**α**_ _I_ _k_ _I_ _k_ +1 _,_ _**β**_ _I_ _k_ _I_ _k_ +1 _,_ _**γ**_ _I_ _k_ _I_ _k_ +1 ) can be preintegrated with zero initial conditions (identity for rotation) since they only depend on IMU measurements and bias in interval

[ _t_ _k_ _, t_ _k_ +1 ]. They can be obtained by closed-form or numerical integration with certain piece-wise
assumptions. The numerical integration is normally presented by Runge-Kutta integration in different orders(1 [th] order Euler, 2 [nd] order Tustin or mid-point, or 4 [th] order RK4) with increasing
precision and computational cost. The closed-form integration is derived by Eckenhoff _et al._ in

[14]. In terms of the piece-wise assumptions during integration, Forster _et al._ assume piecewise
constant ( _I_ _**ω**_ _W I_ _,_ _W_ _**a**_ _W I_ ) in [48][49], where Eckenhoff _et al._ [50][14] consider two models that assumes
piecewise constant ( _I_ ˜ _**ω**_ _W I_ _,_ _I_ ˜ _**a**_ _W I_ ) and ( _I_ ˜ _**ω**_ _W I_ _,_ _I_ _**a**_ _W I_ ). Since preintegration terms also depend on the
IMU bias, a first-order Taylor expansion on the bias linearization point is usually used to avoid
re-preintegration when the bias linearization point changes[14][48][13].
By applying the closed-form or numerical integration with proper piece-wise assumption, a noisefree discretized model without integral terms **f** _d_ ( _·_ ) can be obtained to propagate state estimate in
the filter-based framework or to construct the residual terms in the optimization-based framework
in Equation 8a and 8b respectively. _⊖_ denotes a generic minus operation.


**State Propagation:** **x** _I_ _k_ +1 = **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0) (8a)

**State Residuals:** **e** _k_ = **x** _I_ _k_ +1 _⊖_ **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0) (8b)


Since the IMU measurement rate is much faster than that of the camera, the state propagation
will take place several times before a new camera measurement arises for state update. In this case,
we drop the conventional notion of the posteriori estimate _{·}_ [+] after update and the priori estimate
_{·}_ _[−]_ through propagation.
To propagate uncertainty through dynamics, we adopt concepts from Sol`a’s paper[51] on error
state dynamics, in which case, the true state **x** [true] _I_ can be decomposed into a nominal state **x** _I_
containing a large signal and an error state _δ_ **x** _I_ containing a small signal.


**x** [true] _I_ = **x** _I_ _⊕_ _δ_ **x** _I_


where _⊕_ denotes a generic plus operation. Hence, there exist two dynamics: nominal state
and error state dynamic, where the nominal state dynamic can be used for propagating the state
estimate (mean), and the error state dynamic can be used for propagating uncertainty (covariance).
The nominal state is propagated by **f** _d_ ( _·_ ). In order to propagate the error state, we first define the
error state in VIN as,


_δ_ **x** _I_ = [ _W_ _δ_ **t** [⊺] _W I_ _[, δ]_ _**[θ]**_ _W I_ [⊺] _[,]_ _[ W]_ _[ δ]_ **[v]** _W I_ [⊺] _[, δ]_ **[b]** [⊺] _g_ _[, δ]_ **[b]** _a_ [⊺] []] [⊺] _[∈]_ [R] [3] _[ ×]_ [ R] [3] _[ ×]_ [ R] [3] _[ ×]_ [ R] [3] _[ ×]_ [ R] [3]


where


10


_δ_ _**θ**_ _W I_ _∈_ R [3] : Perturbation of rotation, which is Lie Algebra of unit quaternion in Cartesian
vector space


_·_
By using quaternion exponential exp( ) that maps Lie Algebra to Lie Group, the composition of
nominal and error state of rotation can be obtained as follows,



0
**q** [true] = **q** _⊗_ exp _δ_ _**θ**_
�� 2



��



Error state consists of only small signals, it can be considered as a perturbation of the nominal state.
Thus, the error state dynamic preserves linearity. We adopt the linearized error-state process model
in [51] but drop the gravity term for generality.


**Translation:** _W_ _[δ]_ **[˙t]** _W I_ [=] _W_ _[δ]_ **[v]** _W I_ (9a)

**Velocity:** _W_ _[δ]_ **[ ˙v]** _W I_ [=] _[ −]_ **[R]** _W I_ _[⌊]_ _I_ _**[a]**_ [˜] _W I_ _[−]_ **[b]** _a_ _[⌋]_ _×_ _[δ]_ _**[θ]**_ _W I_ _[−]_ **[R]** _W I_ _[δ]_ **[b]** _a_ _[−]_ **[R]** _W I_ **[n]** **b** _a_ (9b)

**Rotation:** _δ_ _**θ**_ [˙] _W I_ = _−⌊_ _I_ ˜ _**ω**_ _W I_ _−_ **b** _g_ _⌋_ _×_ _δ_ _**θ**_ _W I_ _−_ _δ_ **b** _g_ _−_ **n** **b** _g_ (9c)

**Gyroscope bias:** _δ_ **b** **[˙]** _g_ = **n** **b** _g_ (9d)

**Accelerometer bias:** _δ_ **b** **[˙]** _a_ = **n** **b** _a_ (9e)


where the skew operation _⌊·⌋_ _×_ is defined as,



 _∈_ R [3] _,_ _⌊_ _**ω**_ _⌋_ _×_ =





0 _−ω_ 3 _ω_ 2

 _ω_ 3 0 _−ω_ 1

 _−ω_ 2 _ω_ 1 0



 _∈_ R [3] _[×]_ [3]



_**ω**_ =



_ω_ 1

 _ω_ 2

 _ω_ 3



A continuous-time linearization error state dynamic model over the nominal state (linearization
point) can be obtained by Equations 9a-9e in state space form as,


_δ_ ˙ **x** _I_ = **F** ( **x** _I_ ) _δ_ **x** _I_ + **G** ( **x** _I_ ) **w** (10)


where


**F** ( **x** _I_ ) _∈_ R [15] _[×]_ [15] : Linearized IMU state Jacobian over nominal state **x** _I_
**G** ( **x** _I_ ) _∈_ R [15] _[×]_ [12] : IMU process noise Jacobian over nominal state **x** _I_


⊺

Note that **w** = � **n** [⊺] _g_ _[,]_ **[ n]** _a_ [⊺] _[,]_ **[ n]** [⊺] **b** _g_ _[,]_ **[ n]** **b** [⊺] _a_ � is the process noise of the dynamic, we assume the time
invariant process noise covariance matrix **Q** is known by sensor calibration. The IMU state error
covariance **P** _I_ can be propagated through continuous-time Riccati differential equation as follow,


**P** ˙ _I_ = **F** ( **x** _I_ ) **P** _I_ + **P** _I_ **F** [⊺] ( **x** _I_ ) + **G** ( **x** _I_ ) **QG** ( **x** _I_ ) [⊺]


where


**P** _I_ _∈_ R [15] _[×]_ [15] : Continuous-time IMU state error covariance
**Q** _∈_ R [12] _[×]_ [12] : Continuous-time IMU process noise covariance matrix


However, obtaining the IMU error covariance in a new time step requires solving the matrix
Riccati differential equation by techniques like matrix fraction decomposition. Alternatively, for
simplicity, we can discretize the linearized error state dynamics, then propagate the error covariance
through the discretized linearized error state dynamics in Equation 11. By exact integration with
the constant assumption of Jacobian **F** ( **x** _I_ _k_ ) and **G** ( **x** _I_ _k_ ) over the interval [ _t_ _k_ _, t_ _k_ +1 ], we obtain


_t_ _k_ +1
_δ_ **x** _I_ _k_ +1 = _e_ **[F]** [(] **[x]** _[Ik]_ [)] [(] _[t]_ _[k]_ [+1] _[−][t]_ _[k]_ [)] _δ_ **x** _I_ _k_ + _e_ **[F]** [(] **[x]** _[Ik]_ [)] [(] _[t]_ _[k]_ [+1] _[−][τ]_ [)] **G** ( **x** _I_ _k_ ) **w** d _τ_ (11)
� _t_ _k_


where


_e_ **[F]** [(] **[x]** _[Ik]_ [)] [(] _[t]_ _[k]_ [+1] _[−][t]_ _[k]_ [)] _∈_ R [15] _[×]_ [15] : Matrix exponential (state-transition matrix) over interval [ _t_ _k_ _, t_ _k_ +1 ].
For simplicity, denote it as **Φ** ( _t_ _k_ +1 _, t_ _k_ )


Given the discretized error state dynamic, the error state covariance matrix is propagated,


11


**P** _I_ _k_ +1 = **Φ** ( _t_ _k_ +1 _, t_ _k_ ) **P** _I_ _k_ **Φ** [⊺] ( _t_ _k_ +1 _, t_ _k_ ) + **Q** _k_ (12)


where


**P** _I_ _k_ _∈_ R [15] _[×]_ [15] : IMU state error covariance in time step _k_
**Q** _k_ _∈_ R [15] _[×]_ [15] : Discrete-time process noise covariance matrix, derived by Equation 13


_t_ _k_ +1
**Q** _k_ = **Φ** ( _t_ _k_ +1 _, τ_ ) **G** ( **x** _I_ _k_ ) **QG** [⊺] ( **x** _I_ _k_ ) **Φ** [⊺] ( _t_ _k_ +1 _, τ_ ) d _τ_ (13)
� _t_ _k_


In conclusion, the noise-free IMU state dynamic and its linearized error state dynamic enable
the propagation of the state estimate and uncertainty. After deriving the corresponding discretization models, the IMU propagation step in the filter-based framework and the IMU cost in the
optimization-based framework can be obtained as follows with the notation _∥_ _**a**_ _∥_ [2] **M** [=] _**[ a]**_ [⊺] **[M]** _[−]_ [1] _**[a]**_ [,]


**Filter-Based Propagation** **Optimization-Based IMU Cost**



**x** _I_ _k_ +1 = **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0)



**x** _I_ _k_ +1 = **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0) **C** _I_ _k_ = �

**P** _I_ _k_ +1 = **Φ** ( _t_ _k_ +1 _, t_ _k_ ) **P** _I_ _k_ **Φ** [⊺] ( _t_ _k_ +1 _, t_ _k_ ) + **Q** _k_ _I_ _k_ _∈I_



_I_ _k_ _∈I_ _k_



2
�� **x** _I_ _k_ +1 _⊖_ **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0)�� **P** _Ik_ +1



The optimization cost is in typical nonlinear least square form, which requires iterative optimization based on linearization with reasonable initialization. We will discuss this later in Section 3.2

and 3.4.


**2.4** **Camera and its Measurement Model**


Camera is the classical exteroceptive sensor. It captures the visual information of the visible
local regions from scene radiance to pixel brightness. There are various cameras for vision-based
motion estimation including monocular, stereo, RGB-D, fisheye, catadioptric, and event cameras.
Monocular cameras have the simplest setup with only one camera, but it needs to take into account
scale ambiguity. This scale vague can be resolved by stereo cameras using two-view geometry. RGBD cameras have better depth estimation under textureless conditions with the help of an active
infrared emitter added to stereo cameras. In terms of large field of view (FoV) cameras (fisheye and
catadioptric cameras), Zhang _et al._ in [52] found that they are more suitable in narrow and small
environments, while smaller FoV cameras perform better in larger scale scenarios. In recent years,
event cameras have gained much attention for their high frame rates and wide dynamic range by only
capturing the brightness changes at pixel level[53]. Regarding image capture modes of the camera,
there are two distinct modes: Rolling Shutter and Global Shutter. Rolling shutter cameras read off
image row by row, while global shutter cameras capture the whole picture simultaneously. In most
VI-SLAM-related datasets, a global shutter camera is used by default, although it is more expensive
than a rolling shutter camera. There are two major types of projection models for describing camera
measurements: perspective and catadioptric projection[54]. In this work, we focus on the classical
pinhole perspective projection model. We assume data association (feature extraction and matching
or tracking) is done in this article. First, transform the 3D landmark from the world frame to the
camera frame. Note that we use the Euclidean parametrization of 3D landmarks for generality.


**Transformation:** _C_ **[L]** [ =] **[ h]** **T** � **x** _I_ _,_ _W_ **L** _,_ ( **R** _CI_ _,_ _C_ **t** _CI_ )� ≜ **R** _CI_ **R** _IW_ ( _W_ **L** _−_ _W_ **t** _W I_ ) + _C_ **t** _CI_
(14)


where


_W_ **[L]** [ = [] _[x]_ _w_ _[, y]_ _w_ _[, z]_ _w_ []] [⊺] _[∈]_ [R] [3] : Landmark in world frame, _W_ **t** _W L_

_C_ **[L]** [ = [] _[x]_ _c_ _[, y]_ _c_ _[, z]_ _c_ []] [⊺] _[∈]_ [R] [3] : Landmark in camera frame, _C_ **t** _CL_
**R** _IW_ _∈_ SO(3) : Rotation from world frame to IMU frame obtained from **x** _I_

_I_ **[t]** _IW_ _[∈]_ [R] [3] : Position of world frame wrt. IMU frame expressed in IMU frame
obtained from **x** _I_
( **R** _CI_ _,_ _C_ **t** _CI_ ) _∈_ SO(3) _×_ R [3] : Camera-IMU extrinsic


Then the 3D point in the camera frame is then projected into the 2D image frame.


12


_x_ _c_ _/z_ _c_
**Projection:** **z** _p_ = **h** **P** ( _C_ **L** ) ≜
� _y_ _c_ _/z_ _c_


where


**z** _p_ = [ _x_ _p_ _, y_ _p_ ] [⊺] _∈_ R [2] : Projected 2D points in image frame



(15)
�



The projected 2D point is then distorted by the distortion function,



**Distortion:** **z** _d_ = **h** **D** � **z** _p_ _,_ ( **k** _,_ **p** )� ≜ [1 +] _[ k]_ [1] _[r]_ [2] [ +] _[ k]_ [2] _[r]_ [4] [ +] _[ k]_ [3] _[r]_ [6]

1 + _k_ 4 _r_ [2] + _k_ 5 _r_ [4] + _k_ 6 _r_ [6]
~~�~~ ~~�~~ � ~~�~~
Radial Distortion


where _r_ [2] = _x_ _p_ [2] + _y_ _p_ [2],



_x_ _p_
� _y_ _p_



2 _p_ 1 _x_ _p_ _y_ _p_ + _p_ 2 � _r_ [2] + 2 _x_ _p_ [2] [�]
+
� � 2 _p_ 2 _x_ _p_ _y_ _p_ + _p_ 1 � _r_ [2] + 2 _y_ _p_ [2] [�] �

� ~~��~~ ~~�~~
Tangential Distortion
(16)



**z** _d_ = [ _x_ _d_ _, y_ _d_ ] [⊺] _∈_ R [2] : Distorted 2D points in image frame
_r ∈_ R : Radius from the origin of the image to projected point in meter
**k** = [k 1 _,_ k 2 _,_ k 3 _,_ k 4 _,_ k 5 _,_ k 6 ] [⊺] _∈_ R [6] : Radial distortion coefficients
**p** = [p 1 _,_ p 2 ] [⊺] _∈_ R [2] : Tangential distortion coefficients


Finally, the distorted 2D points in the image frame will move the origin from the center to the
top right corner and transfer the units from meters to pixels, as shown below.



+ _n_ _u_
� � _n_ _v_



˜ _f_ _x_ 0 _x_ _d_
**Origin shift and Unit change:** **z** = **h** **K** ( **z** _d_ _,_ **K** ) + **v** ≜
� 0 _f_ _y_ �� _y_ _d_


where



+ _c_ _x_
� � _c_ _y_



�



(17)









**K** =



 _f_ 0 _x_ _f_ 0 _y_ _cc_ _xy_

0 0 1




**z** ˜ = [ _u, v_ ] [⊺] _∈_ R [2] : Distorted camera measurement in image frame in pixel unit
_c_ _x_ _, c_ _y_ _∈_ R : Principal point in origin shifted image frame in pixel unit
_f_ _x_ _, f_ _y_ _∈_ R : Focal lengths in pixel
**K** _∈_ R [3] _[×]_ [3] : Camera intrinsics matrix
**v** = [ _n_ _u_ _, n_ _v_ ] [⊺] _∈_ R [2] : Image noise vector


The camera-IMU extrinsic, camera intrinsic, and distortion coefficients are assumed to be given
through camera calibration. In summary, a single 3D landmark can be observed by the camera
frame in the camera measurement model in Equation 18,


**z** ˜ = **h** � **x** _I_ _,_ _W_ **L** _,_ ( **R** _CI_ _,_ _C_ **t** _CI_ ) _,_ ( **k** _,_ **p** ) _,_ **K** � + **v**

(18)
≜ **h** **K** � **h** **D** � **h** **P** � **h** **T** � **x** _I_ _,_ _W_ **L** _,_ ( **R** _CI_ _,_ _C_ **t** _CI_ )�� _,_ ( **k** _,_ **p** )� _,_ **K** � + **v**


Since the image distortion can be removed in a preprocessing step, the undistorted camera
measurement model can be obtained in Equation 19,


**z** ˜ = **h** � **x** _I_ _,_ _W_ **L** _,_ ( **R** _CI_ _,_ _C_ **t** _CI_ ) _,_ **K** � + **v**

(19)
≜ **h** **K** � **h** **P** � **h** **T** � **x** _I_ _,_ _W_ **L** _,_ ( **R** _CI_ _,_ _C_ **t** _CI_ )�� _,_ **K** � + **v**


Since ( **R** _CI_ _,_ _C_ **t** _CI_ ) _,_ **K** are assumed known constants and camera and IMU are time synchronized,
the undistorted camera measurement model can be simplified as


˜ indexing ˜
**z** = **h** ( **x** _I_ _,_ _W_ **L** ) + **v** ===== _⇒_ **z** _[k]_ _j_ [=] **[ h]** [ (] **[x]** _[I]_ _k_ _[,]_ _[ W]_ **[L]** _[j]_ [) +] **[ v]**


where


13


**x** _I_ _k_ _∈_ R [3] _×_ S [3] _×_ R [3] _×_ R [3] _×_ R [3] : IMU state at time step _k_, where there is a time-synchronized
camera frame token in this time step

_W_ **[L]** _j_ _[∈]_ [R] [3] : Landmark in world frame with a unique index _j_
**z** ˜ _[k]_ _j_ _[∈]_ [R] [2] : 2D feature measurement pixel location of single landmark _j_
token at camera frame at time step _k_ in image coordinate


Note that 2D features in camera measurement contain both geometric and photometric information. In this case, there are two types of measurement errors: **geometric** and **photometric**
differences, used as innovation terms for the update in the filter-based framework or as measurement residuals in the optimization-based framework. The geometric difference is normally called
”reprojection error” in computer vision since it measures the difference between the landmark measurement and its prediction obtained by reprojecting the landmark prediction onto the predicted
camera frame. Define the reprojection residual (measurement residual or innovation term) of one
2D feature as Equation 20,


˜ _k_
**Geometric Reprojection Residual:** **r** _[k]_ _j_ [=] **[ r]** **[g]** � **z** _j_ _[,]_ **[ h]** [ (] **[x]** _[I]_ _k_ _[,]_ _[ W]_ **[L]** _[j]_ [)] � (20)


where


**r** _[k]_ _j_ _[∈]_ [R][ or][ R] [2] [ or][ R] [3] [ : Reprojection residual of single landmark] _[ j]_ [ prediction and its measurement]
in camera frame at time step _k_ based on the geometric error metrics **r** **g** ( _·_ ).
For example, it contains 2 dimensions in pixel location difference and 3
dimensions in unit bearing vector error.


Based on the parametrization of 2D feature measurement, different error metrics can be used
for parametrizing the reprojection error. One standard reprojection error is the pixel location
difference (image plane error) on the image plane, Zhang _et al._ [52] introduce a similar faster error
metric on the unit plane. In order to present some typical error metrics, we first define backprojection _**π**_ _[−]_ [1] ( _·_ ) : R [2] _�→_ P [2] that recovers the bearing vector(up to scale) from the undistorted
camera measurement as Equation 21,


**Back-projection(up to scale):** **m** = _**π**_ _[−]_ [1] (˜ **z** ) ≜ **K** _[−]_ [1] **z** ˜ (21)


where


**m** = [ _m_ _x_ _, m_ _y_ _, m_ _z_ ] [⊺] _∈_ P [2] : Bearing vector(up to scale) from measurement’s camera frame

˜
**z** = [˜ **z** _,_ 1] [⊺] _∈_ P [2] : Camera measurement in homogeneous form with symbol _{·}_


Here are some of the commonly used error metrics for the geometric difference in Equation
22a-22d.


**Image Plane Error:** **r** **z** ˜ = ˜ **z** _−_ **h** ( **x** _I_ _,_ _W_ **L** ) (22a)



(22b)
�



_m_ _x_ _/m_ _z_
**Unit Plane Error:** **r** **m** ¯ [=]
� _m_ _y_ _/m_ _z_



_−_ _x_ _c_ _/z_ _c_
� � _y_ _c_ _/z_ _c_



**m** _C_ **[L]**
**Unit Bearing Vector Error:** **r** **m** ˆ [=] (22c)
_∥_ **m** _∥_ _[−]_ _∥_ _C_ **L** _∥_

**m** [⊺] _C_ **L**
**Bearing Angle Error:** r _θ_ = arccos( (22d)
_∥_ **m** _∥∥_ _C_ **L** _∥_ [)]


where


**r** **z** ˜ _∈_ R [2] : Error on image plane
**r** **m** ¯ _[∈]_ [R] [2] [ : Error on unit plane, where ¯] **[m]** [ =] **[ m]** _[/m]_ _z_ [is the bearing vector on unit plane]
**r** **m** ˆ _[∈]_ [R] [3] [ : Error between two unit bearing vectors, where ˆ] **[m]** [ =] **[ m]** _[/][∥]_ **[m]** _[∥]_ [is the unit bearing vector]
r _θ_ _∈_ R : Angular error between two bearing vectors


Essentially, the geometric information captured by the camera is the bearing angle. For cameras
with a large FoV, in-plane reprojection error becomes less sensitive at the edge of the plane where a
large difference on the plane may correspond to a small bearing angle difference, as shown in Figure
5. In this case, Zhang _et al._ [52] suggest unit plane error metric for small FoVs cameras and unit


14


**Optical Center**


Figure 5: Reprojection Error on the image plane, unit sphere, and bearing angle in two different
viewing angles with the same bearing angle difference, where _d_ 1 and _d_ 2 are the image plane error,
_r_ 1 and _r_ 2 are unit bearing vector error, _θ_ 1 and _θ_ 2 are bearing angle error.


bearing vector error for large FoVs cameras based on their efficiency and performance over FoVs.
In this article, for generality, we adopt the standard image plane error as the geometric residual for
further discussion.

Note that in the above geometric error metrics, the prediction of the 3D landmark can be
parametrized as a bearing vector in the _local_ camera frame, which corresponds to the anchored point
parametrization suggested by Sol`a _et al._ in [30]. In this case, we adopt the IDP parametrization[28][29]
for 3D landmark prediction, which consists of a unit bearing vector ˆ **m** and an inverse depth _ρ_ in the
local frame. Here, we define the corresponding back-projection _**π**_ _[−]_ [1] ( _·_ ) : R [2] _×_ R _�→_ R [3] that recovers
the 3D landmark from IDP parametrization in Equation 23.


**Back-projection:** _C_ **[L]** [ =] _**[ π]**_ _[−]_ [1] [(˜] **[z]** _[, ρ]_ [)][ ≜] **[m]** [ˆ] (23)

_ρ_


where


**m** ˆ = **m** _/∥_ **m** _∥∈_ S [2] : Unit bearing vector from camera frame origin to landmark
_ρ ∈_ R : Inverse depth from camera frame origin to landmark


This back-projected landmark in the camera frame can be inversely transformed into the world
frame by the inverse transformation in Equation 24,


**Inverse Transformation:** _W_ **[L]** [ =] **[ h]** _[−]_ **T** [1] [(] **[x]** _[I]_ _[,]_ _[ C]_ **[L]** [)][ ≜] **[R]** [⊺] _IW_ **[R]** [⊺] _CI_ [(] _[C]_ **[L]** _[ −]_ _[C]_ **[t]** _[CI]_ [) +] _[ W]_ **[ t]** _[W I]_ (24)


Note that when evaluating the geometric difference between landmark measurement and prediction, the reprojection residual is constructed based on the 3D-to-2D correspondences. Compared
to 2D-to-2D correspondences, 3D-to-2D correspondences include the estimation of 3D landmarks.
According to Scaramuzza and Fraundorfer’s tutorial [54], 2D-to-2D and 3D-to-2D methods are more
accurate than 3D-to-3D methods. However, in the case of photometric difference, since the image
intensity does not live in 3D space, there are only pair-wise 2D-to-2D correspondences (theoretically
2D-to-3D-to-2D) between two images in photometric residual. For representing photometric residual, we firstly define an image intensity function over a pixel location of camera frame at time _k_ as
**I** _k_ ( _·_ ) : R [2] _�→_ R. Since photometric residual is based on 2D-to-2D correspondences, we define this
residual for single feature _j_ visible in an image pair at time step _k_ 1 and _k_ 2 as Equation 25,



**Photometric Residual:** r _[k]_ _j_ [1] ~~_[ )]_~~ _[k]_ [2] = r **p** � **z** ˜ _[k]_ _j_ [1] _[, ρ]_ _[k]_ _j_ [1] _[,]_ **[ x]** _[I]_ _k_ 1 _[,]_ **[ x]** _[I]_ _k_ 2



˜ ˜
� ≜ **I** _k_ 1 � **z** _[k]_ _j_ [1] � _−_ **I** _k_ 2 � **w** � **z** _[k]_ _j_ [1] ��

(25)



where wrap function **w** ( _·_ ) : R [2] _�→_ R [2], that back-projecting a feature from _k_ 1 frame to world frame
then transforms and projects it onto _k_ 2 frame, shown as follow,


˜
**w** � **z** _[k]_ _j_ [1] � = **h** � **x** _I_ _k_ 2 _,_ **h** _[−]_ **T** [1] � **x** _I_ _k_ 1 _,_ _**π**_ _[−]_ [1] (˜ **z** _[k]_ _j_ [1] _[, ρ]_ _[k]_ _j_ [1] [)] � [�]


15


r _[k]_ _j_ [1] ~~_[ )]_~~ _[k]_ [2] _∈_ R : Photometric difference of feature _j_ between its intensity measurement on _k_ 1 frame
and its predicted location’s intensity on _k_ 2 frame based on photometric error
metrics r **p** ( _·_ ).
**z** ˜ _[k]_ _j_ [1] _∈_ R [2] : 2D feature measurement pixel location of single landmark _j_ token at camera frame
at time step _k_ 1 in image coordinate
_ρ_ _[k]_ _j_ [1] _∈_ R : Inverse depth of 3D landmark corresponding to feature _j_ on _k_ 1 frame
**x** _I_ _k_ 1 _,_ **x** _I_ _k_ 2 : IMU states at time step _k_ 1 and _k_ 2



Note that in Equation 25, the photometric residual is computed over a pair of pixels, which is
commonly used in dense (DTAM[55], DVO[56][57]) and semi-dense (LSD-SLAM[34]) feature frameworks for ease of computation. However, in sparse (VI-DSO[58][59], ROVIO[60], SVO[58][61]) feature frameworks, this photometric residual is computed in a pair of pixel patches that contain the
weighted average intensity difference of neighboring pixels centered on the feature pixel. In DSO[33],
Engel _et al._ evaluate the accuracy and efficiency of nine different pixel patch patterns. They also use
a more precise photometric camera model that considers exposure time, camera response function,
and vignetting effect, since the photometric errors are sensitive to varying brightness. For simplicity,
we adopt pixel-wise instead of patch-wise photometric residual for ease of notation.
In order to update the state and error covariance in the filter-based framework or construct the
visual cost (bundle adjustment (BA) in computer vision) on the optimization-based framework, we
first define the measurement Jacobian matrix **H** _k_ over the linearization point _X_ _k_ . This measurement
Jacobian matrix can be represented in rows form, where one rows is corresponding to a single feature
2D measurement. It also can be divided into columns, where left columns are derivatives over IMU
states _[∂]_ **[h]** _[∂]_ **[h]** [for all measurements.]




_[∂]_ **[h]** _[∂]_ **[h]**

_∂I_ [and right columns are derivatives over landmarks] _∂L_



_∂L_ [for all measurements.]



_∂_ **h** 1

_∂X_

...
_∂_ **h** _j_

_∂X_

...

_∂_ **h** _m_

_∂X_









**h** _j_ _∂_ **h** _j_

_∂I_ _∂L_



_∂L_



_j_ _∂_ **h** _j_

_∂X_ [=] � _∂I_



**Measurement Jacobian:** **H** _k_ =









= _∂_ **h**
� _∂I_



_∂_ **h** _∂_ **h** _∂_ **h** _j_

_∂I_ _∂L_ � _,_



(26)
�




_[∂]_ **[h]** _[j]_

_∂X_ [=] _[ ∂]_ _∂_ **[h]** _X_ _[j]_



where _[∂]_ **[h]** _[j]_



_∂_ **[h]** _X_ _[j]_ ��� _X_ = _X_ _k_ is the Jacobian corresponding to feature _j_ 2D measurement. The Jacobian



of each measurement can be divided into columns, which include derivatives over IMU states _[∂]_ **[h]** _[j]_



_∂I_
and derivatives over landmarks _[∂]_ **[h]** _[j]_

[.]



_∂L_ [.]
The visual update step in the filter-based framework and visual cost in the optimization-based
framework can be obtained as follows, where _∥· ∥_ _γ_ is a robust penalty function ( _L_ 1, Huber, Cauchy,
etc.). **R** denotes the visual measurement noise covariance.



**Filter-Based Upate** **Optimization-Based Visual Cost**



**K** = **PH** [⊺] _k_ [(] **[H]** _[k]_ **[PH]** [⊺] _k_ [+] **[ R]** [)] _[−]_ [1]

**x** _I_ _k_ = **x** _I_ _k_ + **Kr** **C** _V_ _k_ =

**P** _I_ _k_ = ( **I** _−_ **KH** _k_ ) **P** _I_ _k_ ( **I** _−_ **KH** _k_ ) [⊺] + **KRK** [⊺]







˜ _k_ 2

� � � �� **r** **g** � **z** _j_ _[,]_ **[ h]** [ (] **[x]** _[I]_ _k_ _[,]_ _[ W]_ **[ L]** _[j]_ [)] ��� **R** _[k]_

_∈I_ _k_ **L** _∈L_ _k_ _j∈_ obs( **L** ) _j_


˜ _k_

� � � � ��r **p** � **z** _[, ρ]_ _[k]_ _[,]_ **[ x]** _[I]_ _k_ _[,]_ **[ x]** _[I]_ _i_



�



�



�



_j∈_ obs( **L** )



_k∈I_ _k_



**L** _∈L_ _k_



�



�



�



_j∈_ obs( **L** )



_i∈_ **N** ( _k_ )



˜ _k_
��r **p** � **z** _j_ _[, ρ]_ _[k]_ _j_ _[,]_ **[ x]** _[I]_ _k_ _[,]_ **[ x]** _[I]_ _i_ ��� _γ_



_k∈I_ _k_



**L** _∈L_ _k_



where in the filter-based update, **r** is the visual measurement residuals (innovation) from the
geometric **r** **g** ( _·_ ) or photometric r **p** ( _·_ ) difference. In optimization-based visual cost, _k ∈I_ _k_ denotes
the frame index _k_ in IMU frame set _I_ _k_, **L** _∈L_ _k_ denotes landmark **L** in landmark set _L_ _k_, _j ∈_ obs( **L** )
denotes the feature index _j_ from observable landmark **L** in IMU frame _k_ corresponded camera frame.
_i ∈_ **N** ( _k_ ) denotes the neighborhood frame _i_ around frame _k_ .


**2.5** **Factor Graph representation of VIN**


The evolution of visual-inertial navigation can be naturally represented in probabilistic graphical
models in terms of a directed acyclic graph (Bayesian Network) or an undirected graph (Markov
Random Field), as shown in Figure 6. In the Bayesian network shown in Figure 6a, nodes clearly
indicate three types of quantities: to be estimated states, observed measurements, and controllable
actions, while in VIN, control inputs are replaced by IMU measurements. Arrows in the Bayesian
network clearly indicate causal relationships between nodes, in which the IMU state is propagated


16


through IMU measurements, and camera measurements depends on both the state of IMU and
landmarks. In Markov Random Fields shown in Figure 6b, the dependencies of nodes are clearly
presented through undirected links. The sampling rate of the IMU measurements is much faster than
the camera measurements, and in this case, since we assume IMU and camera measurements are
time-synchronized, there will be several IMU states between two camera-synchronized IMU states
(For simplicity, there is only one IMU state between the two camera-synced IMU states in Figure 6).
Under the Gaussian uncertainty assumption, VIN can be represented in Gaussian Markov Random
Fields[62], where the linkage and correlation between nodes can be characterized by the information
matrix. The information matrix of the graphical model in VIN preserves certain sparsity. The
nonzero entries of the information matrix indicate the links between nodes, and the magnitude of
entries indicates the ”strength” of the link(correlation between nodes). The sparsity of the information matrix directly relates to the computational efficiency of VIN[63]. Optimizing the VIN problem
based on its graph structure is important for the efficiency of graph-based optimization, i.e. reasonably reducing the number of nodes (marginalization) and links (sparsification)[64][65]. For example,
marginalizing the pass nodes in the filter (MSCKF[12], ROVIO[66][60], OpenVINS[67]) and fixed-lag
smoother (OKVIS[68], VINS-Mono[13], VI-DSO[59], BASALT[69]), and removing the weak links for
sparsification (SEIF[63][70]). Marginalization and sparsification will eventually affect the pattern
of the information matrix, we refer readers to Eustice’s thesis[71] for an in-depth understanding of
inference in the information form.



IMU Measurement at time step


Camera Measurement of  feature at frame


(a) Bayesian Network



IMU State with
IMU State **Landmark**
Camera Frame


(b) Markov Random Field



Figure 6: Probabilistic Graphical Models in VIN


Factor graph is a bipartite graph originally operated by sum-product algorithm (belief propagation algorithm)[72][73]. It contains two types of nodes: variables (unknown states) and factors
(known functions). Compared to Bayesian Network and Markov Random Field, factor graphs present
relationships (factors) between variables. VIN’s factor graph has three types of factors: an optional
prior factor, IMU factors, and visual factors. The prior factor is a prior distribution over the IMU
states, by including this factor, the minimization of optimization cost in VIN can be viewed as a
maximum a posteriori (MAP) estimate. IMU factors are characterized by IMU dynamic model with
IMU measurements and their noises and visual factors are characterized by the camera measurement
model with camera measurements and their noises. Note that in this section the graph includes all
states (full batch optimization) at each time step and is incremented over time. The prior distribution describes the uncertainty of the initial state, whereas, in fixed-lag smoother, it contains prior
information from marginalization. The full factor graph describing VIN is shown in Figure 7.
As the relationship between the Gaussian Markov Random field and information matrix, the
factor graph is naturally related to the Jacobian matrix in VIN where rows in Jacobian show factors
that indicate the relation between variables. IMU factors and visual factors are binary factors,
where IMU factors link two IMU states, and visual factors link one IMU state and one landmark
respectively. The prior factor is a uni-nary factor for the initial state in batch optimization or n-nary
factors in fixed-lag smoother depending on marginalization. A conceptual diagram representing the
relationship between rows and columns in the Jacobian and Hessian (information matrix) is shown
in Figure 8.
The factor graph of VIN can be formulated as an optimization cost function in terms of factor
terms. The full cost of VIN in batch optimization includes three costs: Prior cost, IMU cost, and
Visual cost shown in Equations 27a-27c, where the prior cost of the initial states assumes Gaussian


17


**Prior Factor** **IMU Factor** **Visual Factor**


Figure 7: Factor Graph in VIN



IMU

states



Landmarks



Prior
factor


IMU
factors


Visual
factors



IMU
states Landmarks



IMU

states


Landmarks





Figure 8: Jacobian and Hessian(Information Matrix) in VIN


prior distribution with mean **x** _p_ and covariance **P** _p_ .


**Prior Cost:** **C** _p_ = _∥_ **x** _I_ 0 _⊖_ **x** _p_ _∥_ **P** [2] _p_ (27a)



**IMU Cost:** **C** _I_ _k_ = �

_I_ _k_ _∈I_ _k_



2
�� **x** _I_ _k_ +1 _⊖_ **f** _d_ ( **x** _I_ _k_ _,_ ˜ **u** _k_ _,_ 0)�� **P** _Ik_ +1 (27b)



˜ _k_ 2

� � � �� **r** **g** � **z** _j_ _[,]_ **[ h]** [ (] **[x]** _[I]_ _k_ _[,]_ _[ W]_ **[ L]** _[j]_ [)] ��� **R** _[k]_ Geometric

_∈I_ _k_ **L** _∈L_ _k_ _j∈_ obs( **L** ) _j_


˜ _k_

� � � � ��r **p** � **z** _[, ρ]_ _[k]_ _[,]_ **[ x]** _[I]_ _k_ _[,]_ **[ x]** _[I]_ _i_ ��� Photometric



�



�



�



_j∈_ obs( **L** )



**L** _∈L_ _k_



**Visual Cost:** **C** _V_ _k_ =







_k∈I_ _k_



�



�



�



_j∈_ obs( **L** )



_i∈_ **N** ( _k_ )



˜ _k_ (27c)
��r **p** � **z** _j_ _[, ρ]_ _[k]_ _j_ _[,]_ **[ x]** _[I]_ _k_ _[,]_ **[ x]** _[I]_ _i_ ��� _γ_ Photometric



_k∈I_ _k_



**L** _∈L_ _k_



The full cost can be written in Equation 28. By minimizing the cost, the optimal states and
their uncertainties estimation can be obtained, with Gaussian assumption, the optimal states and
uncertainties are presented in means and covariances (or information matrix) respectively.


**C** ( _X_ _k_ ) = **C** _p_ + **C** _I_ _k_ + **C** _V_ _k_ (28)

#### **3 State Estimation Methods in VIN**


In the preview section, we introduce the relevant quantities (states of interest and support quantities) and models (IMU and camera models) in VIN and their graph-based representations. In this
section, we discuss the existing methodologies for VIN state estimation. In this tutorial, we mainly
focus on tightly-coupled visual-inertial fusion. There are two main tightly-coupled visual-inertial
state estimation schemes: filter-based and optimization-based. As such, we briefly classify these
state estimation methods in terms of the width of the state horizon.


  **Filter (one-step recursive estimation):**


18


It normally estimates the latest state given the latest measurement. It boosts efficiency but
loses accuracy due to the marginalization of all past information. The loss of accuracy is also
due to the accumulation of linearization errors[49].


  **Fixed-lag smoother (moving horizon estimator or sliding window filter):**


It estimates the recent states given the recent measurements. It balances the speed and
precision of estimation by changing the width of the moving horizon. Note that fixed-lag
smoother is normally based on optimization.


  **Batch estimator (full horizon estimation or full smoother):**


It estimates all states given all measurements using nonlinear optimization. It gradually becomes computationally intractable with continuously increasing states.


The factor graphs of the filter and the fixed-lag smoother are shown in Figure 9a and 9b respectively. The factor graph of the batch estimator is shown in the previous section in Figure 7. Note
that due to the marginalization of past states, a dense prior factor will be introduced in both filter
and fixed-lag smoother. This will increase the density of the information matrix, and eventually
reduce the efficiency of the estimation. In this case, to reduce the computational burden from the
”fill-in” in the information matrix caused by the densely connected prior factor, certain sparsification
methods are designed to balance the efficiency and accuracy (either ”break” the links or ”drop” the
nodes).



**Visual Factor** Dense Prior


(a) Filter



**Dense Prior** **Preintegrated IMU Factor**


(b) Fixed-lag Smoother



Figure 9: Filter and Smoother in VIN


**3.1** **Filter-based Methods**


All filter-based methods in VIN are built on the foundation of Bayes filters. Bayes filters provide
a unified framework for probabilistic state estimation[74]. In the Bayes filter, the state information
is represented by the probability distribution (probability density function), and the evolution of the
state information can be characterized as two stages: Propagation and Update. In the propagation
phase, the state loses information due to propagating the state through the dynamic model, while
in the update phase, the state gains information by obtaining measurements from the measurement
model. However, parameterizing state information as probability distributions is computationally
intractable. Numerical approximations or assumptions should be made to deploy Bayesian concepts
into actual implementations. The typical numerical approximation of the Bayes filter is the particle
filter based on Monte Carlo simulation and importance sampling techniques. Also, under the linear Gaussian assumption, the Bayesian filter becomes a Kalman filter, which is the optimal state
estimator under this assumption. However, in VIN, both the IMU dynamics and camera measurement models are nonlinear. In this case, keeping the reasonable Gaussian noises assumption, the
Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) are often used in VIN to cope
with nonlinear models. In Figure 10, We briefly illustrate the relationship between these filters.
The particle filter is capable of working under the nonlinear model with non-Gaussian noise.
However, since particle filters are built based on importance sampling techniques, a large number of
particles (sampling points) are required to characterize the distributions of high-dimensional states
(curse of dimensionality). In this case, the Rao-Blackwellisation technique is used to marginalize out some of the variables[75]. Rao-Blackwellised particle filters (RBPF) have been successfully
applied in many visual-based navigation works, such as FastSLAM[76], FastSLAM2[77], and its improved versions[78][79][80] with inverse depth landmark initialization, fewer particles, and better
particle weight computation respectively. Although the above-mentioned RBPFs work well in the


19


|Bayes Filter|Col2|
|---|---|
|**Numerical Approximation**<br>**Monte Carlo with Sampling**|**Nonlinear Model**<br>**Gaussian Noise**|





**Used in VIN**


Figure 10: Filter-based Methods in VIN


2D environment, they are not computationally effective compared to UKFs and EKFs methods
with high-dimensional states, such as VIN with motion in three dimensions, more dynamic states
(velocity), and sensor biases[81]. Both UKFs and EKFs are designed to overcome nonlinearities in
the models, with Gaussian noise assumptions, and they have similar theoretical complexities. EKFs
require Jacobian over linearization points, while UKFs need to calculate the sigma points in each
iteration. Compared to the analytical Jacobian matrix, the sigma points require repeated calculations and are more computationally intensive, but UKFs and EKFs have similar performance in
SLAM[82]. In this case, filter-based methods in VIN mostly concentrate on the EKF and its dualform extended information filter (EIF) because of their efficiency. EKFs are initially implemented in
visual-only SLAM, including MonoSLAM[83][83] that consider a constant velocity motion model for
active feature searching, and robocentric frameworks[17][16] that change the reference frame from
world to local for improving the consistency of EKFs-based SLAM. In order to reduce computational
burdens while keeping the information, sparsification methods are introduced including sparse extended information filters (SEIF)[63][70] that neglecting or conservatively eliminating the distant
landmarks[70] in the information form.
The observability and consistency of the proposed estimator are always of-interest in filter-based
methods. In [84], an analysis of the observability of non-linear 2D world-centric SLAM is proposed,
which indicates that the direct use of linear observability tools yields inconsistency. This problem
is further investigated in EKF-based SLAM[85] by Huang _et al._ and first-estimates Jacobian EKF
(FEJ-EKF)[86] is proposed to improve the consistency and reduce the linearization error, where
Jacobians are computed using the first available estimate for each state variable. FEJ-EKF is
further improved by observability-constrained EKF (OC-EKF)[87], which selects the linearization
points that preserve observability while minimizing the error between the linearization point and
true state. The concept of OC-EKF was then applied to 3D VIN by Hesch _et al._ ’s work observability
constrained VINS (OC-VINS) [88][89][90].
One of the most well-known filter-based methods in VIN is the multi-state constraint Kalman
filter (MSCKF)[12] introduced by Mourikis and Roumeliotis in 2007. MSCKF is a structureless
EKF-based method that marginalizes all landmarks by nullspace operation. Bloesch _et al._ propose
ROVIO[66][60], an EFK and iterated extended Kalman filter(IEKF) based method that utilizes the
photometric innovation instead of typical geometric innovation for state update. ROVIO includes
the dynamic of a fixed number of stationary landmarks for robust estimation. While conventional
EKFs operate in vector spaces, some states of VIN lie in Lie groups. In this case, Brossard _et al._
propose a UKF on Lie groups[91][92] based on invariant Kalman filter(I-KF) theory[93]. Sol`a presents
a dedicated tutorial on error-state extended Kalman filter(ES-EKF)[51] that enables uncertainty
propagation of the quaternion Lie group under error-state dynamics.


**3.2** **Optimization-based Methods**


Compared to EKF-based methods, most optimization-based methods are also based on linearization operations but are able to iteratively re-linearize at new linearization points and repeat until
the optimization converges. In [94], Bell and Cathey prove the equivalency between IEKF and
Gauss-Newton optimization at filter update. This relinearization process in optimization reduces
the linearization error. The optimization-based methods are also called graph-based methods[95]
in SLAM. As indicated in Section 2.5, VIN has a well-forming graphical interpretation in factor
graphs. The optimization problem in VIN can be formulated as a summation of cost functions over
the factors as shown in Equations 27a-27c including prior, IMU, and visual cost. Over the past


20


decade, optimization-based methods have experienced tremendous development due to the maturity of optimization tools for state estimation including Google Ceres[96], g2o[97], GTSAM[98], etc.
There are many optimization-based masterpieces in VIN are introduced during this period, including OKVIS[68], SVO+GTSAM[48][49], ORB-VIO[99], IS-VIO[100], VINS-Mono[13], VI-DSO[59],
Kimera[101], BASALT[69], DM-VIO[102], etc. To balance efficiency and accuracy, these fixedlag smoother-based methods maintain a subset of the full state compared to batch estimators.
OKVIS[68] introduces a recent keyframe-based graph structure as a moving horizon in the fixedlag smoother framework. VINS-Mono[13] and SVO+GTSAM[48][49] remove IMU states between
consecutive camera keyframes by using IMU preintegrated factors. ORB-VIO[99] uses the covisibility graph that includes the past camera frames sharing enough common features with the current
keyframes. IS-VIO[100] introduces a specific sparsification method to sparsify dense priors while
keeping information loss to a minimum. Kimera[101] supports metric-semantic mapping with dense
mesh. BASALT[69] adopts the sparsification methods proposed in NFR[65] that maintain the information measured by Kullback–Leibler divergence after marginalizing nodes. The above-mentioned
methods are indirect methods that minimize the geometric error in optimization, where VI-DSO[59]
and DM-VIO[102] are direct methods based on photometric error. They both include the scale and
gravity direction in the tight-coupled optimization while utilizing dynamic and delayed marginalization respectively.
For long-term large-scale mapping, batch estimators with loop closure are commonly used in
VIN. In this scenario, the computational complexity increases unboundedly as the graph grows.
Kaess _et al._ propose incremental nonlinear optimization methods: iSAM[103] and iSAM2[104], that
avoiding repeated batch processing steps. However, the computational complexity depends not only
on the increasing scale of the graph but also on the connection density of the graph. In this case,
nodes marginalization(or removal) and edges sparsification are introduced including generic node
removal (GLC)[64] and nonlinear factor recovery (NFR)[65].


**3.3** **On-manifold Operation**


In VIN, the motion of the pose is normally represented by rigid body transformation in special
Euclidean group SE(3), where rotation has various parametrizations with different properties discussed in Section 2.2. All of these parameterizations related to pose transformation are Lie groups,
which is a smooth manifold. However, perturbations in Lie groups are not simple addition and subtraction operations like in vector spaces. Fortunately, all elements in the Lie group can be mapped
into a corresponding tangent space (Lie algebra) in its vector form. In this way, optimization tools
that typically operate on vector spaces can be used for on-manifold optimization. The bijective mapping functions between Lie group _M_ and Lie algebra in vector space R _[m]_ are Exponential mapping
function Exp( _·_ ) : R _[m]_ _�→M_ and Logarithm mapping function Log( _·_ ) : _M �→_ R _[m]_ . In the previous
section, we denote generic plus and minus operations _⊕_ and _⊖_, we specify this operation in vector
space and Lie groups respectively as followed.


**Vector Space** **Lie group and Lie algebra**


_X_
**PLus** ( _⊕_ ) **y** = **x** + _δ_ **x** _Y_ = _X_ Exp � _**τ**_ �


_X_
**Minus** ( _⊖_ ) _δ_ **x** = **y** _−_ **x** _**τ**_ = Log � _X_ _[−]_ [1] _Y_ �


where we adopt the right-hand convention


**x** _,_ **y** _∈_ R _[m]_ : Quantities in vector space
_δ_ **x** _∈_ R _[m]_ : Perturbation around **x** in vector space
_X_ _, Y ∈M_ : Quantities in Lie group
_X_ _**τ**_ _∈_ R _m_ : Perturbation over _X_ (right-hand convention) in Lie algebra with vector form


The back-and-forth mapping (Exponential and Logarithm) between the manifold and tangent
space enables the iterative optimization algorithms on manifolds, such as gradient descent, GaussNewton, and Levenberg-Marquardt methods, etc. Sol`a _et al._ provide a detailed tutorial on Lie
theory for robotics state estimation[105], Forster _et al._ also provide a comprehensive formulation of
on-manifold operations for IMU preintegration[48][49]. In addition, these on-manifold operations are
important for uncertainty propagation on Lie groups. By assuming Gaussian noise on the tangent
space of the manifold, Barfoot and Furgale[106] give an accurate characterization of uncertainty
propagation over SE(3) with left-hand convention. Mangelson _et al._ [107] extend the work of Barfoot
and Furgale by considering jointly correlated poses.


21


**3.4** **Calibration and Initialization in VIN**


Calibration in VIN is to estimate the model-related quantities (mostly time-invariant) discussed
in Section 2.2. Since two types of sensory modalities are used in VIN, the calibration involves
self-calibration and inter-calibration over visual and inertial sensors. Furthermore, these quantities
can be calibrated during estimation (online calibration) or beforehand (offline calibration). Camera self-calibration mainly involves the estimation of the camera intrinsic[108][109] including the
camera’s principal point, focal length, and distortion parameters. For stereo camera setup, stereo
rectification[110] is also needed which makes the epipolar lines horizontal with proper scale for
directly capturing the image disparity. Among the direct methods relying on the photometric differences of feature pixels, a more accurate photometric camera model is needed that takes into
account the time-varying auto-exposure time, camera response function, and vignetting-induced
attenuation factors[35][59][66]. IMU calibration normally requires the assistance of exteroceptive
sensors (magnetometers or cameras). In IMU calibration, the most concerned quantities are the
gyroscope and accelerometer biases. However, these two biases are time-varying quantities as random walk processes, and in this case, both biases are included in the state vector in VIN. The noise
parameters in IMU can be obtained by using Allan standard deviation[42][43][44]. For low-cost
consumer-grade IMU, the effects of axis misalignment, scale factor errors, and linear acceleration on
the gyroscope should be also considered[40][41]. IMU-camera calibration is to estimate the geometric
and temporal difference between two types of sensors. The geometric difference is normally called
IMU-camera extrinsic, which includes the displacement and orientation between two sensors. The
temporal difference is the time offset between camera and IMU measurements caused by their different latency and sampling rate. IMU-camera calibration can be solved offline by kalibr[111][40]. The
IMU-camera extrinsic is calibrated online in many VIN works including OKVIS[68], ROVIO[66][60],
VINS-Mono[13], and OpenVINS[67] in filter-based or optimization-based frameworks. The time offset between IMU and camera measurements can also be estimated online in [19][20] or avoided by
hardware synchronization[22].
The initialization is critical for the nonlinear least square optimization in fixed-lag smoother
and batch estimator in VIN. Proper initialization can prevent the optimization from converging to
false local minima due to nonlinearity and non-convexity. Although, Olson _et al._ [112] have shown
that stochastic gradient descent (SGD) can be used for poor initial estimates. In optimization-based
visual-only and visual-inertial navigation, initialization consists of the majority of the workload since
optimization toolboxes(Ceres, g2o, GTSAM, etc) are used for final code implementation. The initialization from the visual-only navigation relies on the classical multiple-view geometry in computer vision. In quantities required to be initialized in tight-coupled optimization-based VIN contains active
states (the pose of camera frames corresponding IMU frames, velocity, gyroscope and accelerometer
biases, and 3D landmarks position), scale, and gravity direction. For the monocular camera, the
relative 2-view camera pose can be estimated by the commonly used five-point algorithm[113] by
Nister or eight-point algorithm[114] by Longuet-Higgins, or by decomposing a homography matrix if
viewing planar scenes[115]. After obtaining the relative pose (rotation and up-to-scale translation),
all features observed in these two frames can be triangulated to estimate their 3D positions up to
scale. Then the absolute pose of the camera frame that observes these 3D landmarks knowing their
3D positions can be estimated by the perspective-n-point (PnP) methods like P3P[116][117] and
EPnP[118], etc. Kneip and Furgale provide a unified software library OpenGV[119] for relative and
absolute camera pose estimation. Dong-Si and Mourikis[120][121] present closed-form solutions for
VIN while considering the number of features and frames with certain trajectories needed for possible solutions. Martinelli[122] derives intuitive closed-form solutions that investigate in detail the
number of distinct solutions with different numbers of features and frames with different motions
under the biased and unbiased case. In ORB-VIO[99], Mur-Artal and Tardos propose a novel IMU
initialization method that first estimates gyroscope bias by consecutive keyframes visual odometry then approximates the scale and gravity direction given the preintegration over at least four
keyframes, after this accelerometer bias is estimated and scale and gravity direction are refined in
a similar manner, eventually velocity can be estimated given two consecutive position estimates,
gravity vector estimate, and the preintegrated factors between them. In VINS-Mono[13], Qin _et al._
presents an efficient loosely coupled initialization procedure that ignores accelerometer bias, which
is difficult to observe because most of the magnitude of the acceleration is due to gravity. Campos
_et al._ [123] introduces an outstanding inertial-only optimization for visual-inertial initialization that
jointly optimizes all the IMU-related variables and the scale factor in one step using MAP estimation.


22


#### **4 Performance Evaluation and Improvement in VIN**

The performance of VIN can be evaluated in terms of accuracy, efficiency, and robustness. Accuracy and efficiency can be assessed quantitatively, while robustness assessment is more qualitative
based on illumination changes, motion blur, and low-texture scenes. Accuracy metrics in VIN include absolute trajectory error (ATE) and relative pose error (RPE) that evaluate the geometric
error over the whole trajectory or over segmented sub-trajectory respectively[124][125][126]. When
considering the uncertainty of the estimate, the accuracy can be evaluated in terms of consistency by
normalized estimation error squared (NEES), which can be tested only in simulations[127]. Efficiency
metrics in VIN are normally evaluated in terms of computing resource usage and average processing
time instead of floating point operations per second (FLOPs) for precise computational complexity
measurement which is hard to compute depending on the convergence of iterative optimization and
randomness induced by RANSAC. Delmerico and Scaramuzza present a detailed comparison[128]
of monocular VIO in terms of accuracy, latency, and computational constraints over popular VIO
pipelines. We list some of the public-available datasets for VIN performance evaluation in Table 2.

|Dataset|Environment|Platform|Groudtruth|Year|Ref.|
|---|---|---|---|---|---|
|NTU VIRAL|Outdoor|UAV|Total Station|2022|[129]|
|Newer College|Outdoor|Handheld|LiDAR Scan Prior Map|2021|[130]|
|MADMAX|Outdoor|Rover|RTK-GNSS|2021|[131]|
|Hilti|Indoor<br>Outdoor|Handheld|Motion Capture System<br>Total Station|2021|[132]|
|4Seasons|Outdoor|Car|RTK-GNSS|2020|[133]|
|UZH-FPV Drone|Indoor<br>Outdoor|UAV|Total Station|2019|[134]|
|KAIST Urban|Outdoor|Car|RTK-GNSS<br>SLAM|2019|[135]|
|TUM VI|Indoor<br>Outdoor|Handheld|Motion Capture System|2018|[136]|
|Canoe VI|Outdoor River|USV|GPS/INS|2018|[137]|
|Oxford Car|Outdoor|Car|GPS/INS|2017|[138]|
|NCLT|Outdoor|Rover|RTK-GNSS<br>SLAM|2016|[139]|
|EuRoC|Indoor|UAV|Motion Capture System<br>Total Station|2016|[140]|
|KITTI|Outdoor|Car|RTK-GNSS/INS|2012|[124]|



Table 2: VIN Datasets


**4.1** **Accuracy, Efficiency, and Robustness Improvements**


In this section, we summarize efforts to improve the accuracy, efficiency, and robustness of existing
filter-based and optimization-based frameworks. Since EKF-based methods are more efficient than
UKF-based methods while having similar accuracy performance[82], we focus on analyzing EKFbased methods. EKF-based and optimization-based methods are normally based on linearization
and Gaussian assumptions. In EKF-based frameworks, linearization error is the main cause of
inaccuracy. In this case, inverse depth landmark parametrization is presented[31][28][29], which
reduces the linearization error of the measurement model in low-parallax scenes. Robot-centric map
joining[16][17] also improves the linearization of the model by the robot-centered representation
that bounds the uncertainty. This linearization error can also be reduced by carefully selecting
linearization points[86][88] or iterative update linearization points in IEKF[60]. However, these filterbased methods continuously marginalize past states, which leads to a densely connected prior factor
with landmarks as shown in Figure 9a. Strasdat _et al._ [141][142] indicate that the computational
cost of filter-based methods scales poorly with the number of landmarks. They conclude that the
number of landmarks involved is the key factor for increasing the accuracy of visual SLAM, while
in VIN, Bloesch _et al._ [60] point out that the quality of tracked landmarks is more important than
the quantity because the IMU provides a good motion prior. Optimization-based methods utilize
the iterative optimization methods that re-linearize the model in a new linearization point in each
iteration, this mechanism naturally reduces the linearization error. These smoothing methods also
maintain the past information that will be marginalized in the filter-based methods. For long

23


term navigation, loop closure by pose graph optimization and covisibility graph[143][99] increase the
accuracy while maintaining the efficiency.
The efficiency of VIN can be improved by reducing nodes and factors (edges in MRF) or
incremental solutions. Nodes can be marginalized or removed, and resulting dense connections
caused by fill-in in marginalization can be re-assigned by different topologies[65] or simply breaking the weak link[63][70]. In VIN, the past visual nodes (camera frame with corresponding IMU
frame) are marginalized in filter and fixed-lag smoother, which leads to a dense prior factor. In
filter-based works, sparsification that drops the weak link is used[63][70], whereas in fixed-lag
smoother, specific sparsification(IS-VIO[100]) that drops connections between landmarks and velocity and biases nodes or general sparsifications that enable different Markov blanket approximation including tree, subgraph and cliquey subgraph topologies while keeping the information loss
to a minimum(NRF[65]) are used for increasing the efficiency. In keyframes-based works including
OKVIS[68], VINS-Mono[13], and SVO+GTSAM[48][49], the visual factors are simply dropped in
non-keyframe visual nodes, and the IMU nodes are preintegrated as a preintegration factor between
consecutive keyframes. In some extreme cases, all landmarks are marginalized out using nullspace
operation[12] or using smart factors[23][48][49]. Compared to feature-based indirect methods, direct methods are faster because they skip the data association stage avoiding the expensive feature
extraction, matching, and outlier removal processes[58][59].
In real-world scenes, visual conditions involving dynamic objects, motion blur, low texture, and
illumination changes often occur. In VIN, the combination of visual and inertial sensing modalities
naturally robustifies the estimation since visual information avoids the drift caused by biased dead
reckoning in inertial integration and IMU provides motion hints for visual sensing. The robustness
of VIN can be further improved by alleviating the effect caused by misleading visual information,
including outlier rejection and inlier selection or robust M-estimators. The outlier rejection is commonly achieved by RANSAC[144] or Mahalanobis distance test over the landmark innovation and
its covariance prediction[12][66][60]. By involving the motion model as constraints, fewer points are
needed in RANSAC motion estimation procedures, from normally five[113] or eight points[114] for
six-degree motion, to two points in 2D relative motion[145], to one point in 2D motion considering
Ackermann steering model[146][147][148]. Additionally, the motion prior provided by IMU can be
used to robustly track the static features in a highly dynamic environment[149] with aggressive camera motion[150]. In terms of inlier selection, Zhao and Vela present an active feature selection and
matching algorithm[151][152] that reduces computational cost while maintaining the accuracy and
robustness of pose tracking. The robust M-estimators are commonly used in VIN for reducing the
effects caused by unrejected outliers. Mactavish and Barfoot provide a comparison[153] of different
robust cost functions in visual navigation. Yang _et al._ [154] introduce a robust penalty function with
a control parameter for graduated non-convexity, they claim that the proposed approach can be a
valid replacement for RANSAC with better performance in many spatial perception problems. In
specific VIN works, ROVIO[66][60] robustifies its filtering estimation by considering the dynamics of
the static landmarks. VINS-Mono[13] presents a robust initialization procedure to deal with the gyro
bias. Failure detection and recovery procedures are also important for the overall system robustness, as presented in typical direct methods (DSO[33], etc) and indirect methods (ORB-SLAM[155],
VINS-Mono, etc). DSO detects the failure motion by examining the current motion’s RMSE and
attempts to recover by trying up to 27 different small rotations in different directions. ORB-SLAM
detects the failure motion by examining the solutions provided by decomposing a homography or
fundamental matrix and repeats the initialization process for recovery if not a clear winner solution
is provided. VINS-Mono detects the failure by examining the number of features, continuity of position or rotation in consecutive frames, and change in gyro bias estimate. It repeats the initialization
like ORB-SLAM for failure recovery.

#### **5 Learning Era in VIN**


Both the filter-based and optimization-based approaches discussed above are typical model-based
methods built on models of IMU dynamics and camera measurements. Due to the vast development
of deep learning, data-driven methods gradually challenge(or replace) the classical model-based
methods in many areas, especially in the domain of natural language processing and computer vision.
The successful data-driven applications related to visual and inertial navigation involve single image
depth estimation (SIDE), deep visual odometry, deep inertial odometry, novel view synthesis (NVS),
semantic SLAM, feature detection and matching, etc. Single image depth estimation[156][157][158]
provided a learning-based solution for feature depth estimation which can be used for monocular


24


visual odometry and dense mapping. For deep visual odometry, Yang _et al._ [159] introduce D3VO
that predicts monocular depth, photometric uncertainty, and relative camera pose in CNN-based
network architectures. Koestler _et al._ present TANDEM[160], a real-time monocular tracking and
dense mapping framework, which combines DSO[33] and dense depth maps rendered from the global
truncated signed distance function (TSDF) model to achieve visual odometry. Ummenhofer _et_
_al._ introduce a supervised CNNs-based framework DeMoN[161] for depth and pose estimation of
two frames, where the network is based on encoder-decoder pairs with an iterative loop structure.
For deep inertial odometry, Chen _et al._ propose IONet[162], which is the first end-to-end learning
framework that takes raw IMU measurements and outputs 2D inertial odometry trajectories. Yan
_et al._ provide RoNIN[163] dataset and propose three deep network architectures based on ResNet,
LSTM, and TCN for data-driven inertial navigation. Brossard _et al._ propose the AI-IMU[164],
which adopts the filter-based framework for IMU state estimation and uses a deep neural network
to dynamically adapt the noise parameters. Liu _et al._ introduce TLIO[165] which incorporates
learning-based displacement distribution estimation into EKF-based inertial odometry. Buchanan
_et al._ introduce a learning-based IMU bias predictions[166] using two commonly used sequential
networks: LSTMs and Transformers. In the case of novel view synthesis, Mildenhall _et al._ introduce
neural radiance field (NeRF) [167], a state-of-the-art view synthesis method using a fully connected
deep network and principles from classical ray tracing.
Learning-based methods can also exact the semantic information in visual measurements. Landmarks in the environment not only contain geometric information but also semantic category information, jointly utilizing the semantic and geometric can achieve more robust and informative
landmarks distinguishment. Xiao _et al._ proposed Dynamic-SLAM[168], which added semantic segmentation to distinguish static and dynamic objects under the ORB-SLAM framework. Doherty _et_
_al._ [169] propose a robust semantic SLAM with probabilistic data association. Rosinol _et al._ provide
an open-source C++ library Kimera[101] that enables real-time VIN with 3D mesh reconstruction
and semantic labeling.
In terms of feature extraction and matching, DeTone _et al._ propose a CNN-based feature detection and description algorithm SuperPoint[170]. Sarlin _et al._ introduce SuperGlue[171], a learningbased feature matching based on graph neural networks. The data-driven approach achieves excellent
performance across many components of the vision and initial navigation pipelines. However, there
is still room for the complete end-to-end learning-based VIN. We refer the reader to paper [172]
for considering the limitations and potentials of the learning-based methods in robotics perception.
Chen _et al._ provide a comprehensive survey[173] for data-driven visual and inertial localization and
mapping.

#### **6 Conclusions**


Visual inertial navigation fuses the information provided by the camera and IMU sensor to obtain
navigation-related geometric information. The complementary property and low-cost lightweight
characteristic of these two sensory modalities make them popular in many navigation applications.
VIN is a typical state estimation problem, in this article, we clearly define the relevant quantities in
VIN and their parametrization and symbolization. Furthermore, the IMU dynamic and camera measurement models are also presented while considering IMU dynamic propagation with preintegration
and its linearized discretized error state dynamic, and camera model with geometric and photometric
residuals. In model-based methods including filter and optimization, VIN can be straightforwardly
visualized from a factor graph perspective. The performance of VIN is continuously improved using graph-based optimization methods. The data-driven methods have revolutionized many aspects
of the visual part of VIN, these learning-based methods provide alternatives with respect to the
classical model-based methods including the end-to-end learning methods and hybrid methods that
combine data-driven and model-based methods. Overall, This article hopes to provide a comprehensive overview of the VIN in terms of its relevant quantity presentation, model formulation, and
possible methodologies with certain improvements.

#### **References**


[1] Peter Corke, Jorge Lobo, and Jorge Dias. An introduction to inertial and visual sensing. _The_
_International Journal of Robotics Research_, 26(6):519–535, 2007.


25


[2] Groves Paul. _Principles of GNSS, Inertial, and Multisensor Integrated Navigation Systems,_
_Second Edition_ . Artech, 2013.


[3] Timothy D Barfoot. _State estimation for robotics_ . Cambridge University Press, 2017.


[4] Brian Curless and Marc Levoy. A volumetric method for building complex models from range
images. In _Proceedings of the 23rd annual conference on Computer graphics and interactive_
_techniques_, pages 303–312, 1996.


[5] Ke Sun, Kartik Mohta, Bernd Pfrommer, Michael Watterson, Sikang Liu, Yash Mulgaonkar,
Camillo Jose Taylor, and Vijay Kumar. Robust stereo visual inertial odometry for fast autonomous flight. In _International Conference on Robotics and Automation_, volume 3, pages
965–972, 2018.


[6] Yonggen Ling, Tianbo Liu, and Shaojie Shen. Aggressive quadrotor flight using dense visualinertial fusion. In _2016 IEEE International Conference on Robotics and Automation (ICRA)_ .
IEEE, 2016.


[7] Yulin Yang and Guoquan Huang. Acoustic-inertial underwater navigation. In _2017 IEEE_
_International Conference on Robotics and Automation (ICRA)_ . IEEE, 2017.


[8] Helen Oleynikova, Michael Burri, Simon Lynen, and Roland Siegwart. Real-time visual-inertial
localization for aerial and ground robots. In _2015 IEEE/RSJ International Conference on_
_Intelligent Robots and Systems (IROS)_ . IEEE, 2015.


[9] Jeremy Ma, Max Bajracharya, Sara Susca, Larry Matthies, and Matt Malchano. Real-time
pose estimation of a dynamic quadruped in gps-denied environments for 24-hour operation.
_The International Journal of Robotics Research_, 35(6):631–653, 2016.


[10] Anastasios I Mourikis, Nikolas Trawny, Stergios I Roumeliotis, Andrew E Johnson, Adnan
Ansar, and Larry Matthies. Vision-aided inertial navigation for spacecraft entry, descent, and
landing. _IEEE Transactions on Robotics_, 25(2):264–280, 2009.


[11] Kejian Wu, Ahmed Ahmed, Georgios Georgiou, and Stergios Roumeliotis. A square root
inverse filter for efficient vision-aided inertial navigation on mobile devices. In _Robotics: Science_
_and Systems XI_ . Robotics: Science and Systems Foundation, 2015.


[12] Anastasios I. Mourikis and Stergios I. Roumeliotis. A multi-state constraint kalman filter for
vision-aided inertial navigation. In _IEEE International Conference on Robotics and Automa-_
_tion_ . IEEE, 2007.


[13] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: A robust and versatile monocular
visual-inertial state estimator. _IEEE Transactions on Robotics_, 34(4):1004–1020, 2018.


[14] Kevin Eckenhoff, Patrick Geneva, and Guoquan Huang. _High-Accuracy Preintegration for_
_Visual-Inertial Navigation_, pages 48–63. Springer International Publishing, 2020.


[15] Dan Simon. _Optimal state estimation: Kalman, H infinity, and nonlinear approaches_ . John
Wiley & Sons, 2006.


[16] J.A. Castellanos, R. Martinez-Cantin, J.D. Tard´os, and J. Neira. Robocentric map joining:
Improving the consistency of ekf-slam. _Robotics and Autonomous Systems_, 55(1):21–29, 2007.


[17] Jos´e A Castellanos, Jos´e Neira, and Juan D Tard´os. Limits to the consistency of ekf-based
slam. _IFAC Proceedings Volumes_, 37(8):716–721, 2004.


[18] Jonathan Kelly, Nicholas Roy, and Gaurav S. Sukhatme. Determining the time delay between
inertial and visual sensor measurements. _IEEE Transactions on Robotics_, 30(6):1514–1523,
2014.


[19] Mingyang Li and Anastasios I. Mourikis. Online temporal calibration for camera–imu systems:
Theory and algorithms. _The International Journal of Robotics Research_, 33(7):947–964, 2014.


[20] Tong Qin and Shaojie Shen. Online temporal calibration for monocular visual-inertial systems.
In _IEEE/RSJ International Conference on Intelligent Robots and Systems_, 2018.


26


[21] Yulin Yang, Patrick Geneva, and Xingxing Zuo. Online self-calibration for visual-inertial
navigation systems: Models, analysis and degeneracy. _arXiv preprint arXiv:2201.09170_, 2022.


[22] Janosch Nikolic, Joern Rehder, Michael Burri, Pascal Gohl, Stefan Leutenegger, Paul T.
Furgale, and Roland Siegwart. A synchronized visual-inertial sensor system with fpga preprocessing for accurate real-time slam. In _IEEE International Conference on Robotics and_
_Automation_ . IEEE, 2014.


[23] Luca Carlone, Zsolt Kira, Chris Beall, Vadim Indelman, and Frank Dellaert. Eliminating
conditionally independent sets in factor graphs: A unifying perspective based on smart factors.
In _IEEE International Conference on Robotics and Automation_ . IEEE, 2014.


[24] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In _2011 International Conference on Computer Vision_ . IEEE, 2011.


[25] Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua. _BRIEF: Binary Robust_
_Independent Elementary Features_, pages 778–792. Springer Berlin Heidelberg, 2010.


[26] Paul Furgale. Representing robot pose: The good, the bad, and the ugly. In _workshop on_
_Lessons Learned from Building Complex Systems, IEEE International Conference on Robotics_
_and Automation (ICRA). http://paulfurgale. info/news/2_, volume 14, page 9, 2014.


[27] J. Sola, A. Monin, M. Devy, and T. Lemaire. Undelayed initialization in bearing only slam.
In _2005 IEEE/RSJ International Conference on Intelligent Robots and Systems_ . IEEE, 2005.


[28] Jos´e Marıa Martinez Montiel, Javier Civera, and Andrew J Davison. Unified inverse depth
parametrization for monocular slam. In _Robotics: Science and Systems_, 2006.


[29] J. Civera, A.J. Davison, and J. Montiel. Inverse depth parametrization for monocular slam.
_IEEE Transactions on Robotics_, 24(5):932–945, 2008.


[30] Joan Sol`a, Teresa Vidal-Calleja, Javier Civera, and Jos´e Mar´ıa Mart´ınez Montiel. Impact of
landmark parametrization on monocular ekf-slam with points and lines. _International Journal_
_of Computer Vision_, 97(3):339–368, 2012.


[31] Joan Sol`a. Consistency of the monocular ekf-slam algorithm for three different landmark
parametrizations. In _2010 IEEE International Conference on Robotics and Automation_ . IEEE,
2010.


[32] Jon Zubizarreta, Iker Aguinaga, and Jose Maria Martinez Montiel. Direct sparse mapping.
_IEEE Transactions on Robotics_, 36(4):1363–1370, 2020.


[33] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. _IEEE Transactions_
_on Pattern Analysis and Machine Intelligence_, 2018.


[34] Jakob Engel, Thomas Sch¨ops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular
slam. In _European Conference on Computer Vision_, pages 834–849, 2014.


[35] Paul Bergmann, Rui Wang, and Daniel Cremers. Online photometric calibration of auto
exposure video for realtime visual odometry and slam. _IEEE Robotics and Automation Letters_,
3(2):627–634, 2017.


[36] M.D. Grossberg and S.K. Nayar. Modeling the space of camera response functions. _IEEE_
_Transactions on Pattern Analysis and Machine Intelligence_, 26(10):1272–1282, 2004.


[37] Juan D. Tard´os, Jos´e Neira, Paul M. Newman, and John J. Leonard. Robust mapping and
localization in indoor environments using sonar data. _The International Journal of Robotics_
_Research_, 21(4):311–330, 2002.


[38] N. Barbour and G. Schmidt. Inertial sensor technology trends. _IEEE Sensors Journal_,
1(4):332–339, 2001.


[39] Naser El-Sheimy and Ahmed Youssef. Inertial sensors technologies for navigation applications:
state of the art and future trends. _Satellite Navigation_, 1(1), 2020.


27


[40] Joern Rehder, Janosch Nikolic, Thomas Schneider, Timo Hinzmann, and Roland Siegwart.
Extending kalibr: Calibrating the extrinsics of multiple imus and of individual axes. In _2016_
_IEEE International Conference on Robotics and Automation (ICRA)_, pages 4304–4311. IEEE,
2016.


[41] Christian Krebs. Generic imu-camera calibration algorithm: Influence of imu-axis on each
other. _Autonomous Systems Lab, ETH Zurich, Tech. Rep_, 2012.


[42] Oliver J Woodman. An introduction to inertial navigation. Report, University of Cambridge,
Computer Laboratory, 2007.


[43] Leslie Barreda Pupo. _Characterization of errors and noises in MEMS inertial sensors using_
_Allan variance method_ . Thesis, Polytechnic University of Catalonia, 2016.


[44] Naser El-Sheimy, Haiying Hou, and Xiaoji Niu. Analysis and modeling of inertial sensors using
allan variance. _IEEE Transactions on Instrumentation and Measurement_, 57(1):140–149, 2008.


[45] Todd Lupton and Salah Sukkarieh. Visual-inertial-aided navigation for high-dynamic motion
in built environments without initial conditions. _IEEE Transactions on Robotics_, 28(1):61–76,
2012.


[46] Michael Boyle. The integration of angular velocity. _Advances in Applied Clifford Algebras_,
27(3):2345–2374, 2017.


[47] Nikolas Trawny and Stergios I. Roumeliotis. Indirect kalman filter for 3d attitude estimation.
_University of Minnesota, Dept. of Comp. Sci. and Eng., Tech. Rep_, 2:2005, 2005.


[48] Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza. Imu preintegration
on manifold for efficient visual-inertial maximum-a-posteriori estimation. In _Robotics: Science_
_and Systems_ . Georgia Institute of Technology, 2015.


[49] Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza. On-manifold preintegration for real-time visual-inertial odometry. _IEEE Transactions on Robotics_, 33(1):1–21,
2017.


[50] Kevin Eckenhoff, Patrick Geneva, and Guoquan Huang. Closed-form preintegration methods
for graph-based visual–inertial navigation. _The International Journal of Robotics Research_,
38(5):563–586, 2019.


[51] Joan Sol`a. Quaternion kinematics for the error-state kalman filter. _arXiv pre-print server_,
2017.


[52] Zichao Zhang, Henri Rebecq, Christian Forster, and Davide Scaramuzza. Benefit of large fieldof-view cameras for visual odometry. In _2016 IEEE International Conference on Robotics and_
_Automation (ICRA)_ . IEEE, 2016.


[53] Simon Klenk, Jason Chui, Nikolaus Demmel, and Daniel Cremers. Tum-vie: The tum stereo
visual-inertial event dataset. In _International Conference on Intelligent Robots and Systems_
_(IROS)_, pages 8601–8608. IEEE, 2021.


[54] Friedrich Fraundorfer and Davide Scaramuzza. Visual odometry: Part i: The first 30 years
and fundamentals. _IEEE Robotics and Automation Magazine_, 18(4):80–92, 2011.


[55] Richard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. Dtam: Dense tracking
and mapping in real-time. In _International Conference on Computer Vision_, pages 2320–2327,
2011.


[56] Christian Kerl, Jurgen Sturm, and Daniel Cremers. Dense visual slam for rgb-d cameras. In
_Intelligent Robots and Systems_, pages 2100–2106, 2013.


[57] Christian Kerl, Jurgen Sturm, and Daniel Cremers. Robust odometry estimation for rgb-d
cameras. In _2013 IEEE International Conference on Robotics and Automation_ . IEEE, 2013.


[58] Christian Forster, Matia Pizzoli, and Davide Scaramuzza. Svo: Fast semi-direct monocular
visual odometry. In _International Conference on Robotics and Automation_, pages 15–22, 2014.


28


[59] Lukas Von Stumberg, Vladyslav Usenko, and Daniel Cremers. Direct sparse visual-inertial
odometry using dynamic marginalization. In _2018 IEEE International Conference on Robotics_
_and Automation (ICRA)_ . IEEE, 2018.


[60] Michael Bloesch, Michael Burri, Sammy Omari, Marco Hutter, and Roland Siegwart. Iterated
extended kalman filter based visual-inertial odometry using direct photometric feedback. _The_
_International Journal of Robotics Research_, 36(10):1053–1072, 2017.


[61] Christian Forster, Zichao Zhang, Michael Gassner, Manuel Werlberger, and Davide Scaramuzza. Svo: Semidirect visual odometry for monocular and multicamera systems. _IEEE_
_Transactions on Robotics_, 33(2):249–265, 2017.


[62] Yair Weiss and William T. Freeman. Correctness of belief propagation in gaussian graphical
models of arbitrary topology. _Neural Computation_, 13(10):2173–2200, 2001.


[63] Sebastian Thrun, Yufeng Liu, Daphne Koller, Andrew Y. Ng, Zoubin Ghahramani, and Hugh
Durrant-Whyte. Simultaneous localization and mapping with sparse extended information
filters. _The International Journal of Robotics Research_, 23(7-8):693–716, 2004.


[64] Nicholas Carlevaris-Bianco, Michael Kaess, and Ryan M. Eustice. Generic node removal for
factor-graph slam. _IEEE Transactions on Robotics_, 30(6):1371–1385, 2014.


[65] Mladen Mazuran, Wolfram Burgard, and Gian Diego Tipaldi. Nonlinear factor recovery for
long-term slam. _The International Journal of Robotics Research_, 35(1-3):50–72, 2016.


[66] Michael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart. Robust visual inertial
odometry using a direct ekf-based approach. In _2015 IEEE/RSJ International Conference on_
_Intelligent Robots and Systems (IROS)_, pages 298–304, 2015.


[67] Patrick Geneva, Kevin Eckenhoff, Woosik Lee, Yulin Yang, and Guoquan Huang. Openvins:
A research platform for visual-inertial estimation. In _2020 IEEE International Conference on_
_Robotics and Automation (ICRA)_ . IEEE, 2020.


[68] Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart, and Paul Furgale.
Keyframe-based visual–inertial odometry using nonlinear optimization. _The International_
_Journal of Robotics Research_, 34(3):314–334, 2015.


[69] Vladyslav Usenko, Nikolaus Demmel, David Schubert, Jorg Stuckler, and Daniel Cremers.
Visual-inertial mapping with non-linear factor recovery. _IEEE Robotics and Automation Let-_
_ters_, 5(2):422–429, 2020.


[70] U. Frese. A proof for the approximate sparsity of slam information matrices. In _IEEE Inter-_
_national Conference on Robotics and Automation_ . IEEE, 2005.


[71] Ryan M Eustice. _Large-area visually augmented navigation for autonomous underwater vehi-_
_cles_ . Thesis, Massachusetts Institute of Technology, 2005.


[72] F. R. Kschischang, B. J. Frey, and H. Loeliger. Factor graphs and the sum-product algorithm.
_IEEE Transactions on Information Theory_, 47(2):498–519, 2001.


[73] H. Loeliger. An introduction to factor graphs. _IEEE Signal Processing Magazine_, 21(1):28–41,
2004.


[74] Sebastian Thrun. Probabilistic robotics. _Communications of the ACM_, 45(3):52–57, 2002.


[75] Kevin Murphy and Stuart Russell. _Rao-Blackwellised particle filtering for dynamic Bayesian_
_networks_, pages 499–515. Springer, 2001.


[76] Michael Montemerlo, Sebastian Thrun, Daphne Koller, and Ben Wegbreit. Fastslam: A factored solution to the simultaneous localization and mapping problem. _Aaai/iaai_, 593598,
2002.


[77] Michael Montemerlo, Sebastian Thrun, Daphne Koller, and Ben Wegbreit. Fastslam 2.0: An
improved particle filtering algorithm for simultaneous localization and mapping that provably
converges. In _Int. Joint Conf. Artif. Intell._, volume 3, pages 1151–1156, 2003.


29


[78] Ethan Eade and Tom Drummond. Scalable monocular slam. In _2006 IEEE Computer Society_
_Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR’06)_ . IEEE, 2006.


[79] G. Grisetti, C. Stachniss, and W. Burgard. Improved techniques for grid mapping with raoblackwellized particle filters. _ieee transactions on robotics_, 23(1):34–46, 2007.


[80] Teddy Yap, Mingyang Li, Anastasios I. Mourikis, and Christian R. Shelton. A particle filter
for monocular vision-aided odometry. In _2011 IEEE International Conference on Robotics and_
_Automation_ . IEEE, 2011.


[81] Fredrik Gustafsson. Particle filter theory and practice with positioning applications. _IEEE_
_Aerospace and Electronic Systems Magazine_, 25(7):53–82, 2010.


[82] Zeyneb Kurt-Yavuz and Sirma Yavuz. A comparison of ekf, ukf, fastslam2.0, and ukf-based
fastslam algorithms. In _2012 IEEE 16th International Conference on Intelligent Engineering_
_Systems (INES)_ . IEEE, 2012.


[83] Davison. Real-time simultaneous localisation and mapping with a single camera. In _Proceedings_
_Ninth IEEE International Conference on Computer Vision_ . IEEE, 2003.


[84] Kwang Lee, W Wijesoma, and Javier Guzman. On the observability and observability analysis
of slam. In _IEEE/RSJ International Conference on Intelligent Robots and Systems_ . IEEE,
2006.


[85] Guoquan Huang, Anastasios I Mourikis, and Stergios I Roumeliotis. Genearalized analysis and
improvement of the consistency for ekf-based slam. _University of Minnesota_, pages 2008–0001,
2008.


[86] Guoquan P. Huang, Anastasios I. Mourikis, and Stergios I. Roumeliotis. _A First-Estimates_
_Jacobian EKF for Improving SLAM Consistency_, pages 373–382. Springer Berlin Heidelberg,
2009.


[87] Guoquan P. Huang, Anastasios I. Mourikis, and Stergios I. Roumeliotis. Observability-based
rules for designing consistent ekf slam estimators. _The International Journal of Robotics_
_Research_, 29(5):502–528, 2010.


[88] Joel A Hesch, Dimitrios G Kottas, Sean L Bowman, and Stergios I Roumeliotis. Observabilityconstrained vision-aided inertial navigation. _University of Minnesota, Dept. of Comp. Sci. &_
_Eng., MARS Lab, Tech. Rep_, 1:6, 2012.


[89] Joel A. Hesch, Dimitrios G. Kottas, Sean L. Bowman, and Stergios I. Roumeliotis. Consistency
analysis and improvement of vision-aided inertial navigation. _IEEE Transactions on Robotics_,
30(1):158–176, 2014.


[90] Dimitrios G. Kottas, Joel A. Hesch, Sean L. Bowman, and Stergios I. Roumeliotis. _On the_
_Consistency of Vision-Aided Inertial Navigation_, pages 303–317. Springer International Publishing, 2013.


[91] Martin Brossard, Silvere Bonnabel, and Axel Barrau. Invariant kalman filtering for visual
inertial slam. In _2018 21st International Conference on Information Fusion (FUSION)_ . IEEE,
2018.


[92] Martin Brossard, Silvere Bonnabel, and Axel Barrau. Unscented kalman filter on lie groups for
visual inertial odometry. In _2018 IEEE/RSJ International Conference on Intelligent Robots_
_and Systems (IROS)_ . IEEE, 2018.


[93] Axel Barrau and Silv`ere Bonnabel. Invariant kalman filtering. _Annual Review of Control,_
_Robotics, and Autonomous Systems_, 1(1):237–257, 2018.


[94] B.M. Bell and F.W. Cathey. The iterated kalman filter update as a gauss-newton method.
_IEEE Transactions on Automatic Control_, 38(2):294–297, 1993.


[95] G Grisetti, R Kummerle, C Stachniss, and W Burgard. A tutorial on graph-based slam. _IEEE_
_Intelligent Transportation Systems Magazine_, 2(4):31–43, 2010.


[96] 2022.


30


[97] R. K¨ummerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard. G¡sup¿2¡/sup¿o: A
general framework for graph optimization. In _2011 IEEE International Conference on Robotics_
_and Automation_, pages 3607–3613, 2011.


[98] Frank Dellaert. Factor graphs and gtsam: A hands-on introduction. Report, Georgia Institute
of Technology, 2012.


[99] Raul Mur-Artal and Juan D. Tardos. Visual-inertial monocular slam with map reuse. _IEEE_
_Robotics and Automation Letters_, 2(2):796–803, 2017.


[100] Jerry Hsiung, Ming Hsiao, Eric Westman, Rafael Valencia, and Michael Kaess. Information
sparsification in visual-inertial odometry. In _2018 IEEE/RSJ International Conference on_
_Intelligent Robots and Systems (IROS)_ . IEEE, 2018.


[101] Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. Kimera: an open-source
library for real-time metric-semantic localization and mapping. In _2020 IEEE International_
_Conference on Robotics and Automation (ICRA)_, pages 1689–1696. IEEE, 2020.


[102] Lukas Von Stumberg and Daniel Cremers. Dm-vio: Delayed marginalization visual-inertial
odometry. _IEEE Robotics and Automation Letters_, 7(2):1408–1415, 2022.


[103] M. Kaess, A. Ranganathan, and F. Dellaert. isam: Incremental smoothing and mapping. _IEEE_
_Transactions on Robotics_, 24(6):1365–1378, 2008.


[104] Michael Kaess, Hordur Johannsson, Richard Roberts, Viorela Ila, John J Leonard, and Frank
Dellaert. isam2: Incremental smoothing and mapping using the bayes tree. _The International_
_Journal of Robotics Research_, 31(2):216–235, 2012.


[105] Joan Sol`a, Jeremie Deray, and Dinesh Atchuthan. A micro lie theory for state estimation in
robotics. _arXiv preprint arXiv:1812.01537_, 2020.


[106] Timothy D. Barfoot and Paul T. Furgale. Associating uncertainty with three-dimensional
poses for use in estimation problems. _IEEE Transactions on Robotics_, 30(3):679–693, 2014.


[107] Joshua G. Mangelson, Maani Ghaffari, Ram Vasudevan, and Ryan M. Eustice. Characterizing
the uncertainty of jointly distributed poses in the lie algebra. _IEEE Transactions on Robotics_,
36(5):1371–1388, 2020.


[108] R. Tsai. A versatile camera calibration technique for high-accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. _IEEE Journal on Robotics and Automation_,
3(4):323–344, 1987.


[109] Z. Zhang. A flexible new technique for camera calibration. _IEEE Transactions on Pattern_
_Analysis and Machine Intelligence_, 22(11):1330–1334, 2000.


[110] C. Loop and Zhengyou Zhang. Computing rectifying homographies for stereo vision. In _Pro-_
_ceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recogni-_
_tion (Cat. No PR00149)_ . IEEE Comput. Soc, 1999.


[111] Paul Furgale, Joern Rehder, and Roland Siegwart. Unified temporal and spatial calibration
for multi-sensor systems. In _2013 IEEE/RSJ International Conference on Intelligent Robots_
_and Systems_ . IEEE, 2013.


[112] E. Olson, J. Leonard, and S. Teller. Fast iterative alignment of pose graphs with poor initial
estimates. In _Proceedings 2006 IEEE International Conference on Robotics and Automation,_
_2006. ICRA 2006._ IEEE, 2006.


[113] D. Nister. An efficient solution to the five-point relative pose problem. _IEEE Transactions on_
_Pattern Analysis and Machine Intelligence_, 26(6):756–770, 2004.


[114] H. C. Longuet-Higgins. A computer algorithm for reconstructing a scene from two projections.
_Nature_, 293(5828):133–135, 1981.


[115] Ezio Malis and Manuel Vargas. _Deeper understanding of the homography decomposition for_
_vision-based control_ . Thesis, INRIA, 2007.


31


[116] Xiao-Shan Gao, Xiao-Rong Hou, Jianliang Tang, and Hang-Fei Cheng. Complete solution
classification for the perspective-three-point problem. _IEEE Transactions on Pattern Analysis_
_and Machine Intelligence_, 25(8):930–943, 2003.


[117] Laurent Kneip, Davide Scaramuzza, and Roland Siegwart. A novel parametrization of the
perspective-three-point problem for a direct computation of absolute camera position and
orientation. In _CVPR 2011_ . IEEE, 2011.


[118] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An accurate o (n) solution
to the pnp problem. _International journal of computer vision_, 81(2):155–166, 2009.


[119] Laurent Kneip and Paul Furgale. Opengv: A unified and generalized approach to real-time calibrated geometric vision. In _2014 IEEE International Conference on Robotics and Automation_
_(ICRA)_ . IEEE, 2014.


[120] Tue-Cuong Dong-Si and Anastasios I Mourikis. Closed-form solutions for vision-aided inertial
navigation. Technical report, University of California, 2011.


[121] Tue-Cuong Dong-Si and Anastasios I. Mourikis. Estimator initialization in vision-aided inertial navigation with unknown camera-imu calibration. In _2012 IEEE/RSJ International_
_Conference on Intelligent Robots and Systems_ . IEEE, 2012.


[122] Agostino Martinelli. Closed-form solution of visual-inertial structure from motion. _Interna-_
_tional Journal of Computer Vision_, 106(2):138–152, 2014.


[123] Carlos Campos, Jos´e MM Montiel, and Juan D Tard´os. Inertial-only optimization for visualinertial initialization. In _2020 IEEE International Conference on Robotics and Automation_
_(ICRA)_, pages 51–57. IEEE, 2020.


[124] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision
benchmark suite. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_,
pages 3354–3361, 2012.


[125] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A benchmark for the
evaluation of rgb-d slam systems. In _2012 IEEE/RSJ International Conference on Intelligent_
_Robots and Systems_, pages 573–580, 2012.


[126] Zichao Zhang and Davide Scaramuzza. A tutorial on quantitative trajectory evaluation for
visual(-inertial) odometry. In _2018 IEEE/RSJ International Conference on Intelligent Robots_
_and Systems (IROS)_ . IEEE, 2018.


[127] Yaakov Bar-Shalom, X Rong Li, and Thiagalingam Kirubarajan. _Estimation with applications_
_to tracking and navigation: theory algorithms and software_ . John Wiley & Sons, 2004.


[128] Jeffrey Delmerico and Davide Scaramuzza. A benchmark comparison of monocular visualinertial odometry algorithms for flying robots. In _2018 IEEE International Conference on_
_Robotics and Automation (ICRA)_ . IEEE, 2018.


[129] Thien-Minh Nguyen, Shenghai Yuan, Muqing Cao, Yang Lyu, Thien H Nguyen, and Lihua
Xie. Ntu viral: A visual-inertial-ranging-lidar dataset, from an aerial vehicle viewpoint. _The_
_International Journal of Robotics Research_, 41(3):270–280, 2022.


[130] Lintong Zhang, Marco Camurri, and Maurice Fallon. Multi-camera lidar inertial extension to
the newer college dataset. _arXiv preprint arXiv:2112.08854_, 2021.


[131] Lukas Meyer, Michal Sm´ıˇsek, Alejandro Fontan Villacampa, Laura Oliva Maza, Daniel Medina,
Martin J. Schuster, Florian Steidle, Mallikarjuna Vayugundla, Marcus G. M¨uller, Bernhard
Rebele, Armin Wedler, and Rudolph Triebel. The madmax data set for visual-inertial rover
navigation on mars. _Journal of Field Robotics_, 38(6):833–853, 2021.


[132] Michael Helmberger, Kristian Morin, Nitish Kumar, Danwei Wang, Yufeng Yue, Giovanni Cioffi, and Davide Scaramuzza. The hilti slam challenge dataset. _arXiv preprint_
_arXiv:2109.11316_, 2021.


32


[133] Patrick Wenzel, Rui Wang, Nan Yang, Qing Cheng, Qadeer Khan, Lukas von Stumberg,
Niclas Zeller, and Daniel Cremers. 4seasons: A cross-season dataset for multi-weather slam in
autonomous driving. In _DAGM German Conference on Pattern Recognition_, pages 404–417.
Springer, 2020.


[134] Jeffrey Delmerico, Titus Cieslewski, Henri Rebecq, Matthias Faessler, and Davide Scaramuzza.
Are we ready for autonomous drone racing? the uzh-fpv drone racing dataset. In _2019 Inter-_
_national Conference on Robotics and Automation (ICRA)_ . IEEE, 2019.


[135] Jinyong Jeong, Younggun Cho, Young-Sik Shin, Hyunchul Roh, and Ayoung Kim. Complex
urban dataset with multi-level sensors from highly diverse urban environments. _The Interna-_
_tional Journal of Robotics Research_, 38(6):642–657, 2019.


[136] David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, J¨org St¨uckler, and Daniel
Cremers. The tum vi benchmark for evaluating visual-inertial odometry. In _International_
_Conference on Intelligent Robots and Systems (IROS)_, pages 1680–1687. IEEE, 2018.


[137] Martin Miller, Soon-Jo Chung, and Seth Hutchinson. The visual–inertial canoe dataset. _The_
_International Journal of Robotics Research_, 37(1):13–20, 2018.


[138] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The
oxford robotcar dataset. _The International Journal of Robotics Research_, 36(1):3–15, 2017.


[139] Nicholas Carlevaris-Bianco, Arash K Ushani, and Ryan M Eustice. University of michigan
north campus long-term vision and lidar dataset. _The International Journal of Robotics Re-_
_search_, 35(9):1023–1035, 2016.


[140] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari,
Markus W. Achtelik, and Roland Siegwart. The euroc micro aerial vehicle datasets. _The_
_International Journal of Robotics Research_, 35(10):1157–1163, 2016.


[141] Hauke Strasdat, Jos´e MM Montiel, and Andrew J Davison. Visual slam: why filter? _Image_
_and Vision Computing_, 30(2):65–77, 2012.


[142] Hauke Strasdat, J M M Montiel, and Andrew J Davison. Real-time monocular slam: Why
filter? In _2010 IEEE International Conference on Robotics and Automation_ . IEEE, 2010.


[143] Hauke Strasdat, Andrew J. Davison, J.M.M. Montiel, and Kurt Konolige. Double window
optimisation for constant time visual slam. In _2011 International Conference on Computer_
_Vision_ . IEEE, 2011.


[144] Martin A. Fischler and Robert C. Bolles. Random sample consensus. _Communications of the_
_ACM_, 24(6):381–395, 1981.


[145] Diego Ortin and Jos´e Marıa Martınez Montiel. Indoor robot motion based on monocular
images. _Robotica_, 19(3):331–342, 2001.


[146] Davide Scaramuzza. 1-point-ransac structure from motion for vehicle-mounted cameras by
exploiting non-holonomic constraints. _International Journal of Computer Vision_, 95(1):74–85,
2011.


[147] Davide Scaramuzza. Performance evaluation of 1-point-ransac visual odometry. _Journal of_
_Field Robotics_, 28(5):792–811, 2011.


[148] Davide Scaramuzza, Friedrich Fraundorfer, and Roland Siegwart. Real-time monocular visual
odometry for on-road vehicles with 1-point ransac. In _2009 IEEE International Conference on_
_Robotics and Automation_ . IEEE, 2009.


[149] Deok-Hwa Kim, Seung-Beom Han, and Jong-Hwan Kim. _Visual Odometry Algorithm Us-_
_ing an RGB-D Sensor and IMU in a Highly Dynamic Environment_, pages 11–26. Springer
International Publishing, 2015.


[150] Myung Hwangbo, Jun-Sik Kim, and Takeo Kanade. Inertial-aided klt feature tracking for
a moving camera. In _2009 IEEE/RSJ International Conference on Intelligent Robots and_
_Systems_ . IEEE, 2009.


33


[151] Yipu Zhao and Patricio A. Vela. Good feature selection for least squares pose optimization
in vo/vslam. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems_
_(IROS)_ . IEEE, 2018.


[152] Yipu Zhao and Patricio A. Vela. Good feature matching: Toward accurate, robust vo/vslam
with low latency. _IEEE Transactions on Robotics_, 36(3):657–675, 2020.


[153] Kirk Mactavish and Timothy D. Barfoot. At all costs: A comparison of robust cost functions
for camera correspondence outliers. In _2015 12th Conference on Computer and Robot Vision_ .
IEEE, 2015.


[154] Heng Yang, Pasquale Antonante, Vasileios Tzoumas, and Luca Carlone. Graduated nonconvexity for robust spatial perception: From non-minimal solvers to global outlier rejection.
_IEEE Robotics and Automation Letters_, 5(2):1127–1134, 2020.


[155] Ra´ul Mur-Artal, J. M. M. Montiel, and Juan D. Tard´os. Orb-slam: A versatile and accurate
monocular slam system. _ieee transactions on robotics_, 31(5):1147–1163, 2015.


[156] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image
using a multi-scale deep network. _Advances in neural information processing systems_, 27, 2014.


[157] Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural fields for depth
estimation from a single image. In _IEEE conference on computer vision and pattern recognition_,
pages 5162–5170, 2015.


[158] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single
view depth estimation: Geometry to the rescue. In _European conference on computer vision_,
pages 740–756. Springer, 2016.


[159] Nan Yang, Lukas von Stumberg, Rui Wang, and Daniel Cremers. D3vo: Deep depth, deep
pose and deep uncertainty for monocular visual odometry. In _Proceedings of the IEEE/CVF_
_Conference on Computer Vision and Pattern Recognition_, pages 1281–1292, 2020.


[160] Lukas Koestler, Nan Yang, Niclas Zeller, and Daniel Cremers. Tandem: Tracking and dense
mapping in real-time using deep multi-view stereo. In _Conference on Robot Learning_, pages
34–45, 2022.


[161] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo.
In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_ . IEEE, 2017.


[162] Changhao Chen, Xiaoxuan Lu, Andrew Markham, and Niki Trigoni. Ionet: Learning to cure
the curse of drift in inertial odometry. In _Proceedings of the AAAI Conference on Artificial_
_Intelligence_, volume 32, 2018.


[163] Hang Yan, Sachini Herath, and Yasutaka Furukawa. Ronin: Robust neural inertial navigation
in the wild: Benchmark, evaluations, and new methods. _arXiv preprint arXiv:1905.12853_,
2019.


[164] Martin Brossard, Axel Barrau, and Silvere Bonnabel. Ai-imu dead-reckoning. _IEEE Transac-_
_tions on Intelligent Vehicles_, 5(4):585–595, 2020.


[165] Wenxin Liu, David Caruso, Eddy Ilg, Jing Dong, Anastasios I. Mourikis, Kostas Daniilidis,
Vijay Kumar, and Jakob Engel. Tlio: Tight learned inertial odometry. _IEEE Robotics and_
_Automation Letters_, 5(4):5653–5660, 2020.


[166] Russell Buchanan, Varun Agrawal, Marco Camurri, Frank Dellaert, and Maurice Fallon. Deep
imu bias inference for robust visual-inertial odometry with factor graphs. _IEEE Robotics and_
_Automation Letters_, 8(1):41–48, 2022.


[167] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. _NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis_,
pages 405–421. Springer International Publishing, 2020.


[168] Linhui Xiao, Jinge Wang, Xiaosong Qiu, Zheng Rong, and Xudong Zou. Dynamic-slam:
Semantic monocular visual localization and mapping based on deep learning in dynamic environment. _Robotics and Autonomous Systems_, 117:1–16, 2019.


34


[169] Kevin J Doherty, David P Baxter, Edward Schneeweiss, and John J Leonard. Probabilistic data
association via mixture models for robust semantic slam. In _IEEE International Conference_
_on Robotics and Automation (ICRA)_, pages 1098–1104, 2020.


[170] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised
interest point detection and description. In _Proceedings of the IEEE conference on computer_
_vision and pattern recognition workshops_, pages 224–236, 2018.


[171] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:
Learning feature matching with graph neural networks. In _Proceedings of the IEEE/CVF_
_conference on computer vision and pattern recognition_, pages 4938–4947, 2020.


[172] Niko S¨underhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, J¨urgen Leitner,
Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, and Peter Corke. The limits
and potentials of deep learning for robotics. _The International Journal of Robotics Research_,
37(4-5):405–420, 2018.


[173] Changhao Chen, Bing Wang, Chris Xiaoxuan Lu, Niki Trigoni, and Andrew Markham. A
survey on deep learning for localization and mapping: Towards the age of spatial machine
intelligence. _arXiv preprint arXiv:2006.12567_, 2020.


35


