Personal use of this material is permitted. Permission from the author(s) and/or copyright holder(s), must be obtained for all other uses. Please contact us and provide details if
you believe this document breaches copyrights.

## Quaternion-based Unscented Kalman Filter for 6-DoF Vision-based Inertial Navigation in GPS-denied Regions


Khashayar Ghanizadegan and Hashim A. Hashim



_**Abstract**_ **—This paper investigates the orientation, position,**
**and linear velocity estimation problem of a rigid-body moving**
**in three-dimensional (3D) space with six degrees-of-freedom (6**
**DoF). The highly nonlinear navigation kinematics are formulated**
**to ensure global representation of the navigation problem. A com-**
**putationally efficient Quaternion-based Navigation Unscented**
**Kalman Filter (QNUKF) is proposed on** S [3] _×_ R [3] _×_ R [3] **imitating the**
**true nonlinear navigation kinematics and utilize onboard Visual-**
**Inertial Navigation (VIN) units to achieve successful GPS-denied**
**navigation. The proposed QNUKF is designed in discrete form**
**to operate based on the data fusion of photographs garnered**
**by a vision unit (stereo or monocular camera) and information**
**collected by a low-cost inertial measurement unit (IMU). The**
**photographs are processed to extract feature points in 3D space,**
**while the 6-axis IMU supplies angular velocity and accelerom-**
**eter measurements expressed with respect to the body-frame.**
**Robustness and effectiveness of the proposed QNUKF have been**
**confirmed through experiments on a real-world dataset collected**
**by a drone navigating in 3D and consisting of stereo images**
**and 6-axis IMU measurements. Also, the proposed approach is**
**validated against standard state-of-the-art filtering techniques.**


_**Index Terms**_ **—Localization, Navigation, Unmanned Aerial Ve-**
**hicle, Sensor-fusion, Inertial Measurement Unit, Vision Unit.**


[For video of navigation experiment visit: link](https://youtu.be/CP3xiOcGrTc)


[For video of navigation experiment comparison visit: link](https://youtu.be/BWraOI0LAXo)


I. I NTRODUCTION


_A. Motivation_
# N AVIGATION is generally defined as the process ofdetermining the position, orientation, and linear velocity

of a rigid-body in space, performed by either an observer
or external entities. Navigation, both in its complete and
partial forms, plays a crucial role in numerous systems and
fields, significantly improving operational performance and
efficiency. Its applications range from robotics to smartphones,
aerospace, and marines [1], [2], highlighting its vital importance in driving technological and scientific progress. In the
smartphone industry, the precise estimation of a pedestrian’s
position and heading through their mobile device is essential
for improving location-based services and facilitating seamless


This work was supported in part by National Sciences and Engineering
Research Council of Canada (NSERC), under the grants RGPIN-2022-04937
and DGECR-2022-00103.
K. Ghanizadegan and H. A. Hashim are with the Department of Mechanical
and Aerospace Engineering, Carleton University, Ottawa, Ontario, K1S-5B6,
Canada (e-mail: hhashim@carleton.ca).



navigation between outdoor and indoor environments. Advancing accuracy and reliability of the estimation is critical
for enriching user experiences in wayfinding applications, as
documented in recent studies [3]–[7]. In aerospace, the precise
estimation of satellite orientation is pivotal for the functional
integrity of satellites, particularly for the accurate processing
of observational data. This aspect is fundamental to ensuring
the reliability and effectiveness of satellite operations [8]–[10].
Autonomous and unmanned robots employ navigation techniques to enhance the precision of Global Navigation Satellite
System (GNSS)-based localization systems, such as the Global
Positioning System (GPS) and GLONASS. These enhancements are critical for ensuring operational functionality in
environments where GPS signals are obstructed or unavailable

[5], [11]. This capability is particularly vital for Unmanned
Aerial Vehicles (UAVs) navigating indoors or within densely
constructed urban areas, where direct line-of-sight to satellites
is frequently obscured. Navigation methods that do not rely
on GNSS hold significant importance in maritime contexts,
where GNSS signals can be unreliable or entirely absent,
especially in deep waters or in the vicinity of harbors [12].
Such GNSS-independent navigation techniques are crucial for
ensuring the safety and efficiency of marine operations, where
traditional satellite-based positioning systems may not provide
the necessary accuracy or reliability.


_B. Related Work_


A naive approach to localizing an agent in a GNSS-denied
environment is to apply Dead Reckoning (DR) using a 6axis Inertial Measurement Unit (IMU) rigidly attached to the
vehicle and comprised of an accelerometer (supplying the apparent acceleration measurements) and a gyroscope (supplying
angular velocity measurements) [1], [5]. DR is able to produce
the vehicle navigation state utilizing gyroscope measurements
and integration of acceleration provided that initial navigation
state is available [5], [13]. DR is widely utilized for pedestrian
position and heading estimation in scenarios where GNSS
is unreliable (e.g., indoor environments). Typically, the step
count, step length, and heading angle are each determined
by an independent sensor, and these measurements are then
incorporated into the last known position to estimate the new
position. This method serves as a preprocessing step for the
DR, and aims to enhance the accuracy of heading estimation.
However, a major limitation of DR is the accumulation of
errors in the absence of any measurement model, leading



**K. Ghanizadegan and H. A. Hashim, ”Quaternion-based Unscented Kalman Filter for 6-DoF Vision-based Inertial Navigation in GPS-denied Regions,”**
**IEEE Transactions on Instrumentation and Measurement, 2025.** [doi: 10.1109/TIM.2024.3509582](https://doi.org/10.1109/TIM.2024.3509582)


to drift over time. In the pedestrian DR domain, multiple
solutions have been proposed to mitigate the drift issue, such
as collection of distance information from known landmarks,
a deep-learning based approach to supply filtered acceleration
data [6], Recurrent Neural Network (RNN) with adaptive
estimation of noise covariance [14] and others. These approaches are subsequently coupled with a Kalman-type filter
to propagate the state estimate. However, these methods, while
enhancing the robustness of the filter, fail to correct the state
estimate if significant drift occurs.
In scenarios where a robot navigates in a known environment, leveraging measurements from the environment can
be a solution to the drift problem encountered by the pure
IMU-based navigation algorithms. Ultra-wideband (UWB) has
recently gained popularity as an additional sensor in the
navigation suite. A robot equipped with a UWB tag gains
knowledge of its positions by communicating with fixed UWB
anchors with known positions [2], [15]–[18]. This is beneficial
for structured environments with sufficient fixed reference
points (e.g., ship docking [17] and warehouse [15]). However,
the need for infrastructure with known fixed anchors limits
the generalizability of UWB-based navigation algorithms in
unstructured regions [3]. With advances in three-dimensional
(3D) point cloud registration and processing technologies,
such as the Iterative Closest Point (ICP) [19] and Coherent
Point Drift (CPD) [20], the dependency on initial environment
knowledge is significantly reduced. Utilizing these algorithms,
a robot equipped only with its perspective of 3D points at multiple steps can reconstruct the environment. This reconstructed
environment then is treated as known, allowing the feature
points obtained from the environment to serve as independent
measurements alongside the IMU [1], [21]. Technologies such
as Sound Navigation And Ranging (SONAR), Light Detection
And Ranging (LIDAR), and visual inputs from mono or
stereo cameras exemplify such measurements. SONAR and
LIDAR sensors emit mechanical and electromagnetic waves,
respectively, and construct 3D point clouds by measuring the
distances that each wave travels. SONAR is a popular sensor
in marine applications [22], as water propagates mechanical
waves (sound waves) significantly better than electromagnetic
waves (light waves). LIDAR is commonly used in space
application [23] since sound waves cannot propagate through
space while light can. However, both LIDAR and SONAR
struggle in complex indoor and urban environments due to
their inability to capture colour and texture. Therefore, visionbased aided navigation (stereo or monocular camera + IMU)
has emerged as a reliable navigation alternative for GPSdenied regions. In particular, modern UAVs are equipped with
high-resolution cameras which are more cost-effective and
computationally reliable.


_C. Vision-based Navigation and Contributions_


Feature coordinates in 3D space can be extracted using
a monocular camera (given a sequence of two images with
persistent vehicle movement) or a stereo (binocular) camera

[5], [24]. While stereo vision-based navigation algorithms,
such as those proposed in [25] and [26], generally outperform



2


monocular-based systems, they encounter difficulties in dark
environments and in absence of clear features, such as a drone
facing a blank wall. Fusing data from IMUs and vision sensors
is a widely accepted solution to address navigation challenges,
as these modalities complement each other [5], [27]–[29]. This
integration of vision sensors and IMU is commonly referred to
as Visual-Inertial Navigation (VIN) [30]. Kalman-type filters
have been extensively used to address the VIN problem
due to their low computational requirements and ability to
handle non-linear models via linearization. An early significant
contribution in this area was made by Mourikis et al. [29], who
developed a multi-state Kalman filter-based algorithm for a
VIN system that operates based on the error state dynamics.
This approach allows the orientation, which has three degrees
of freedom, to be represented by a three-dimensional vector.
To address the issue of computational complexity and expand
the algorithm proposed in [29], the work in [31] explored
various methods for fusing IMU and vision data using the
Extended Kalman Filter (EKF). Specifically, they examined
the integration of these data sources during the EKF prediction
and update phases. Although EKF is an efficient algorithm,
its major shortcoming is the use of local linearization which
disregards the high nonlinearity of the navigation kinematics.
Therefore, there is a need for multiple interacting models to
tackle EKF shortcomings [32]. Moreover, EKF is designed to
account only for white noise attached to the measurements,
making it unfit for measurements with colored noise [8].
_Contributions:_ Unscented Kalman Filter (UKF) generally
outperforms the EKF [8] since it can accurately capture the
nonlinear kinematics while handling white and colored noise

[8], [33]. Motivated by the above discussion, the contributions
of this work are as follows: (1) Novel geometric Quaternionbased Navigation Unscented Kalman Filter (QNUKF) is proposed on S [3] _×_ R [3] _×_ R [3] in discrete form mimicking the true
nonlinear navigation kinematics of a rigid-body moving in the
6 DoF; (2) The proposed QNUKF is tailored to supply attitude,
position, and linear velocity estimates of a rigid-body equipped
with a VIN unit and applicable for GPS-denied navigation; (3)
The computational efficiency and robustness of the proposed
QNUKF is tested using a real-world dataset collected by a
quadrotor flying in the 6 DoF at a low sampling rate and
subsequently compared to state-of-the-art industry standard
filtering techniques.


_D. Structure_


The remainder of the paper is organized as follows: Section
II discusses preliminary concepts and mathematical foundations; Section III formulates the nonlinear navigation kinematics problem; Section IV introduces the structure of the novel
QNUKF; Section V validates the algorithm’s performance
using a real-world dataset; and Section VI provides concluding
remarks.


II. P RELIMINARIES


_Notation:_ In this paper, the set of integers, positive integers, and _m_ 1 -by- _m_ 2 matrices of real numbers are represented
by Z, Z [+], and R _[m]_ [1] _[×][m]_ [2], respectively. The Euclidean norm of


TABLE I: Nomenclature


_{B}_ / _{W}_ : Fixed body-frame / fixed world-frame
SO (3) : Special Orthogonal Group of order 3

S [3] : Three-unit-sphere

Z, Z [+] : Integer and positive integer space

_q_ _k_ _,_ ˆ _q_ _k_ : True and estimated quaternion at step _k_

_p_ _k_ _,_ ˆ _p_ _k_ : True and estimated position at step _k_

_v_ _k_ _,_ ˆ _v_ _k_ : True and estimated linear velocity at step _k_

_r_ _e,k_, _p_ _e,k_, _v_ _e,k_ : Attitude, position, and velocity estimation error
_a_ _k_ _, a_ _m,k_ : True and measured acceleration at step _k_
_ω_ _k_ _, ω_ _m,k_ : True and measured angular velocity at step _k_
_n_ _ω,k_ _, n_ _a,k_ : Angular velocity and acceleration measurements
noise

_b_ _ω,k_ _, b_ _a,k_ : Angular velocity and acceleration measurements
bias

_C_ _×_ : Covariance matrix of _n_ _×_ .

_f_ _b_ _, f_ _w_ : Feature points coordinates in _{B}_ and _{W}_ .

_x_ _k_, _x_ _[a]_ _k_ [,] _[ u]_ _[k]_ : The state, augmented state, and input vectors at
the _k_ th time step

_z_ ˆ _k_ _, z_ _k_ : Predicted and true measurement
_{X_ _i|j_ _}_, _{X_ _i_ _[a]_ _|j_ _[}]_ [,] : Sigma points of state, augmented state, and
measurements
_{Z_ _i|j_ _}_


a vector _v ∈_ R _[m]_ _[v]_ is defined by _||v||_ = _√v_ _[⊤]_ _v_, where _v_ _[⊤]_ refers

to the transpose of _v_ . **I** _m_ is identity matrix with dimension _m_ by- _m_ . The world-frame _{W}_ refers to a reference-frame fixed
to the Earth and _{B}_ describes the body-frame which is fixed
to the vehicle’s body. Table I provides important notation used
throughout the paper. The skew-symmetric [ _·_ ] _×_ of _v ∈_ R [3] is
defined by:



3


subject to singularities [35]. Angle-axis and Rodrigues vector
parameterization are also subject to singularity in multiple
configurations [35]. Unit-quaternion provides singularity-free
orientation representation, while being intuitive and having
only 4 parameters with 1 constraint to satisfy the 3 DoF [35].
A quaternion vector _q_ is defined in the scaler-first format by
_q_ = [ _q_ _w_ _, q_ _x_ _, q_ _y_ _, q_ _z_ ] _[⊤]_ = [ _q_ _w_ _, q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] [ with] _[ q]_ _[v]_ _[∈]_ [R] [3] [,] _[ q]_ _[w]_ _[∈]_ [R][,]
and
S [3] := � _q ∈_ R [4] [��] _||q||_ = 1� (5)


Let _⊗_ denote quaternion product of two quaternion vectors.
The quaternion product of two rotations _q_ 1 = [ _q_ _w_ 1 _, q_ _v_ 1 ] _[⊤]_ and
_q_ 2 = [ _q_ _w_ 2 _, q_ _v_ 2 ] _[⊤]_ is given by [35]:


_q_ 3 = _q_ 1 _⊗_ _q_ 2



= _q_ _w_ 1 _q_ _w_ 2 _−_ _q_ _v_ _[⊤]_ 1 _[q]_ _[v]_ [2]
� _q_ _w_ 1 _q_ _v_ 2 + _q_ _w_ 2 _q_ _v_ 1 + [ _q_ _v_ 1 ] _×_ _q_ _v_ 2



_∈_ S [3] (6)
�



The inverse quaternion of _q_ = [ _q_ _w_ _, q_ _x_ _, q_ _y_ _, q_ _z_ ] _[⊤]_ = [ _q_ _w_ _, q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_
S [3] is represented by


_q_ _[−]_ [1] = [ _q_ _w_ _, −q_ _x_ _, −q_ _y_ _, −q_ _z_ ] _[⊤]_ = [ _q_ _w_ _, −q_ _v_ _[⊤]_ []] _[⊤]_ _[∈]_ [S] [3] (7)


Note that _q_ _I_ = [1 _,_ 0 _,_ 0 _,_ 0] _[⊤]_ refers to the quaternion identity
where _q ⊗_ _q_ _[−]_ [1] = _q_ _I_ . The rotation matrix can be described
using quaternion _R_ _q_ ( _q_ ) such that _R_ _q_ : S [3] _→_ SO(3) [35]:


_R_ _q_ ( _q_ ) = ( _q_ _w_ [2] _[−||][q]_ _[v]_ _[||]_ [2] [)] _[I]_ [3] [+ 2] _[q]_ _[v]_ _[q]_ _v_ _[⊤]_ [+ 2] _[q]_ _[w]_ [[] _[q]_ _[v]_ []] _[×]_ _[∈]_ [SO][(3)][ (8)]


Also, quaternion can be extracted given a rotation matrix _R ∈_
SO(3) and the mapping _q_ _r_ : SO(3) _→_ S [3] is defined by [35]:









12 �1 + _R_ (1 _,_ 1) + _R_ (2 _,_ 2) + _R_ (3 _,_ 3)
1
4 _q_ _w_ [(] _[R]_ [(3] _[,]_ [2)] _[ −]_ _[R]_ [(2] _[,]_ [3)] [)]
1
4 _q_ _w_ [(] _[R]_ [(1] _[,]_ [3)] _[ −]_ _[R]_ [(3] _[,]_ [1)] [)]
1
4 _q_ _w_ [(] _[R]_ [(2] _[,]_ [1)] _[ −]_ _[R]_ [(1] _[,]_ [2)] [)]












=










_q_ _w_

_q_ _x_

_q_ _y_

_q_ _z_



_v_ 1

 _v_ 2

 _v_ 3



(9)
The rigid-body’s orientation can be extracted using angle-axis
parameterization through a unity vector _u_ = [ _u_ 1 _, u_ 2 _, u_ 3 ] _∈_ S [2]

rotating by an angle _θ ∈_ R [35] where S [2] := _{_ _u ∈_ R [3] [��] _||u||_ =
1 _}_ . The rotation vector _r_ can be defined using the angle-axis
parameterization such that _r_ _θ,u_ : R _×_ S [2] _→_ R [3] :


_r_ = _r_ _θ,u_ ( _θ, u_ ) = _θu ∈_ R [3] (10)


The rotation matrix can be extracted given the rotation vector
and the related map is given by _R_ _r_ : R [3] _→_ SO(3) [35]


_R_ _r_ ( _r_ ) = exp([ _r_ ] _×_ ) _∈_ SO(3)

= **I** 3 + sin( _θ_ ) [ _u_ ] _×_ + (1 _−_ cos( _θ_ )) [ _u_ ] [2] _×_ (11)


The unity vector and rotation angle can be obtained from
rotation matrix _θ_ _R_ _, u_ _R_ : SO(3) _→_ R [3] _×_ R [35]:
 _θ_ _R_ = arccos Tr( _R_ ) _−_ 1 _∈_ R

� 2 �

(12)

 1



_q_ _r_ ( _R_ ) =



 _∈_ so(3) _,_ _v_ =







(1)





[ _v_ ] _×_ =



0 _−v_ 3 _v_ 2

 _v_ 3 0 _−v_ 1

 _−v_ 2 _v_ 1 0




_·_
The operator vex( ) is the inverse mapping skew-symmetric
operator to vector vex : so(3) _→_ R [3]


vex([ _v_ ] _×_ ) = _v ∈_ R [3] (2)


The anti-symmetric projection operator _P_ _a_ ( _·_ ) : R [3] _[×]_ [3] _→_ so(3)
is defined as:

_P_ _a_ ( _M_ ) = [1] (3)

2 [(] _[M][ −]_ _[M]_ _[ ⊤]_ [)] _[ ∈]_ [so][(3)] _[,][ ∀][M][ ∈]_ [R] _[m][×][m]_


_A. Orientation Representation_


SO(3) denotes the Special Orthogonal Group of order 3 and
is defined by:


SO(3) := � _R ∈_ R [3] _[×]_ [3] [��] _det_ ( _R_ ) = +1 _, RR_ _[⊤]_ = **I** 3 � (4)


where _R ∈_ SO(3) denotes orientation of a rigid-body. Euler
angle parameterization provides an intuitive representation of
the rigid-body’s orientation in 3D space, often considered
analogous to roll, pitch, and yaw angles around the _x_, _y_, and
_z_ axes, respectively. This representation has been adopted in
many pose estimation and navigation problems such as [22],

[34]. However, this representation is kinematically singular
and not globally defined. Thus, Euler angles fail to represent the vehicle orientation in several configurations and are



Tr( _R_ ) _−_ 1
_θ_ _R_ = arccos
� 2



_∈_ R
�



2



(12)
1
_u_ _R_ = vex( _P_ _a_ ( _R_ )) _∈_ S [2]
sin _θ_ _R_







Given the rotation vector _r_ in (10), in view of (11) and (9),
one has [35]


_q_ _r_ ( _r_ ) = _q_ _R_ ( _R_ _r_ ( _r_ )) _∈_ S [3]

= �cos( _θ/_ 2) _,_ sin( _θ/_ 2) _u_ _[⊤]_ [�] _[⊤]_ _∈_ S [3] (13)


Finally, to find the rotation vector _r_ _q_ ( _q_ ) corresponding to the
quaternion _q_, equations (8), (12), and (10) are used as follows:


_r_ _q_ ( _q_ ) = _r_ _θ,u_ ( _θ_ _R_ ( _R_ _q_ ( _q_ )) _, u_ _R_ ( _R_ _q_ ( _q_ ))) _∈_ R [3] (14)


_B. Summation, Deduction Operators, and Weighted Average_


Quaternions and rotation vectors cannot be directly added or
subtracted in a straightforward manner, whether separately or
side-dependent. Let us define the side-dependent summation _⊕_
and subtraction _⊖_ operators to enable quaternion and rotation
vector summation. In view of (2), (7), (10), and (13), one has:


_q ⊕_ _r_ := _q_ _r_ ( _r_ ) _⊗_ _q ∈_ S [3] (15)

_q ⊖_ _r_ := _q_ _r_ ( _r_ ) _[−]_ [1] _⊗_ _q ∈_ S [3] (16)


where _q ∈_ S [3] denotes quaternion vector and _r ∈_ R [3] denotes
rotation vector. The equations (15) and (16) provide the
resultant quaternion after subsequent rotations represented by
_q_ combined with _r_, and _q_ combined with the inverse of _r_,
respectively. Using (6), (7), and (14), the subtraction operator
of _q_ 1 _∈_ S [3] and _q_ 2 _∈_ S [3] maps S [3] _×_ S [3] _→_ R [3] and is defined
as follows:


_q_ 1 _⊖_ _q_ 2 := _r_ _q_ ( _q_ 1 _⊗_ _q_ 2 _[−]_ [1] [)] _[ ∈]_ [R] [3] (17)


The expression (17) provides the rotation vector that represents
the orientation error between the two quaternions _q_ 1 and _q_ 2 .
Note that the expressions in (15), (16), and (17) do not only
resolve the addition and subtraction operations in the S [3] and
R [3] spaces, but also provide physical meaning for these operations, specifically in terms of subsequent orientations. The
expression (17) provides the rotation vector that represents the
orientation error between the two quaternions _q_ 1 and _q_ 2 . The
algorithms provided in [8], [36] are used to obtain weighted
mean WM( _Q, W_ ) of a set of quaternions _Q_ = _{q_ _i_ _∈_ S [3] _}_ and
scaler weights _W_ = _{w_ _i_ _}_ . In order to calculate this weighted
average, first the 4-by-4 matrix _M_ is found by:


_M_ = � _w_ _i_ _q_ _i_ _q_ _i_ _[⊤]_ _[∈]_ [R] [4] _[×]_ [4]


Next, the unit eigenvector corresponding to the eigenvalue with
the highest magnitude is regarded as the weighted average. In
other words:


WM( _Q, W_ ) = EigVector( _M_ ) _i_ _∈_ S [3] (18)


where
_i_ = argmax( _|_ EigValue( _M_ ) _i_ _|_ ) _∈_ R


with EigVector( _M_ ) _i_ in (18) and EigValue( _M_ ) _i_ being the _i_ th
eigenvector and eigenvalue of _M_, respectively.


_C. Probability_


The probability of a Random Variable (RV) _A_ being equal
to _a_, where _a ∈_ R _[m]_ _[A]_, is denoted by P( _A_ = _a_ ), or more
compactly, P( _a_ ). Consider another RV _B_ with a possible value
_b ∈_ R _[m]_ _[B]_ . The conditional probability of _A_ = _a_ given that
_B_ = _b_, denoted by P( _a|b_ ), can be expressed as:

P( _a|b_ ) = [P][(] _[a][,][ b]_ [)]

P( _b_ )



4


An _m_ -dimensional RV _V_ _∈_ R _[m]_ drawn from a Gaussian

distribution with a mean of _V ∈_ R _[m]_ and a covariance matrix
of _P_ _v_ _∈_ R _[m][×][m]_ is represented by the following:


_V ∼N_ ( _V, P_ _v_ )


The Gaussian probability density function of _V_ is formulated
below:


P( _V_ ) = _N_ ( _V |V, P_ _v_ )



_−_ [1]
� 2
= [exp]



_∈_ R
(2 _π_ ) _[m]_ det( _P_ _v_ )



2 [1] [(] _[V][ −]_ _[V]_ [)] _[⊤]_ _[P]_ _[ −]_ _v_ [1] ( _V −_ _V_ )�



~~�~~



Let:
_A_

_B_
�



_A_
_∼N_

_B_

� ��



_P_ _A_ _P_ _A,B_
� _,_ � _P_ _A,B_ _[⊤]_ _P_ _B_



(19)
��



Then, given (19), the conditional probability function P( _A|B_ )
can be calculated as:


P( _A|B_ ) = _N_ ( _A, B|A_ + _P_ _A,B_ _P_ _B_ _[−]_ [1] [(] _[B][ −]_ _[B]_ [)] _[,]_

_P_ _A_ _−_ _P_ _A,B_ _P_ _B_ _[−]_ [1] _[P]_ _[ ⊤]_ _A,B_ [)] (20)


_D. The Unscented Transform_


Consider the square symmetric semi-positive definite matrix
_M ∈_ R _[m]_ _[M]_ _[×][m]_ _[M]_ . Using Singular Value Decomposition (SVD),
let _U_, _S_, and _V_ _∈_ R _[m]_ _[M]_ _[×][m]_ _[M]_ be the matrices of left
singular vectors, singular values, and right singular vectors,
respectively, such that _M_ = _USV_ _[⊤]_ . The matrix square root
of _M_, denoted as _√M_, is given by [37]:



_M_, is given by [37]:

_√M_ = _U_ _√SV_ _[⊤]_ _∈_ R _[m]_ _[M]_



_M_ = _U_ _√_



_SV_ _[⊤]_ _∈_ R _[m]_ _[M]_ _[×][m]_ _[M]_ _,_ (21)



where the square root of a diagonal matrix, such as _S_, is computed by taking the square root of its diagonal elements. The
Unscented Transform (UT) is an approach used to estimate the
probability distribution of a RV after it undergoes through a
nonlinear transformation [38]. The problem addressed by the
UT involves determining the resultant distribution of a random
variable _D ∈_ R _[m]_ _[D]_ after it is connected through a nonlinear
function f nl ( _._ ) to another random variable _C ∈_ R _[m]_ _[C]_, where
the distribution of _C_ is known:


_D_ = f nl ( _C_ )


Let _C_ be from a Gaussian distribution with a mean of _C_ and a
covariance matrix of _P_ _C_ . The sigma points representing P( _C_ ),
denoted as the set _{C_ _j_ _}_ are calculated as:
 _C_ 0 = _C ∈_ R _[m]_ _[C]_

_C_ _j_ = _C_ + ~~�~~ ( _m_ _C_ + _λ_ ) _P_ _C_

 � � _j_



_C_ 0 = _C ∈_ R _[m]_ _[C]_



_C_ _j_ = _C_ + ~~�~~
�



_j_



_j_ = _{_ 1 _,_ 2 _, . . .,_ 2 _m_ _C_ _}_
_j_ _[,]_







( _m_ _C_ + _λ_ ) _P_ _C_
�


( _m_ _C_ + _λ_ ) _P_ _C_
�



_C_ _j_ + _m_ _C_ = _C −_ ~~�~~
�



(22)
where _λ ∈_ R is a scaling parameter, and � ~~�~~ ( _m_ _C_ + _λ_ ) _P_ _C_ � _j_

is the _j_ the column of matrix square root of ( _m_ _C_ + _λ_ ) _P_ _C_ (as
defined in (21)) of ( _m_ _C_ + _λ_ ) _P_ _C_ . Hence, from (22) each sigma
point _C_ _j_ will be propagated through the nonlinear function and
sigma points representing P( _D_ ), denoted as the set _{D_ _j_ _}_ are
calculated as:



( _m_ _C_ + _λ_ ) _P_ _C_
�



_D_ _j_ = f nl ( _C_ _j_ ) _j_ = _{_ 0 _,_ 2 _, . . .,_ 2 _m_ C _}_


Accordingly, the weighted mean and covariance of the set
_{D_ _j_ _}_ provide an accurate representation up to the third degree
of the real mean and covariance matrix of _D_ . The weights can
be found as:
 _w_ 0 _[m]_ [=] _λ_ + _λλ m_ _C_ _∈_ R

_w_ 0 _[c]_ [=] + 1 _−_ _α_ [2] + _β ∈_ R

 _λ_ + _m_



5


Let _q_ _k_ _∈_ S [3], _p_ _k_ _∈_ R [3], _v_ _k_ _∈_ R [3], _ω_ _k_ _∈_ R [3], and _a_ _k_ _∈_ R [3]

represent _q ∈_ S [3], _p ∈_ R [3], _v ∈_ R [3], _ω ∈_ R [3], and _a ∈_ R [3] at
the _k_ th discrete time step, respectively. The system kinematics
can then be discretized with a sample time of _dT_, in a similar
manner to the approach described in [1], [21]:


_q_ _k_ _q_ _k−_ 1

   







_λ_
_w_ 0 _[m]_ [=] _∈_ R
_λ_ + _m_ _C_







= exp( _M_ _ck−_ 1 _[dT]_ [)]








 (28)



_q_ _k_

_p_ _k_

_v_ _k_

1









_q_ _k−_ 1

_p_ _k−_ 1

_v_ _k−_ 1

1







_λ_
_w_ 0 _[c]_ [=] + 1 _−_ _α_ [2] + _β ∈_ R
_λ_ + _m_ _C_



1
_w_ _j_ _[m]_ [=] _[ w]_ _j_ _[c]_ [=] 2( _m_ _C_ + _λ_ ) _[∈]_ [R] _[,]_ _j_ = _{_ 1 _,_ 2 _, . . .,_ 2 _m_ _C_ _}_



(23)
where _α_ and _β_ are scaling parameters in R. Given (23), the
estimated mean _D_ ~~[ˆ]~~ and covariance _P_ [ˆ] _D_ of the distribution are

calculated as follows:



where _M_ _k_ _[c]_ _−_ 1 = _M_ _[c]_ ( _q_ _k−_ 1 _, ω_ _k−_ 1 _, a_ _k−_ 1 ). At the _k_ th time
step, the measured angular velocity _ω_ _m,k_ _∈_ R [3] and linear
acceleration _a_ _m,k_ _∈_ R [3] are affected by additive white noise
and biases ( _b_ _ω,k_ _∈_ R [3] for angular velocity and _b_ _a,k_ _∈_ R [3]

for linear acceleration), which are modeled as random walks.
The relationship between the true and measured values can be
expressed as:
 _ωa_ _m,km,k_ = = _ω a_ _kk_ + + _b b_ _a,kω,k_ + + _n n_ _a,kω,k_

(29)

 _b_ _b_



_ω_ _m,k_ = _ω_ _k_ + _b_ _ω,k_ + _n_ _ω,k_

_a_ _m,k_ = _a_ _k_ + _b_ _a,k_ + _n_ _a,k_

_b_ _ω,k_ = _b_ _ω,k−_ 1 + _n_ _bω,k−_ 1

_b_ _a,k_ = _b_ _a,k−_ 1 + _n_ _ba,k−_ 1



~~ˆ~~
_D_ =


_P_ ˆ _D_ =



2 _m_ _C_
� _w_ _j_ _[m]_ _[D]_ _[j]_ (24)

_j_ =0


2 _m_ _C_
� _w_ _j_ _[c]_ [[] _[D]_ _[j]_ _[−]_ _[D]_ ~~[ˆ]~~ [][] _[D]_ _[j]_ _[−]_ _[D]_ ~~[ˆ]~~ []] _[⊤]_ _[∈]_ [R] _[m]_ _[D]_ _[×][m]_ _[D]_ (25)

_j_ =0



(29)



III. P ROBLEM F ORMULATION AND M EASUREMENTS


In this section, we will develop a state transition function
that describes the relationship between the current state, the
previous state, and the input vectors. Additionally, a measurement function is formulated to describe the relationship
between the state vector and the measurement vector. These

functions are crucial for enhancing filter performance.


_A. Model Kinematics_


Consider a rigid-body moving in 3D space, with an angular
velocity _ω ∈_ R [3] and an acceleration _a ∈_ R [3], measured
and expressed w.r.t _{B}_ . Let the position _p ∈_ R [3] and linear
velocity _v ∈_ R [3] of the vehicle be expressed in _{W}_, while
the vehicle’s orientation in terms of quaternion _q ∈_ S [3] be
expressed in _{B}_ . The system kinematics in continuous space
can then be expressed as [1], [5]:
 _q_ ˙ = 2 [1] [Γ(] _[ω]_ [)] _[q][ ∈]_ [S] [3]


˙

 _p_ = _v ∈_ R [3] (26)







˙
_q_ = [1]



2 [Γ(] _[ω]_ [)] _[q][ ∈]_ [S] [3]



where _n_ _ω,k_ _∈_ R [3], _n_ _a,k_ _∈_ R [3], _n_ _bω,k_ _∈_ R [3], and _n_ _ba,k_ _∈_ R [3]

are noise vectors from zero mean Gaussian distributions and
covariance matrices of _C_ _ω,k_, _C_ _a,k_, _C_ _bω,k_, and _C_ _ba,k_, respectively. From (28) and (29), let us define the following state

vector:


_x_ _k_ = � _q_ _k⊤_ _p_ _[⊤]_ _k_ _v_ _k_ _[⊤]_ _b_ _[⊤]_ _ω,k_ _b_ _[⊤]_ _a,k_ � _⊤_ _∈_ R _m_ _x_ (30)


with _m_ _x_ = 16 representing the state dimension. Let the
augmented and additive noise vectors with dimensions of
_m_ _n_ _x_ = 6 and _m_ _n_ _w_ = 16 be defined as:
_n_ _x,k_ = � _n_ _⊤ω,k_ _n_ _[⊤]_ _a,k_ � _⊤_ _∈_ R _m_ _nx_
� _n_ _w,k_ = �0 _⊤_ 10 _×_ 1 _n_ _[⊤]_ _bω,k_ _n_ _[⊤]_ _ba,k_ � _⊤_ _∈_ R _m_ _nw_ (31)


Let the input vector _u_ _k_ at time step _k_ be defined as:

_u_ _k_ = � _ω_ _m,k⊤_ _a_ _[⊤]_ _m,k_ � _⊤_ _∈_ R _m_ _u_ (32)


where _m_ _u_ = 6. From (30) and (31), let us define the
augmented state vector as:

_x_ _[a]_ _k_ [=] � _x_ _⊤k_ _n_ _[⊤]_ _x,k_ � _⊤_ _∈_ R _m_ _a_ (33)


with _m_ _a_ = _m_ _x_ + _m_ _n_ _x_ . Combining (28), (29), (31), (32),
and (33), one can re-express the system kinematics in discrete
space as follows:


_x_ _k_ = f( _x_ _[a]_ _k−_ 1 _[, u]_ _[k][−]_ [1] [) +] _[ n]_ _[w,k][−]_ [1] (34)


where f( _·_ ) : R _[m]_ _[a]_ _×_ R _[m]_ _[u]_ _→_ R _[m]_ _[x]_ denotes the state transition
matrix.


_B. VIN Measurement Model_


Consider _f_ _w,i_ _∈_ R [3] to be the coordinates of the _i_ th feature
point in _{W}_ obtained by using a series of stereo camera
observations and _f_ _b,i_ _∈_ R [3] be the coordinates of the _i_ th
feature point in _{B}_ reconstructed using the stereo camera



˙
_p_ = _v ∈_ R [3]



(26)







˙
_v_ = _g_ + _R_ _q_ ( _q_ ) _a ∈_ R [3]



where



0 _−ω_ _[⊤]_
Γ( _ω_ ) = � _ω_ _−_ [ _ω_ ] _×_



_∈_ R [4] _[×]_ [4]
�



The kinematics expression in (26) can be re-arranged in a
geometric form in a similar manner to geometric navigation
in [1], [21] as follows:

_q_ ˙˙ 12 [Γ(] _[ω]_ [)] _[q]_ 0 0 0 _q_

     










1 0 0 0
2 [Γ(] _[ω]_ [)] _[q]_
0 0 _I_ 3 0
0 0 0 _g_ + _R_ _q_ ( _q_ ) _a_
0 0 0 0













=








 (27)







_q_ ˙
_p_ ˙
_v_ ˙

0










_q_

_p_

_v_

1



~~�~~ � ~~�~~ ~~�~~
_M_ _[c]_ ( _q,ω,a_ )


As the sensors operate in discrete space, the expression in
(27) is discretized for filter derivation and implementation.


Fig. 1: Navigation problem visualization in the 6 DoF.


measurement at the _k_ th time step. The model describing the
relationship between each _f_ _w,i_ and _f_ _b,i_ can be expressed as

[5], [24]:


_f_ _b,i_ = _R_ _q_ ( _q_ _k_ ) _[⊤]_ ( _f_ _w,i_ _−_ _p_ _k_ ) + _n_ _f,i_ _∈_ R [3] (35)


where _n_ _f,i_ refers to white Gaussian noise associated with each
measurement for all _i ∈{_ 1 _,_ 2 _, . . ., m_ _f_ _}_ and _m_ _f_ _∈_ R denotes
the number of feature points detected in each measurement
step. Note that the quantity of feature points _m_ _f_ may vary
across images obtained at different measurement steps. Let us
define the set of feature points in _{W}_ as _f_ _w_, the set of feature
points in _{B}_ as _f_ _b_, and the measurement noise vector as _n_ _f_
such that
 _f_ _b_ = � _f_ _b,_ _[⊤]_ 1 _f_ _b,_ _[⊤]_ 2 _· · ·_ _f_ _b,m_ _[⊤]_ _f_ � _⊤_ _∈_ R [3] _[m]_ _[f]_
 _f_ _w_ = � _f_ _w, ⊤_ 1 _f_ _w,_ _[⊤]_ 2 _· · ·_ _f_ _w,m_ _[⊤]_ _f_ � _⊤_ _∈_ R 3 _m_ _f_ (36)



_⊤_
_f_ _b_ = � _f_ _b,_ _[⊤]_ 1 _f_ _b,_ _[⊤]_ 2 _· · ·_ _f_ _b,m_ _[⊤]_ _f_ � _∈_ R [3] _[m]_ _[f]_



_f_ _w_ = � _f_ _w, ⊤_ 1 _f_ _w,_ _[⊤]_ 2 _· · ·_ _f_ _w,m_ _[⊤]_ _f_ � _⊤_ _∈_ R 3 _m_ _f_


_⊤_
_n_ _f_ = � _n_ _[⊤]_ _f,_ 1 _n_ _[⊤]_ _f,_ 2 _· · ·_ _n_ _[⊤]_ _f,m_ _f_ � _∈_ R [3] _[m]_ _[f]_



(36)



6


values of _x_ _k_ and _z_ _k_, respectively. In this model, the expected
values of the RVs are accounted as the real values. However,
it should be noted that directly calculating these expected
values is not feasible; instead, they are estimated. Let the
estimated expected values of the probabilities P( _X_ _k−_ 1 _|Z_ _k−_ 1 )
and P( _X_ _k_ _|Z_ _k−_ 1 ) be ˆ _x_ _k−_ 1 _|k−_ 1 and ˆ _x_ _k|k−_ 1, respectively, and
the expected value of P( _Z_ _k_ ) be ˆ _z_ _k|k−_ 1 .


_A. Filter Initialization_


_Step 1. State and Covariance Initialization:_ At the QNUKF
initialization, we establish the initial state estimate ˆ _x_ 0 _|_ 0
and the covariance matrix _P_ 0 _|_ 0 . While this step is standard
in CUKF, managing quaternion-based navigation introduces
unique challenges related to quaternion representation and
covariance. A critical challenge arises from the fact that the
covariance is computed based on differences from the mean.
Thereby, straightforward quaternion subtraction is not feasible.
To address this limitation, we employ the custom quaternion
subtraction introduced in (17) and specifically designed for
quaternion operations. Moreover, quaternions inherently possess three degrees of freedom while having four components,
requiring adjustments to the covariance matrix to reflect the
reduced dimensionality. Let the initial estimated state vector be

_⊤_
defined as _x_ ˆ 0 _|_ 0 _,q_ _∈_ S ˆ _x_ [3] 0 and _|_ 0 = ˆ _x_ � 0 _x_ ˆ _|_ 0 _[⊤]_ 0 _,|−_ 0 _,q_ _[,]_ _∈_ [ ˆ] _[x]_ 0 _[⊤]_ R _|_ 0 _[m]_ _,−_ _[x]_ � _[−]_ [4] _∈_ represent the quaternionR _[m]_ _[x]_ (see (30)), where
and non-quaternion components of ˆ _x_ 0 _|_ 0, respectively, with _m_ _x_
being the number of rows in the state vector. The filter is
initialized as:


_⊤_
_x_ ˆ 0 _|_ 0 = � _x_ ˆ _[⊤]_ 0 _|_ 0 _,q_ _[,]_ [ ˆ] _[x]_ 0 _[⊤]_ _|_ 0 _,−_ � _∈_ R _[m]_ _[x]_ (38)

_P_ 0 _|_ 0 = diag( _P_ _x_ ˆ 0 _|_ 0 _,q_ _, P_ 0 _|_ 0 _,−_ ) _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [1)] (39)


where _P_ 0 _|_ 0 _,q_ _∈_ R [3] _[×]_ [3], _P_ 0 _|_ 0 _,−_ _∈_ R [(] _[m]_ _[x]_ _[−]_ [4)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [4)], and _P_ 0 _|_ 0
represent the covariance matrices corresponding with the uncertainties of ˆ _x_ 0 _|_ 0 _,q_, ˆ _x_ 0 _|_ 0 _,−_, and ˆ _x_ 0 _|_ 0, respectively.


_B. Prediction_


The prediction step involves estimating the state of the
system at the next time step leveraging the system’s kinematics
given the initial state (38) and covariance (39). At each _k_
time step, the process begins with augmenting the state vector
with non-additive noise terms, a critical step that ensures the
filter accounts for the uncertainty which these noise terms
introduce. Subsequently, sigma points are generated from the
augmented state vector and covariance matrix. These sigma
points effectively represent the distribution P( _X_ _k−_ 1 _|Z_ _k−_ 1 ),
capturing the state’s uncertainty at the previous time step
given the previous observation, see (22). (20). The next phase
involves computing the posterior sigma points by applying
the state transition function to each sigma point, thereby
representing the distribution P( _X_ _k_ _|X_ _k−_ 1 ) which is equivalent
to P( _X_ _k_ _|Z_ _k−_ 1 ). This distribution reflects the predicted state’s
uncertainty before the current observation is considered. The
predicted state vector is then obtained as the weighted average
of these posterior sigma points. This process is described in
detail in this subsection.







Using (36), the term in (35) can be rewritten in form of a
measurement function as follows:


_f_ _b,k_ = _z_ _k_ = h( _x_ _k_ _, f_ _w_ ) + _n_ _f,k_ _∈_ R _[m]_ _[z]_ (37)


with
_n_ _f_ _∼_ _N_ (0 _, C_ _f_ _∈_ R _[m]_ _[z]_ _[×][m]_ _[z]_ )


Note that _z_ _k_ is the measurement vector at the _k_ th time step and
_m_ _z_ = 3 _m_ _f_ _∈_ Z [+] is the dimension of the measurement vector.
Typically, the camera sensor operates at a lower sampling
frequency when compared to the IMU sensor. As a result,
there may be a timing discrepancy between the availability
of images and the corresponding IMU data. The conceptual
illustration of the VIN problem is presented in Fig. 1.


IV. QNUKF- BASED VIN D ESIGN


To develop the QNUKF, we will explore how the Conventional UKF (CUKF) [33] functions within the system. Next,
we will modify each step to address the CUKF limitations.
Stochastic filters, such as UKF models system parameters as
RVs and aim to estimate the expected values of their distributions, which are then used as the estimates of the parameters.
Let _X_ _k_ and _Z_ _k_ represent the RVs associated with the real


_Step 2. Augmentation:_ Prior to calculating the sigma points,
the state vector and covariance matrix are augmented to
incorporate non-additive process noise. This involves adding
rows and columns to the state vector and covariance matrix to

represent the process noise. The expected value of the noise
terms, zero in this case, are augmented to the previous state estimate ˆ _x_ _k−_ 1 _|k−_ 1 to form the augmented state vector ˆ _x_ _[a]_ _k−_ 1 _|k−_ 1 [.]
Similarly, the last estimated state covariance matrix _P_ _k−_ 1 _|k−_ 1
and the noise covariance matrix _C_ _x,k_ are augmented together
to form the augmented state covariance matrix _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 [. This]
is formulated as follows:


_⊤_
_x_ ˆ _[a]_ _k−_ 1 _|k−_ 1 [=] � _x_ ˆ _[⊤]_ _k−_ 1 _|k−_ 1 _[,]_ [ 0] _m_ _[⊤]_ _nx_ _×_ 1 � _∈_ R _[m]_ _[a]_ (40)

_P_ _k_ _[a]_ _−_ 1 _|k−_ 1 [=][ diag][(] _[P]_ _[k][−]_ [1] _[|][k][−]_ [1] _[, C]_ _[x,k]_ [)] _[ ∈]_ [R] [(] _[m]_ _[a]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[a]_ _[−]_ [1)] [ (41)]


The covariance matrix _C_ _x,k_ in (41) is defined by:


_C_ _x,k_ = diag( _C_ _ω,k_ _, C_ _a,k_ ) _∈_ R _[m]_ _[nx]_ _[×][m]_ _[nx]_ (42)



7


to the reduced dimensionality of the quaternion. To resolve this
limitation, the propose QNUKF modifies _δx_ ˆ _[a]_ _k−_ 1 _,j_ [as follows:]



_δx_ ˆ _[a]_ _k−_ 1 _,j_ [:=] ��



( _m_ _a_ _−_ 1 + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 �



_j_ _[∈]_ [R] _[m]_ _[a]_ _[−]_ [1] (45)



Overall, considering (44) and (45), the QNUKF utilizes the
modified version of (43) as follows:
 _XX_ _kk_ _[a][a]_ _−−_ 11 _||kk−−_ 11 _,,j_ 0 [= ˆ] := [= ˆ] _[x][x]_ � _[a]_ _k_ _[a]_ _k_ _x_ ˆ _−−_ _x_ ˆ _[a]_ _k_ 11 _[a]_ _k−||−kk_ 1 _−−_ 1 _||k_ 11 _k−−_ _[⊕][∈]_ 11 _,,q−_ _[δ]_ [R] _[x]_ [ˆ] _[m]_ _[⊕]_ [+] _[a]_ _j_ _[a]_ _[ δ][δ][x][x]_ [ˆ][ˆ] _k_ _[a][a]_ _k−−_ 11 _,j,r,j,−_ � _∈_ R _[m]_ _[a]_
 _X_ _[a]_ [= ˆ] _[a]_ _[δ]_ [ˆ] _[a]_



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,_ 0 [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 _[∈]_ [R] _[m]_ _[a]_



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 _[⊕]_ _[δ][x]_ [ˆ] _[a]_ _j_



�



:=



� _x_ ˆ _x_ ˆ _[a]_ _k_ _[a]_ _k−−_ 11 _||kk−−_ 11 _,,q−_ _[⊕]_ [+] _[ δ][δ][x][x]_ [ˆ][ˆ] _k_ _[a][a]_ _k−−_ 11 _,j,r,j,−_



_∈_ R _[m]_ _[a]_



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ + _m_ _a_ [= ˆ] _[x]_ _k_ _[a]_ _−_ 1 _|k−_ 1 _[⊖]_ _[δ][x]_ [ˆ] _[a]_ _j_







�



:=



� _x_ ˆ _x_ ˆ _[a]_ _k_ _[a]_ _k−−_ 11 _||kk−−_ 11 _,,q−_ _[⊖][−]_ _[δ][δ][x][x]_ [ˆ][ˆ] _k_ _[a][a]_ _k−−_ 11 _,j,r,j,−_



_∈_ R _[m]_ _[a]_ _,_



_Step 3. Sigma Points Calculation:_ Conventionally, sigma
points are computed based on the augmented state estimate
and covariance matrix. They serve as a representation for
the distribution and uncertainty of P( _x_ _k−_ 1 _|k−_ 1 _|z_ _k−_ 1 ). In other
words, the greater the uncertainty of the last estimate reflected
by a larger last estimated covariance, the more widespread
the sigma points will be around the last estimated state.
Reformulating (22) according to the CUKF algorithm, the
sigma points, crucial for propagating the state through the
system dynamics, are computed using the following equation:
 _XX_ _kk_ _[a][a]_ _−−_ 11 _||kk−−_ 11 _,,j_ 0 [= ˆ][= ˆ] _[x][x]_ _[a]_ _k_ _[a]_ _k−−_ 11 _||kk−−_ 11 [+] _[∈]_ [R] �� _[m]_ _[a]_ ( _m_ _a_ + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 � _j_

_[a]_ _[a]_



 _j_ = _{_ 1 _,_ 2 _, . . .,_ 2( _m_ _a_ _−_ 1) _}_

(46)
_Step_ _4._ _Propagate_ _Sigma_ _Points:_ The sigma points
_{X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ _[}]_ [ are propagated through the state transition func-]
tion, as defined in (34), to calculate the propagated sigma
points _{X_ _k|k−_ 1 _,j_ _}_ . These points represent the probability distribution P( _X_ _k_ _|X_ _k−_ 1 ), and are computed as follows:


_X_ _k|k−_ 1 _,j_ = f( _X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ _[, u]_ _[k][−]_ [1] [)] _[ ∈]_ [R] _[m]_ _[x]_ (47)


for all _j_ = _{_ 0 _,_ 1 _,_ 2 _, . . .,_ 2( _m_ _a_ _−_ 1) _}_ .
_Step 5. Compute Predicted Mean and Covariance:_ The predicted mean ˆ _x_ _k|k−_ 1 and covariance _P_ _k|k−_ 1 are conventionally
computed using the predicted sigma points by reformulating
(24) and (25) as:



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,_ 0 [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 _[∈]_ [R] _[m]_ _[a]_

_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 [+] ��



_j_



_j_ _[,]_



2 _m_ _a_
� _w_ _j_ _[m]_ _[X]_ _k|k−_ 1 _,j_ _[∈]_ [R] _[m]_ _[x]_ (48)

_j_ =0



2 _m_ _a_
�

_j_ =0



� _w_ _j_ _[c]_ [(] _[X]_ _k|k−_ 1 _,j_ _[−]_ _[x]_ [ˆ] _k|k−_ 1 [)(] _[X]_ _k|k−_ 1 _,j_ _[−]_ _[x]_ [ˆ] _k|k−_ 1 [)] _[⊤]_ [�]



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ + _m_ _a_ [= ˆ] _[x]_ _k_ _[a]_ _−_ 1 _|k−_ 1 _[−]_ ��







( _m_ _a_ + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 �

( _m_ _a_ + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 �



 _j_ = _{_ 1 _,_ 2 _, . . .,_ 2 _m_ _a_ _}_

(43)
Challenges arise when using the sigma points calculation as described in (43) for quaternion-based navigation
problems due to the differing mathematical spaces involved. For the sake of brevity, let us define _δx_ ˆ _[a]_ _k−_ 1 _,j_ [:=]

_x_ � _xδ_ ˆˆ _x_ ˆ _k_ _[a]_ _k_ ~~�~~ _−−_ _[a]_ _k−_ ( 11 _m_ _||_ 1 _kk,j,−−a_ + _−_ 11 _,q_ [and] _[∈]_ _∈ λ_ ) [R] _[ δ]_ _P_ S _[m]_ _k_ _[x]_ [ˆ] [3] _[a]_ _−_ _[a]_, _k_ _[a]_ _δ_ _[−]_ _−_ 1 _|_ [4] 1 _x_ _k_ ˆ [, as outlined below:] _,j−_ _[a]_ _k−_ 1 [into their attitude and non-attitude parts:] � 1 _,j,rj_ _[∈][∈]_ [R][R] _[m]_ [3] _[a]_ [, and] _[−]_ [1] [. It is possible to divide][ ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 _,−_ _[∈]_ [R] _[m]_ _[a]_ _[−]_ [4] [,]

ˆ

_x_ _[a]_ _k−_ 1 _|k−_ 1 [=] �(ˆ _x_ _[a]_ _k−_ 1 _|k−_ 1 _,q_ [)] _[⊤]_ _[,]_ [ (ˆ] _[x]_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,−_ [)] _[⊤]_ [�] _[⊤]_

(44)



_[⊤]_



_x_ ˆ _k|k−_ 1 =


_P_ _k|k−_ 1 =



( _m_ _a_ + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 �



_j_ _[∈]_ [R] _[m]_ _[a]_ _[−]_ [1] [. It is possible to divide]



+ _C_ _w,k_ _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [1)] (49)


where _w_ _j_ _[m]_ and _w_ _j_ _[c]_ _[∈]_ [R][ represent the weights associated]
with the sigma points and are found by (23). Additionally,
_C_ _w,k_ _∈_ R _[m]_ _[x]_ _[−]_ [1] denotes the process noise covariance matrix,
which is the covariance matrix of the noise vector _n_ _w,k_ as
defined in (31). Note that the reduced dimensionality of _C_ _w,k_,
similar to _P_ _k|k−_ 1 and _P_ 0 _|_ 0, is due to the covariance matrices of
quaternion random variables being described by 3 _×_ 3 matrices,
reflecting the three degrees of freedom inherent to quaternions.
_C_ _w,k_ is defined as



ˆ

_x_ _[a]_ _k−_ 1 _|k−_ 1 [=] �(ˆ _x_ _[a]_ _k−_ 1 _|k−_ 1 _,q_ [)] _[⊤]_ _[,]_ [ (ˆ] _[x]_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,−_ [)] _[⊤]_ [�] _[⊤]_

 _δx_ ˆ _[a]_ _k−_ 1 [=] �( _δx_ ˆ _[a]_ _k−_ 1 [)] _[⊤]_ _[,]_ [ (] _[δ][x]_ [ˆ] _k_ _[a]_ _−_ 1 _−_ [)] _[⊤]_ [�] _[⊤]_



_,_ (44)

_δx_ ˆ _[a]_ _k−_ 1 _,j_ [=] �( _δx_ ˆ _[a]_ _k−_ 1 _,j,r_ [)] _[⊤]_ _[,]_ [ (] _[δ][x]_ [ˆ] _k_ _[a]_ _−_ 1 _,j,−_ [)] _[⊤]_ [�] _[⊤]_



 _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [1)] (50)





0 9 _×_ 9 0 3 _×_ 3 0 3 _×_ 3

0 9 _×_ 9 _C_ _bω,k_ 0 3 _×_ 3

0 9 _×_ 9 0 3 _×_ 3 _C_ _ba,k_



From ( _x_ ˆ _[a]_ _k−_ 1 _|k−_ 44 1 _,q_ ), given that [is a quaternion expression in] _δx_ ˆ _[a]_ _k−_ 1 _,j,r_ [is a rotation vector in][ S] [3] [, direct addition][ R] [3] [ and]
or subtraction is not feasible. To resolve this limitation, we
employ the custom subtraction (16) and addition (15) operators, as previously defined. The operation map S [3] _×_ R [3] _→_ S [3]

effectively manages the interaction between quaternions and
rotation vectors. Additionally, the quaternion-based navigation
problem inherently possesses _m_ _a_ _−_ 1 degrees of freedom due



_C_ _w,k_ =



To tailor (48), (49), and (23) for the quaternion-based navigation problem, the following modifications are necessary:
(1) The QNUKF utilizes _m_ _a_ _−_ 1 sigma points, therefore, all
occurrences of _m_ _a_ in (48), (49), and (23) should be replaced
with _m_ _a_ _−_ 1; (2) The average of quaternion components of the
sigma points in (48) should be computed using the quaternion


8





























Fig. 2: Schematic diagram of the proposed QNUKF. First, the sigma points based on the last estimated state vector and its
covariance matrix are calculated. Using these sigma points and IMU measurements, the prediction step is performed. If camera
data exists at that sample point, the correction (also known as update) step is performed. The state and covariance estimation
at that time step is the result of the correction step if camera data exists, and the prediction step if it does not.



weighted average method described in (18). This is necessary
since the typical vector averaging method in (48) cannot be
applied directly to quaternions; (3) The subtraction of quaternion components of the sigma point vector by the quaternion
components of the estimated mean in (49) should utilize the
custom subtraction method defined in (17). This operation
maps S [3] _×_ S [3] to R [3], reflecting the fact that quaternions, which
exist in S [3] space, cannot be subtracted using conventional
vector subtraction techniques.


To facilitate these operations, let us divide ˆ _x_ _k|k−_ 1 and
_X_ _k|k−_ 1 _,j_ to their quaternion ( _x_ ˆ _k|k−_ 1 _,q_ _∈_ S [3], _X_ _k|k−_ 1 _,j,q_ _∈_
S [3] ) and non-quaternion ( _x_ ˆ _k|k−_ 1 _,−_ _∈_ R _[n]_ _[x]_ _[−]_ [4], _X_ _k|k−_ 1 _,j,−_ _∈_
R _[m]_ _[x]_ _[−]_ [4] ) components. These divisions can be represented as
follows:


_x_ ˆ _k|k−_ 1 = � _x_ ˆ _[⊤]_ _k|k−_ 1 _,q_ _x_ ˆ _[⊤]_ _k|k−_ 1 _,−_ � _⊤_ R _[m]_ _[x]_ (51)


_⊤_
_X_ _k|k−_ 1 _,j_ = � _X_ _k_ _[⊤]_ _|k−_ 1 _,q_ _X_ _k_ _[⊤]_ _|k−_ 1 _,−_ � _∈_ R _[m]_ _[x]_ (52)


Consequently, equations (48), (49), and (23) can be reformulated to accommodate these quaternion-specific processes,
utilizing the divisions formulated in (51) and (52) as follows:



and




_λ_
_w_ 0 _[m]_ [=]
_λ_ + ( _m_ _a_ _−_ 1) _[∈]_ [R]



_λ_
_w_ 0 _[c]_ [=]
_λ_ + ( _m_ _a_ _−_ 1) [+ 1] _[ −]_ _[α]_ [2] [ +] _[ β][ ∈]_ [R]



(55)



1
_w_ _j_ _[m]_ [=] _[ w]_ _j_ _[c]_ [=] 2(( _m_ _a_ _−_ 1) + _λ_ ) _[∈]_ [R]







_j_ = _{_ 1 _, . . .,_ 2( _m_ _a_ _−_ 1) _}_



where

_X_ _k|k−_ 1 _,j_ _⊖_ _x_ ˆ _k|k−_ 1 = � _XX_ _kk||kk−−_ 11 _,j,,j,q−_ _⊖−_ _xx_ ˆˆ _kk||kk−−_ 11 _,q,−_


_C. Update_



_∈_ R _[m]_ _[x]_ _[−]_ [1]
�

(56)



� _w_ _j_ _[m]_ _[X]_ _k|k−_ 1 _,j,−_

_j_ =0







In the update step, the propagated sigma points are first
processed through the measurement function to generate
the measurement sigma points, which represent P( _Z_ _k_ _|X_ _k_ ).
Subsequently, utilizing P( _X_ _k−_ 1 _|Z_ _k−_ 1 ), P( _X_ _k_ _|X_ _k−_ 1 ), and
P( _Z_ _k_ _|X_ _k_ ), the conditional probability P( _X_ _k_ _|Z_ _k_ ) is estimated.
The expected value derived from P( _X_ _k_ _|Z_ _k_ ) will serve as the
state estimate for the _k_ th step.
_Step 6. Predict Measurement and Calculate Covariance:_ In
this step, each propagated sigma point _X_ _k|k−_ 1 _,j_, as found in
(47), is processed through the measurement function defined
in (37) to compute the _j_ th predicted sigma point _Z_ _k|k−_ 1 _,j_ .
The set _{Z_ _k|k−_ 1 _,j_ _}_ represent the probability distribution
P( _Z_ _k_ _|X_ _k_ ). For all _j_ = _{_ 0 _,_ 1 _, . . .,_ 2( _m_ _a_ _−_ 1) _}_, the computation
for each sigma point is given by:


_Z_ _k|k−_ 1 _,j_ = _h_ ( _X_ _k|k−_ 1 _,j_ _, f_ _w_ ) _∈_ R _[m]_ _[z]_ (57)


The expected value of P( _Z_ _k_ _|X_ _k_ ) is the maximum likelihood
estimate of the measurement vector. This estimate ˆ _z_ _k|k−_ 1
is calculated according to (58). Additionally, the estimated
measurement covariance matrix _P_ _z_ _k_ _,z_ _k_ _∈_ R _[m]_ _[z]_ _[×][m]_ _[z]_ and the



_x_ ˆ _k|k−_ 1 =









QWA( _{X_ _k|k−_ 1 _,j,q_ _}, {w_ _j_ _[m]_ _[}]_ [)]
2( _m_ _a_ _−_ 1)
� _w_ _j_ _[m]_ _[X]_ _k|k−_ 1 _,j,−_



 _∈_ R _m_ _x_ (53)



_P_ _k|k−_ 1 =



2( _m_ _a_ _−_ 1)
�

_j_ =0



� _w_ _j_ _[c]_ [(] _[X]_ _k|k−_ 1 _,j_ _[⊖]_ _[x]_ [ˆ] _k|k−_ 1 [)(] _[X]_ _k|k−_ 1 _,j_ _[⊖]_ _[x]_ [ˆ] _k|k−_ 1 [)] _[⊤]_ [�]



+ _C_ _w,k_ _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [1)] (54)


state-measurement covariance matrix _P_ _x_ _k_ _,z_ _k_ _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×][m]_ _[z]_
are computed using the following equations:



9


vector in the three-dimensional Euclidean space R [3] . To add the
orientation parts of the predicted state and correction vectors,
the custom summation operator S [3] _×_ R [3] _→_ S [3] described in
(15) should be used as follows


_x_ ˆ _k|k_ = ˆ _x_ _k|k−_ 1 _⊕_ _δx_ ˆ _k|k−_ 1 (66)


where



_z_ ˆ _k|k−_ 1 =


_P_ _z_ _k_ _,z_ _k_ =


_P_ _x_ _k_ _,z_ _k_ =



2( _m_ _a_ _−_ 1)
� _w_ _j_ _[m]_ _[Z]_ _k|k−_ 1 _,j_ (58)

_j_ =0


2( _m_ _a_ _−_ 1)
� _w_ _j_ _[c]_ [[] _[Z]_ _k|k−_ 1 _,j_ _[−]_ _[z]_ [ˆ] _k|k−_ 1 [][] _[Z]_ _k|k−_ 1 _,j_ _[−]_ _[z]_ [ˆ] _k|k−_ 1 []] _[⊤]_

_j_ =0


+ _C_ _f_ (59)


2( _m_ _a_ _−_ 1)
� _w_ _j_ _[c]_ [[] _[X]_ _k|k−_ 1 _,j_ _[⊖]_ _[x]_ [ˆ] _k|k−_ 1 [][] _[Z]_ _k|k−_ 1 _,j_ _[−]_ _[z]_ [ˆ] _k|k−_ 1 []] _[⊤]_

_j_ =0

(60)



_x_ ˆ _k|k−_ 1 _⊕_ _δx_ ˆ _k|k−_ 1 = � _x_ ˆ _x_ ˆ _kk||kk−−_ 11 _,,q−_ _⊕_ + _δδxx_ ˆˆ _kk||kk−−_ 11 _,r,−_



(67)
�



Note that the subtraction operator _⊖_ in (60) denotes state difference operator and follows (56). Now, the joint distribution
of ( _X_ _k_ _, Z_ _k_ ) is estimated as follows:

_X_ _k_ _x_ ˆ _k|k−_ 1 _P_ _k|k−_ 1 _P_ _x_ _k_ _,z_ _k_
� _Z_ _k_ � _∼N_ �� _z_ ˆ _k|k−_ 1 � _,_ � _P_ _x_ _[⊤]_ _k_ _,z_ _k_ _P_ _z_ _k_ _,z_ _k_ �� (61)


These covariance calculations are necessary to estimate the
probability distribution P( _X_ _k_ _|Z_ _k_ ) in the next step.
_Step 7. Update State Estimate:_ In view of (61) to (19), let us
reformulate (20) to find the mean and covariance estimate of
the P( _X_ _k_ _|Z_ _k_ ) distribution. It is worth noting that the estimated
mean is the estimated state vector at this time step. This is
achieved by computing the Kalman gain _K_ _k_ _∈_ R [(] _[m]_ _[x]_ _[−]_ [1)] _[×][m]_ _[z]_
given the matrices found in (59) and (60).


_K_ _k_ = _P_ _x_ _k_ _,z_ _k_ _P_ _z_ _[⊤]_ _k_ _,z_ _k_ _[∈]_ [R] [(] _[m]_ _[x]_ _[−]_ [1)] _[×][m]_ _[z]_ (62)


To find the updated covariance estimate of the state distribution
_P_ _k|k_ in the QNUKF, the conventional formula provided in (63)
is utilized:


_P_ _k|k_ = _P_ _k|k−_ 1 _−_ _K_ _k_ _P_ _z_ _k_ _,z_ _k_ _K_ _k_ _[⊤]_ (63)


However, computing the updated estimated state vector ˆ _x_ _k|k_
requires careful consideration. Define the correction vector
_δx_ ˆ _k|k−_ 1 as:


_δx_ ˆ _k|k−_ 1 := _K_ _k_ ( _z_ _k_ _−_ _z_ ˆ _k|k−_ 1 ) _∈_ R _[m]_ _[x]_ _[−]_ [1] (64)


Recall that _z_ _k_ refers to camera feature measurements in _{B}_ .
Conventionally, the correction vector calculated in (64), is
added to the predicted state vector from (53) to update the
state. However, for a quaternion-based navigation problem,
the dimensions of _δx_ ˆ _k|k−_ 1 and ˆ _x_ _k|k−_ 1 do not match, with
the former being _m_ _x_ _−_ 1 and the latter _m_ _x_ . To address this,
let us divide _δx_ ˆ _k|k−_ 1 into its attitude ( _δx_ ˆ _k|k−_ 1 _,r_ _∈_ R [3] ) and
non-attitude ( _δx_ ˆ _k|k−_ 1 _,−_ _∈_ R _[m]_ _[⋆]_ _[−]_ [4] ) components:

_δx_ ˆ _k|k−_ 1 = � _δx_ ˆ _[⊤]_ _k|k−_ 1 _,r_ _δx_ ˆ _[⊤]_ _k|k−_ 1 _,−_ � _⊤_ (65)


From (65) with (51), we observe that the non-quaternion components of ˆ _x_ _k|k−_ 1 and _δx_ ˆ _k|k−_ 1 are dimensionally consistent
and can be combined using the conventional addition operator.
However, the orientation components of ˆ _x_ _k|k−_ 1 and _δx_ ˆ _k|k−_ 1
do not match dimensionally. The former is represented by a
quaternion within the S [3] space, while the latter is a rotation



��



_∼N_



_x_ ˆ _k|k−_ 1
�� _z_ ˆ _k|k−_ 1



_Step 8. Iterate:_ Go back to Step 2. and iterate from _k →_
_k_ + 1.
The proposed QNUKF algorithm is visually depicted in
Fig. 2. The implementation steps of QNUKF algorithm is
summarized and outlined in Algorithm 1.


**Remark 1.** _The time complexities of the vanilla EKF and UKF_
_algorithms are O_ ( _m_ [3] _x_ [)] _[ and][ O]_ [(] _[m]_ _a_ [3] [)] _[, respectively [][39][]. For nav-]_
_igation tasks, these complexities reduce to O_ (( _m_ _x_ _−_ 1) [3] ) _and_
_O_ (( _m_ _a_ _−_ 1) [3] ) _, respectively, due to the reduced dimensionality_
_of the quaternion in the state vector. The custom operations_
_proposed in QNUKF, such as the one used in_ (67) _, all have_
_complexities less than O_ (( _m_ _a_ _−_ 1)) _and occur only once for_
_each sigma point. Therefore, QNUKF retains the same overall_
_complexity as any UKF, which is O_ (( _m_ _a_ _−_ 1) [3] ) _. The difference_
_between m_ _a_ _and m_ _x_ _arises because UKF can capture non-_
_additive noise terms by augmenting them into the state vector,_
_whereas EKF linearizes them into an additive form._


V. I MPLEMENTATION AND R ESULTS


To evaluate the effectiveness of the proposed QNUKF, the
algorithm is tested using the real-world EuRoC dataset of
drone flight in 6 DoF [40]. For video of the experiment, visit
[the following link. This dataset features an Asctec Firefly hex-](https://youtu.be/CP3xiOcGrTc)
rotor Micro Aerial Vehicle (MAV) flown in a static indoor environment, from which IMU, stereo images, and ground truth
data were collected. The stereo images are two simultaneous
monochrome images obtained via an Aptina MT9V034 global
shutter sensor with a 20 Hertz frequency. Linear acceleration
and angular velocity of the MAV were measured by an
ADIS16448 sensor with a 200 Hertz frequency. Note that the
data sampling frequency of the camera and IMU are different.
Consequently, the measurement data does not necessarily exist
at each IMU data sample point. To incorporate this, the system
only performs the update step when image data is retrieved,
and sets ˆ _x_ _k|k_ equal to the predicted estimated state vector
_x_ ˆ _k|k−_ 1 when image data is unavailable.


_A. The Proposed QNUKF Output Performance_


The feature point count _m_ _f_ is not constant at each measurement step, to compensate for this, we set the measurement
noise covariance to:


_C_ _f_ = _c_ [2] _f_ _[I]_ _[m]_ _f_ (68)


where _m_ _f_ is based on the number of feature points at each
measurement step and _c_ _f_ is a scaler tuned beforehand. To
ensure the covariance matrices remain symmetric and avoid



_P_ _k|k−_ 1 _P_ _x_ _k_ _,z_ _k_
� _P_ _x_ _[⊤]_ _k_ _,z_ _k_ _P_ _z_ _k_ _,z_ _k_



(61)



�



_,_



_Z_ _k_



�



_z_ ˆ _k|k−_ 1


**Algorithm 1** Quaternion Navigation Unscented Kalman Filter

**Initialization** :

1: Set _x_ ˆ 0 _|_ 0 _∈_ R _[m]_ _[x]_ _[−]_ [4], see (30), and _P_ 0 _|_ 0 _∈_
R [(] _[m]_ _[x]_ _[−]_ [1)] _[×]_ [(] _[m]_ _[x]_ _[−]_ [1)] .

2: Set _k_ = 0 and select the filter design parameters _λ_,
_α_, _β ∈_ R, along with the noise covariance matrices
_C_ _a,k_, _C_ _ω,k_, _C_ _ba,k_, _C_ _bω,k_ _∈_ R [3] _[×]_ [3], and the noise variance
_c_ _f_ _∈_ R.
3: Calculate _C_ _x,k_, _C_ _w,k_, and _C_ _f_ as expressed in (42), (50),
and (68), respectively.

**while IMU data exists**


/* Prediction */



_⊤_

ˆ ˆ
_x_ _[a]_ _k−_ 1 _|k−_ 1 = � _x_ _[⊤]_ _k−_ 1 _|k−_ 1 _[,]_ [ 0] 6 _[⊤]_ _×_ 1 � _∈_ R [(] _[m]_ _[a]_ _[−]_ [1)] _[×]_ [1]



4:







_P_ _k_ _[a]_ _−_ 1 _|k−_ 1 =



_P_ _k−_ 1 _|k−_ 1 0 ( _m_ _x_ _−_ 1) _×m_ _nx_
�0 _m_ _nx_ _×_ ( _m_ _x_ _−_ 1) _C_ _x,k_ �



5: _X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,_ 0 [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1
6: **for** _j_ = _{_ 1 _,_ 2 _. . .,_ ( _m_ _a_ _−_ 1) _}_



_j_ [, see (][45][)]



_δx_ ˆ _[a]_ _k−_ 1 _,j_ [:=]
��



( _m_ _a_ _−_ 1 + _λ_ ) _P_ _k_ _[a]_ _−_ 1 _|k−_ 1 �



10


( _m_ _a_ _−_ 1), _α_ = 10 _[−]_ [4], and _β_ = 2. The IMU bias noise
covariances are determined by the standard deviations (std)
of the bias white noise terms, set to 0 _._ 01% of their average values to account for the slow random walk relative
to the IMU sampling rate. These values are configured as:
_C_ _ba,k_ = 10 _[−]_ [8] diag(0 _._ 0022 [2] _,_ 0 _._ 0208 [2] _,_ 0 _._ 0758 [2] ) ( _m/s_ [2] ) [2] and
_C_ _bω,k_ = 10 _[−]_ [8] diag(0 _._ 0147 [2] _,_ 0 _._ 1051 [2] _,_ 0 _._ 0930 [2] ) ( _rad/s_ ) [2] . The
std of the additive white noise terms are set to 1% of

the measured vectors (i.e., linear acceleration and angular
velocity) such that the covariance matrices are _C_ _ω,k_ =
10 _[−]_ [4] diag(0 _._ 1356 [2] _,_ 0 _._ 0386 [2] _,_ 0 _._ 0242 [2] ) ( _m/s_ [2] ) [2] and _C_ _a,k_ =
10 _[−]_ [4] diag(9 _._ 2501 [2] _,_ 0 _._ 0293 [2] _, −_ 3 _._ 3677 [2] ) ( _rad/s_ ) [2] . The measurement noise std is calculated using the measurement vectors
from the V1 01 hard dataset [40], recorded in the same
room as the validation V1 02 medium dataset [40], but with
different drone paths and complexity speeds. Its value is found
as _c_ _f_ = 0 _._ 099538 m. For each set of stereo images obtained,
feature points are identified using the Kanade-Lucas-Tomasi
(KLT) algorithm [41]. An example of the feature match results
from this operation is depicted in Fig. 3.


Fig. 3: Matched feature points from the left and right views
of the EuRoC dataset [40].


Using triangulation [42], the 2D matched points are projected to the 3D space, representing the feature points in _{W}_ .
Using the S [3] _×_ S [3] _→_ R [3] subtraction operator defined in (17),
let us define the orientation estimation error _r_ _e,k_ as:


_r_ _e,k_ := _q_ _k_ _⊖_ _q_ ˆ _k_ (70)


_⊤_
where _r_ _e,k_ = � _r_ _e_ 1 _,k_ _r_ _e_ 2 _,k_ _r_ _e_ 3 _,k_ � _∈_ R [3], with each
component _r_ _ei,k_ _∈_ R representing a dimension of the error
vector. Similarly, let us define the position and linear velocity
estimation errors at the _k_ th time step, represented by _p_ _e,k_ and
_v_ _e,k_, respectively, as:


_p_ _e,k_ := _p_ _k_ _−_ _p_ ˆ _k_ (71)


_v_ _e,k_ := _v_ _k_ _−_ _v_ ˆ _k_ (72)


where _p_ _e,k_ = � _p_ _e_ 1 _,k_ _p_ _e_ 2 _,k_ _p_ _e_ 3 _,k_ � _⊤_ _∈_ R 3 and _v_ _e,k_ =
� _v_ _e_ 1 _,k_ _v_ _e_ 2 _,k_ _v_ _e_ 3 _,k_ � _⊤_ _∈_ R 3 . The robustness and effectiveness of the proposed QNUKF have been confirmed through
experimental results on the EuRoC V1 02 medium dataset

[40] as depicted in Fig. 4 and with the initial estimates for the
state vector and covariance matrix being detailed as follows:



_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ [= ˆ] _[x]_ _[a]_ _k−_ 1 _|k−_ 1 _[⊕]_ _[δ][x]_ [ˆ] _[a]_ _j_ [, see (][46][)]
_X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ + _m_ _a_ [= ˆ] _[x]_ _k_ _[a]_ _−_ 1 _|k−_ 1 _[⊖]_ _[δ][x]_ [ˆ] _[a]_ _j_ [, see (][46][)]
**end for**
7: **for** _j_ = _{_ 1 _,_ 2 _. . .,_ ( _m_ _a_ _−_ 1) _}_



_X_ _k|k−_ 1 _,j_ = f( _X_ _k_ _[a]_ _−_ 1 _|k−_ 1 _,j_ _[, u]_ _[k][−]_ [1] [)]
**end for**, see (47)



� _w_ _j_ _[m]_ _[X]_ _k|k−_ 1 _,j,−_

_j_ =0







8: ˆ _x_ _k|k−_ 1 =









QWA( _{X_ _k|k−_ 1 _,j,q_ _}, {w_ _j_ _[m]_ _[}]_ [)]
2( _m_ _a_ _−_ 1)
� _w_ _j_ _[m]_ _[X]_ _k|k−_ 1 _,j,−_



, see (53)



9: _P_ _k|k−_ 1 = [�] [2(] _j_ =0 _[m]_ _[a]_ _[−]_ [1)] [ _w_ _j_ _[c]_ [(] _[X]_ _[k][|][k][−]_ [1] _[,j]_ _[⊖]_ _[x]_ [ˆ] _[k][|][k][−]_ [1] [)(] _[X]_ _[k][|][k][−]_ [1] _[,j]_ _[⊖]_
_x_ ˆ _k|k−_ 1 ) _[⊤]_ ] + _C_ _w,k_, see (54)
/* Update step */

10: **for** _j_ = _{_ 0 _,_ 1 _,_ 2 _. . .,_ ( _m_ _a_ _−_ 1) _}_

_Z_ _k|k−_ 1 _,j_ = _h_ ( _X_ _k_ _[a]_ _|k−_ 1 _,j_ _[, f]_ _[w]_ [)]
**end for**, see (57)

11: ˆ _z_ _k|k−_ 1 = [�] [2(] _j_ =0 _[m]_ _[a]_ _[−]_ [1)] _w_ _j_ _[m]_ _[Z]_ _[k][|][k][−]_ [1] _[,j]_ [, see (][58][)]

12: _P_ _z_ _k_ _,z_ _k_ = [�] [2(] _j_ =0 _[m]_ _[a]_ _[−]_ [1)] _w_ _j_ _[c]_ [[] _[Z]_ _[k][|][k][−]_ [1] _[,j]_ _[−]_ _[z]_ [ˆ] _[k][|][k][−]_ [1] [][] _[Z]_ _[k][|][k][−]_ [1] _[,j]_ _[−]_
_z_ ˆ _k|k−_ 1 ] _[⊤]_ + _C_ _f_, see (59)

13: _P_ _x_ _k_ _,z_ _k_ = [�] [2(] _j_ =0 _[m]_ _[a]_ _[−]_ [1)] _w_ _j_ _[c]_ [[] _[X]_ _[k][|][k][−]_ [1] _[,j]_ _[⊖]_ _[x]_ [ˆ] _[k][|][k][−]_ [1] [][] _[Z]_ _[k][|][k][−]_ [1] _[,j]_ _[−]_
_z_ ˆ _k|k−_ 1 ] _[⊤]_, see (60)



14:



 _K_ ˆ _k_ = _P_ _x_ _k_ _,z_ _k_ _P_ _z_ _[⊤]_ ˆ _k_ _,z_ _k_

_δx_ _k|k−_ 1 = _K_ ( _z_ _k_ _−_ _z_ _k|k−_ 1 )

ˆ ˆ

 _x_ _k|k_ = ˆ _x_ _k|k−_ 1 _⊕_ _δx_ _k|k−_ 1

 _P_ _k|k_ = _P_ _k|k−_ 1 _−_ _K_ _k_ _P_ _z_ _k_ _,z_ _k_ _K_ _k_ _[⊤]_



, see (67)



15: _k_ = _k_ + 1

**end while**


numerical issues, for any _P ∈_ R _[m]_ _[P]_ _[ ×][m]_ _[P]_, the Symmetrize
function is defined as Symmetrize : R _[m]_ _[P]_ _[ ×][m]_ _[P]_ _→_ R _[m]_ _[P]_ _[ ×][m]_ _[P]_ :

Symmetrize( _P_ ) := _[P]_ [ +] _[ P]_ _[ ⊤]_ _∈_ R _[m]_ _[P]_ _[ ×][m]_ _[P]_ _._ (69)

2


This function is applied to all single-variable covariance
matrices after they are calculated according to the QNUKF.
The filter parameters are configured as follows: _λ_ = 3 _−_


11



2.4


2.2


2


1.8


1.6


1.4


1.2


1


0.8


4





**Dataset Trajectory**



2



3


2.5


2


1.5


1


0.5


0
0 20 40 60 80


2.5


2


1.5


1


0.5


0 20 40 60 80


6


5


4


3


2


1


0 20 40 60 80

Time [s]









-2 -2.5 -2 X (m)





X (m)



Fig. 4: Validation results of the proposed QNUKF using the EuRoC V1 02 medium dataset [40]. On the left, the navigation
trajectory of the drone moving in 3D space is shown with a solid black line, and the drone orientation with respect to _x_, _y_, and
_z_ axes is represented by red, green, and blue dashed lines, respectively. On the right, the magnitudes of the orientation error
vector _||r_ _e,k_ _||_, position error vector _||p_ _e,k_ _||_, and linear velocity error vector _||v_ _e,k_ _||_ are plotted over time in solid red lines.


_B. Comparison with State-of-the-art Literature_



Fig. 5: Estimation errors in rotation vector (left), position
(middle), and linear velocity (right) components, from top to
bottom of the proposed QNUKF.


ˆ
_x_ 0:0 = [0 _._ 1619, 0 _._ 7900, _−_ 0 _._ 2053, 0 _._ 5545, 0 _._ 6153, 2 _._ 0967,
0 _._ 7711, 0, 0, 0, _−_ 0 _._ 0022, 0 _._ 0208, 0 _._ 0758, _−_ 0 _._ 0147, 0 _._ 1051,
0 _._ 0930] _[⊤]_ and _P_ 0 _|_ 0 = diag(80 _I_ 3 _,_ 10 _I_ 3 _,_ 70 _I_ 3 _,_ 10 _I_ 6 ). On the left
side of Fig. 4, the trajectory and orientation of the drone
during the experiment are depicted. On the right side, the
magnitudes of the orientation, position, and linear velocity
error vectors are plotted over time. The results demonstrate
that all errors converge to near-zero values and recover from
initial large errors rapidly, indicating excellent performance.
To examine the filter’s performance in detail, each component
of the estimation error is plotted against time. These plots are
displayed in Fig. 5, showing excellent performance in tracking
each component of orientation, position, and linear velocity.



[For video of the comparison, visit the following link. To](https://youtu.be/BWraOI0LAXo)
evaluate the efficacy of the proposed filter, we compared its
performance with the vanilla Extended Kalman Filter (EKF),
the standard tool in non-linear estimation problems as well
as the Multi-State Constraint Kalman Filter (MSCKF) [43],
which addresses the observability issues of the EKF caused
by linearization. Both models were implemented to operate
with the same state vectors (as defined in (30)), state transition
function (as defined in (34)), and measurement function (as defined in (37)). Additionally, their noise covariance parameters,
as well as initial state and covariance matrices, were set to
identical values in each experiment for all the filters to ensure
a fair comparison. In light of (70), (71), and (72), let us define
the estimation error at time step _k_ denoted by _e_ _k_ as:


_e_ _k_ = _||r_ _e,k_ _||_ + _||p_ _e,k_ _||_ + _||v_ _e,k_ _|| ∈_ R (73)


Using (73), the Root Mean Square Error (RMSE) is calculated

as:



where _m_ _k_ is the total number of time steps. The steady state
root mean square error (SSRMSE) is defined similarly to
RMSE but calculated over the last 20 seconds of the experiment. Table II compares the performance of the EKF, MSCKF,
and QNUKF across three different EuRoC [40] datasets using RMSE and SSRMSE as metrics. Experiments 1, 2, and
3 correspond to the V1 02 medium, V1 03 difficult, and
V2 01 easy datasets, respectively, with the best RMSE and



RMSE =



~~�~~
~~�~~
�


_m_ _k_

� [1]



_m_ _k_
� _e_ [2] _k_


_k_ =1


TABLE II: Performance Comparison of QNUKF (proposed)
vs EKF and MSCKF (literature) on EuRoC Datasets.


Filter Metric Experiment 1 Experiment 2 Experiment 3


RMSE 0 _._ 952955 0 _._ 831777 1 _._ 411102
EKF
SSRMSE 0 _._ 123161 0 _._ 160883 0 _._ 263435


RMSE 0 _._ 866786 1 _._ 175951 1 _._ 454449
MSCKF
SSRMSE 0 _._ 074154 0 _._ 062882 0 _._ 075329


RMSE 0 _._ 331952 0 _._ 275067 0 _._ 265037
QNUKF
SSRMSE 0 _._ 059464 0 _._ 051633 0 _._ 060865


SSRMSE values in each experiment. The proposed QNUKF
algorithm consistently outperformed the other filters across all
experiments and metrics. While the MSCKF showed better
performance than the EKF, it lacked consistency, with RMSE
values ranging from 0 _._ 866786 to 1 _._ 454449. In contrast, the
QNUKF maintained a more stable performance, with RMSE
values tightly clustered between 0 _._ 265037 and 0 _._ 331952.
Let us examine the orientation, position, and linear velocity
estimation error trajectories for all the filters in Experiment
2 as a case study. As depicted in Fig. 6, the QNUKF outperformed the other filters across all estimation errors. The
EKF exhibited a slow response in correcting the initial error,
while the MSCKF corrected course more quickly but initially
overshot compared to the EKF. In contrast, the QNUKF
responded rapidly and maintained smaller steady-state errors
when compared with EKF and MSCKF.


Fig. 6: Estimation errors in orientation (top), position (middle),
and linear velocity (bottom) for the EKF (red thick solid line),
MSCKF (green solid line), and proposed QNUKF (blue dashed
line).


VI. C ONCLUSION


In this paper, we investigated the orientation, position, and
linear velocity estimation problem for a rigid-body navigating
in three-dimensional space with six degrees-of-freedom. A robust Unscented Kalman Filter (UKF) is developed, incorporating quaternion space, ensuring computational efficiency at low



12


sampling rates, handling kinematic nonlinearities effectively,
and avoiding orientation singularities, such as those common in Euler angle-based filters. The proposed Quaternionbased Navigation Unscented Kalman Filter (QNUKF) relies
on onboard Visual-Inertial Navigation (VIN) sensor fusion of
data obtained by a stereo camera (feature observations and
measurements) and a 6-axis Inertial Measurement Unit (IMU)
(angular velocity and linear acceleration). The performance of
the proposed algorithm was validated using real-world dataset
of a drone flight travelling in GPS-denied regions where stereo
camera images and IMU data were collected at low sampling
rate. The results demonstrated that the algorithm consistently
achieved excellent performance, with orientation, position, and
linear velocity estimation errors quickly converging to nearzero values, despite significant initial errors and the use of lowcost sensors with high uncertainty. The proposed algorithm
consistently outperformed both baseline and state-of-the-art
filters, as demonstrated by the three experiments conducted
in two different rooms.

As the next step, we will investigate the effectiveness
of adaptive noise covariance tuning on QNUKF to mitigate
this issue. Additionally, reworking QNUKF to address simultaneous localization and mapping (SLAM) for UAVs is
a consideration for future work, as it represents a natural
progression from navigation.


R EFERENCES


[1] H. A. Hashim, “GPS-denied navigation: Attitude, position, linear velocity, and gravity estimation with nonlinear stochastic observer,” in _2021_
_American Control Conference (ACC)_ . IEEE, 2021, pp. 1149–1154.

[2] Y. Xu, Y. S. Shmaliy, S. Bi, X. Chen, and Y. Zhuang, “Extended
kalman/ufir filters for uwb-based indoor robot localization under timevarying colored measurement noise,” _IEEE Internet of Things Journal_,
vol. 10, no. 17, pp. 15 632–15 641, 2023.

[3] H. A. Hashim, A. E. Eltoukhy, and A. Odry, “Observer-based controller
for VTOL-UAVs tracking using direct vision-aided inertial navigation
measurements,” _ISA transactions_, vol. 137, pp. 133–143, 2023.

[4] Q. Wang, H. Luo, H. Xiong, A. Men, F. Zhao, M. Xia, and C. Ou,
“Pedestrian dead reckoning based on walking pattern recognition and
online magnetic fingerprint trajectory calibration,” _IEEE Internet of_
_Things Journal_, vol. 8, no. 3, pp. 2011–2026, 2020.

[5] H. A. Hashim, M. Abouheaf, and M. A. Abido, “Geometric stochastic
filter with guaranteed performance for autonomous navigation based on
IMU and feature sensor fusion,” _Control Engineering Practice_, vol. 116,
p. 104926, 2021.

[6] O. Asraf, F. Shama, and I. Klein, “Pdrnet: A deep-learning pedestrian
dead reckoning framework,” _IEEE Sensors Journal_, vol. 22, no. 6, pp.
4932–4939, 2021.

[7] C. Jiang, Y. Chen, C. Chen, J. Jia, H. Sun, T. Wang, and J. Hyyppa,
“Implementation and performance analysis of the pdr/gnss integration
on a smartphone,” _GPS Solutions_, vol. 26, no. 3, p. 81, 2022.

[8] H. A. Hashim, L. J. Brown, and K. McIsaac, “Nonlinear stochastic
attitude filters on the special orthogonal group 3: Ito and stratonovich,”
_IEEE Transactions on Systems, Man, and Cybernetics: Systems_, vol. 49,
no. 9, pp. 1853–1865, 2018.

[9] S. Carletta, P. Teofilatto, and M. S. Farissi, “A magnetometer-only attitude determination strategy for small satellites: Design of the algorithm
and hardware-in-the-loop testing,” _Aerospace_, vol. 7, no. 1, p. 3, 2020.

[10] H. A. Hashim, “Systematic convergence of nonlinear stochastic estimators on the special orthogonal group SO (3),” _International Journal of_
_Robust and Nonlinear Control_, vol. 30, no. 10, pp. 3848–3870, 2020.

[11] D. Scaramuzza and et al., “Vision-controlled micro flying robots: from
system design to autonomous navigation and mapping in GPS-denied
environments,” _IEEE Robotics & Automation Magazine_, vol. 21, no. 3,
pp. 26–40, 2014.

[12] H.-P. Tan, R. Diamant, W. K. Seah, and M. Waldmeyer, “A survey of
techniques and challenges in underwater localization,” _Ocean Engineer-_
_ing_, vol. 38, no. 14-15, pp. 1663–1676, 2011.


[13] G. P. Roston and E. P. Krotkov, _Dead Reckoning Navigation For Walking_
_Robots._ Department of Computer Science, Carnegie-Mellon University,
1991.

[14] H. Zhou, Y. Zhao, X. Xiong, Y. Lou, and S. Kamal, “IMU deadreckoning localization with rnn-iekf algorithm,” in _2022 IEEE/RSJ_
_International Conference on Intelligent Robots and Systems (IROS)_ .
IEEE, 2022, pp. 11 382–11 387.

[15] H. A. Hashim, A. E. Eltoukhy, and K. G. Vamvoudakis, “UWB ranging
and IMU data fusion: Overview and nonlinear stochastic filter for inertial
navigation,” _IEEE Transactions on Intelligent Transportation Systems_,
2023.

[16] S. Zheng, Z. Li, Y. Liu, H. Zhang, and X. Zou, “An optimization-based
UWB-IMU fusion framework for UGV,” _IEEE Sensors Journal_, vol. 22,
no. 5, pp. 4369–4377, 2022.

[17] H. H. Helgesen and et al., “Inertial navigation aided by ultra-wideband
ranging for ship docking and harbor maneuvering,” _IEEE Journal of_
_Oceanic Engineering_, vol. 48, no. 1, pp. 27–42, 2022.

[18] W. You, F. Li, L. Liao, and M. Huang, “Data fusion of UWB and IMU
based on unscented kalman filter for indoor localization of quadrotor
uav,” _IEEE Access_, vol. 8, pp. 64 971–64 981, 2020.

[19] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
in _Sensor fusion IV: control paradigms and data structures_, vol. 1611.
Spie, 1992, pp. 586–606.

[20] A. Myronenko and X. Song, “Point set registration: Coherent point drift,”
_IEEE transactions on pattern analysis and machine intelligence_, vol. 32,
no. 12, pp. 2262–2275, 2010.

[21] H. A. Hashim, “A geometric nonlinear stochastic filter for simultaneous
localization and mapping,” _Aerospace Science and Technology_, vol. 111,
p. 106569, 2021.

[22] Z. Bai, H. Xu, Q. Ding, and X. Zhang, “Side-scan sonar image
classification with zero-shot and style transfer,” _IEEE Transactions on_
_Instrumentation and Measurement_, 2024.

[23] J. A. Christian and S. Cryan, “A survey of lidar technology and its use
in spacecraft relative navigation,” in _AIAA Guidance, Navigation, and_
_Control (GNC) Conference_, 2013, p. 4641.

[24] Y. Wei and Y. Xi, “Optimization of 3-d pose measurement method
based on binocular vision,” _IEEE Transactions on Instrumentation and_
_Measurement_, vol. 71, pp. 1–12, 2023.

[25] D. Murray and J. J. Little, “Using real-time stereo vision for mobile
robot navigation,” _autonomous robots_, vol. 8, pp. 161–171, 2000.

[26] T. Huntsberger, H. Aghazarian, A. Howard, and D. C. Trotz, “Stereo
vision–based navigation for autonomous surface vessels,” _Journal of_
_Field Robotics_, vol. 28, no. 1, pp. 3–18, 2011.

[27] J. Kim and S. Sukkarieh, “Real-time implementation of airborne inertialslam,” _Robotics and Autonomous Systems_, vol. 55, no. 1, pp. 62–71,
2007.

[28] J. Huang, S. Wen, W. Liang, and W. Guan, “Vwr-slam: Tightly coupled slam system based on visible light positioning landmark, wheel



13


odometer, and rgb-d camera,” _IEEE Transactions on Instrumentation_
_and Measurement_, vol. 72, pp. 1–12, 2022.

[29] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman
filter for vision-aided inertial navigation,” in _Proceedings 2007 IEEE_
_international conference on robotics and automation_ . IEEE, 2007, pp.
3565–3572.

[30] G. Huang, “Visual-inertial navigation: A concise review,” in _2019_
_international conference on robotics and automation (ICRA)_ . IEEE,
2019, pp. 9572–9582.

[31] A. T. Erdem and A. O. Ercan, “Fusing inertial sensor data in an extended [¨]
kalman filter for 3d camera tracking,” _IEEE Transactions on Image_
_Processing_, vol. 24, no. 2, pp. 538–548, 2014.

[32] M. B. Khalkhali, A. Vahedian, and H. S. Yazdi, “Multi-target state
estimation using interactive kalman filter for multi-vehicle tracking,”
_IEEE Transactions on Intelligent Transportation Systems_, vol. 21, no. 3,
pp. 1131–1144, 2019.

[33] S. Haykin, _Kalman filtering and neural networks_ . John Wiley & Sons,
2004.

[34] C. Jiang and Q. Hu, “Iterative pose estimation for a planar object
using virtual sphere,” _IEEE Transactions on Aerospace and Electronic_
_Systems_, vol. 58, no. 4, pp. 3650–3657, 2022.

[35] H. A. Hashim, “Special orthogonal group SO(3), euler angles, angleaxis, rodriguez vector and unit-quaternion: Overview, mapping and
challenges,” _arXiv preprint arXiv:1909.06669_, 2019.

[36] F. L. Markley, Y. Cheng, J. L. Crassidis, and Y. Oshman, “Quaternion
averaging,” 2007.

[37] Y. Song, N. Sebe, and W. Wang, “Fast differentiable matrix square root
and inverse square root,” _IEEE Transactions on Pattern Analysis and_
_Machine Intelligence_, 2022.

[38] S. J. Julier and J. K. Uhlmann, “New extension of the Kalman filter
to nonlinear systems,” in _Signal Processing, Sensor Fusion, and Target_
_Recognition VI_, I. Kadar, Ed., vol. 3068, International Society for Optics
and Photonics. SPIE, 1997, pp. 182 – 193.

[39] G. P. Huang, A. I. Mourikis, and S. I. Roumeliotis, “On the complexity
and consistency of ukf-based slam,” in _2009 IEEE International Con-_
_ference on Robotics and Automation_, 2009, pp. 4401–4408.

[40] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W.
Achtelik, and R. Siegwart, “The EuRoC micro aerial vehicle datasets,”
_The International Journal of Robotics Research_, vol. 35, no. 10, pp.
1157–1163, 2016.

[41] J. Shi and Tomasi, “Good features to track,” in _1994 Proceedings of_
_IEEE Conference on Computer Vision and Pattern Recognition_, 1994,
pp. 593–600.

[42] R. Hartley and A. Zisserman, _Multiple view geometry in computer vision_ .
Cambridge university press, 2003.

[43] K. Sun, K. Mohta, B. Pfrommer, M. Watterson, S. Liu, Y. Mulgaonkar,
C. J. Taylor, and V. Kumar, “Robust stereo visual inertial odometry for
fast autonomous flight,” _IEEE Robotics and Automation Letters_, vol. 3,
no. 2, pp. 965–972, 2018.


